## DataFrame 다루기

# 01. DataFrame 인덱싱 I

import pandas as pd

iphone_df = pd.read_csv('Downloads/iphone.csv', index_col=0)
iphone_df

iphone_df.loc['iPhone 8', '메모리']

iphone_df.loc['iPhone X', :]

type(iphone_df.loc['iPhone X'])

iphone_df.loc[:, '출시일']



# 실습 설명
지난 시간에 DataFrame에서 원하는 부분을 선택하는 인덱싱을 배웠는데요. 이를 통해서 값을 찾는 연습을 해봅시다.
2016년도에 KBS방송국의 시청률을 찾아봅시다.
데이터를 한번 잘 살펴보고 어떻게 값을 찾아야 할지 고민해보세요!

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)

# 해설
DataFrame이 주어지면, 인덱싱을 통해 자유롭게 우리가 원하는 값을 꺼내올 수 있습니다.

먼저 DataFrame이 어떤 모양인지 확인해 봅시다.


import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)


이런 형태로 되어 있네요.

DataFrame에서 인덱싱을 통해 값을 받아오기 위해서는 loc 메소드를 사용하면 됩니다.


df.loc[row, column]
여기서 우리가 찾으려는 값의 row는 2016이고 column은 'KBS'입니다.

Row가 '2016'이 아니라 2016이라는 점 유의해 주세요!


import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df.loc[2016, 'KBS']

27.583000000000002


# 해설
column에 대한 인덱싱은 두 가지 방식으로 할 수 있습니다. loc를 사용하는 방법과 사용하지 않는 방법이 있죠.

더 깔끔한 코드를 위해서 loc를 사용하지 않는 방법으로 해 봅시다.

한 column에 대한 인덱싱은 이렇게 할 수 있습니다.


df[column]
이런 형식을 적용해서, JTBC의 시청률만 가져와 봅시다.

모범 답안

import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df['JTBC']

2011    7.380
2012    7.878
2013    7.810
2014    7.490
2015    7.267
2016    7.727
2017    9.453
Name: JTBC, dtype: float64

#해설
column에 대한 인덱싱은 두 가지 방식으로 할 수 있습니다. loc를 사용하는 방법과 사용하지 않는 방법이 있죠.

더 깔끔한 코드를 위해서 loc를 사용하지 않는 방법으로 해 봅시다.

한 column에 대한 인덱싱은 이렇게 할 수 있습니다.


df[column]
그럼 여러 column에 대한 인덱싱은 어떻게 할 수 있을까요?

리스트로 인덱싱을 하면 되겠죠!


df[[column1, column2]]
우리가 원하는 column은 'SBS'와 'JTBC' 이니까, 다음과 같이 코드를 작성하면 됩니다.

모범 답안

import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df[['SBS', 'JTBC']]



## 카드사 고객 분석

# 실습 설명
데이터의 중요성을 깨달은 “삼송카드”와 “현디카드”가 협업을 하기로 결정했습니다.

두 카드사는 사람들이 요일별로 지출하는 평균 금액을 “요일”, “식비”, “교통비”, “문화생활비”, “기타” 카테고리로 정리해서 우리에게 공유해 주기로 했는데요. 각각 samsong.csv 파일과 hyundee.csv 파일을 보냈습니다.

두 회사의 데이터를 활용해서, 사람들의 요일별 문화생활비를 분석해보려 합니다. 아래와 같은 형태로 출력이 되도록 DataFrame을 만들어보세요.

# 실습 결과

   day  samsong  hyundee
0  MON     4308     5339
1  TUE     7644     3524
2  WED     5674     5364
3  THU     8621     9942
4  FRI    23052    33511
5  SAT    15330    19397
6  SUN    19030    19925
주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)

# 해설
# 1. 데이터 파악하기
먼저 각각의 DataFrame을 읽어들여봅시다.


import pandas as pd

samsong_df = pd.read_csv('data/samsong.csv')
samsong_df



import pandas as pd

hyundee_df = pd.read_csv('data/hyundee.csv')
hyundee_df


이 중에서 우리가 활용하고 싶은 데이터는 '요일', 그리고 samsong_df의 '문화생활비', hyundee_df의 '문화생활비' 입니다.

각각을 이렇게 가져올 수 있겠네요.


samsong_df['요일']
samsong_df['문화생활비']
hyundee_df['문화생활비']

# 2. 파이썬 사전(dictionary) 만들기
DataFrame을 만드는 여러 방법 중에 파이썬 사전(dictionary)을 활용해 봅시다.

우리가 원하는 세 개의 column은 'day', 'samsong', 'hyundee' 입니다.

그럼 사전을 이렇게 만들 수 있겠네요.


{'day': samsong_df['요일'], 
    'samsong': samsong_df['문화생활비'], 
    'hyundee': hyundee_df['문화생활비']}

# 모범 답안
이제 이 값을 활용해서 DataFrame을 만들면 됩니다.


import pandas as pd

samsong_df = pd.read_csv('data/samsong.csv')
hyundee_df = pd.read_csv('data/hyundee.csv')

combined_df = pd.DataFrame({
    'day': samsong_df['요일'], 
    'samsong': samsong_df['문화생활비'], 
    'hyundee': hyundee_df['문화생활비']
})
combined_df



## 방송사 시청률 받아오기IV
# 실습 설명
이번에는 DataFrame에서 연속된 여러 줄을 찾는 연습을 해보겠습니다.

방송사는 'KBS'에서 'SBS'까지, 연도는 2012년부터 2017년까지의 시청률만 확인하려면 어떻게 하면 될까요?

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)


# 해설
방송사는 'KBS'에서 'SBS'까지, 연도는 2012년부터 2017년까지의 시청률만 확인하려면 어떻게 하면 될까요?

리스트 슬라이싱을 활용하면 됩니다.

만약 2012년부터 2017년까지의 모든 데이터를 확인하고 싶다면


import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df.loc[2012:2017]
이렇게 확인할 수 있겠네요.



만약, 방송사 기준으로 KBS에서 SBS 사이 column의 모든 정보를 확인하고 싶다면


import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df.loc[:, 'KBS':'SBS']


이렇게 확인할 수 있습니다.

그럼 이제 이 두 방식을 조합하면, 우리가 원하는 결과가 나옵니다.

모범 답안

import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df.loc[2012:2017, 'KBS':'SBS']



## DataFrame 조건으로 인덱싱

iphone_df.loc[True, False, False, True]  // 세로 열의 컬럼에서 0,3번째 항목만 표시

iphone_df.loc[:, [True, False, False, True]]  // 세로 열의 모든 컬럼과 가로 행의 0,3번째 항목을 표시

iphone_df['디스플레이'] > 5  // 디스플레이 항목에서 5보다 큰 항목들만 표시 (pandas 시리즈)

iphone_df.loc[iphone_df['디스플레이'] > 5]  // 디스플레이 항목에서 5보다 큰 항목들만 표시 (파이썬처럼 필터링)

iphone_df['Face ID'] == 'Yes'  // Face ID 기능이 있는 항목만 필터링

(iphone_df['디스플레이'] > 5) & (iphone_df.['Face ID'] == 'Yes')  // 디스플레이 크기 5이상과 Face ID 기능이 있는 항목을 필터링
(iphone_df['디스플레이'] > 5) | (iphone_df.['Face ID'] == 'Yes')  // 디스플레이 크기 5이상이거나 Face ID 기능이 있는 항목을 필터링

condition = (iphone_df['디스플레이'] > 5) & (iphone_df.['Face ID'] == 'Yes') // 내용이 길어서 변수에 저장
condition = (iphone_df['디스플레이'] > 5) | (iphone_df.['Face ID'] == 'Yes') 

# iphone_df[condition] // 리스트를 필터링해서 표시



## 방송사 시청률 받아오기 V

# 해설
'KBS'에서 시청률이 30이 넘은 데이터만 확인해보고 싶습니다. 어떻게 하면 될까요?

불린 인덱싱을 활용하면 됩니다.

먼저 'KBS'에 대한 정보만을 다루기 때문에, 'KBS' column만 사용하면 됩니다.


import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df['KBS']

2011    35.951
2012    36.163
2013    31.989
2014    31.210
2015    27.777
2016    27.583
2017    26.890
Name: KBS, dtype: float64
그러면 모든 데이터가 다 나오네요.

여기서 불린 연산을 활용해 봅시다.


df['KBS'] > 30

2011     True
2012     True
2013     True
2014     True
2015    False
2016    False
2017    False
Name: KBS, dtype: bool
이런 식으로, 30이 넘는지 넘지 않는지 여부가 불린값으로 나옵니다.
이 값을 DataFrame 인덱싱에 활용하면 우리가 원하는 값만 뽑아낼 수 있습니다.


boolean_KBS = df['KBS'] > 30
df.loc[boolean_KBS]


그대로 불린을 넣었더니, 2012년부터 2014년까지 모든 방송사의 데이터가 함께 나오네요.

아래와 같이 코드를 작성하면 우리가 원하는 값만 얻을 수 있습니다.

# 모범 답안

import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
boolean_KBS = df['KBS'] > 30
df.loc[boolean_KBS, 'KBS']



## 방송사 시청률 받아오기 VI


# 하루 작성
import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)

comparison = (df['SBS']) < (df['TV CHOSUN'])

df.loc[comparison, 'SBS':'TV CHOSUN']


# 모범 답안
import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)

df.loc[df['SBS'] < df['TV CHOSUN'], ['SBS', 'TV CHOSUN']]




# 해설
SBS가 TV CHOSUN보다 더 시청률이 낮았던 시기의 데이터는 어떻게 확인할 수 있을까요?

이것도 불린 연산을 활용하면 됩니다.
우선 두 column을 비교한 결과를 살펴봅시다.


import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df['SBS'] < df['TV CHOSUN']

2011    False
2012    False
2013    False
2014     True
2015     True
2016     True
2017     True
dtype: bool
언제 SBS보다 TV CHOSUN이 높았는지 확인할 수 있습니다.
True로 출력되는 2014년부터 2017년이 TV CHOSUN의 시청률이 더 높았군요.

DataFrame의 기본 인덱싱 형태 기억하시죠?


df.loc[row, column]
방금 구한 불린 값을 row 인덱싱에 활용하고, column 인덱싱에는 우리가 보고자 하는 column인 'SBS', 'TV CHOSUN' 를 적어주면 됩니다.

# 모범 답안

import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df.loc[df['SBS'] < df['TV CHOSUN'], ['SBS', 'TV CHOSUN']]



## DataFrame 위치로 인덱싱하기

iphone_df.iloc[2, 4]

iphone_df.iloc[[2, 4], [1, 4]]

iphone_df.iloc[3: , 1:4]


## DataFrame 인덱싱 문법 정리

DataFrame 인덱싱을 하는 방법과 종류가 많아서 헷갈리기 쉽습니다.

인덱싱이 익숙해져야 다음 내용을 쉽게 배울 수 있으니, 꼭 숙지하고 넘어가세요!

# 이름으로 인덱싱하기 / 기본 형태 / 단축 형태
하나의 row 이름 / df.loc["row4"]	
row 이름의 리스트 / df.loc[["row4", "row5", "row3"]]	
row 이름의 리스트 슬라이싱 / df.loc["row2":"row5"] / df["row2":"row5"]
하나의 column 이름 / df.loc[:, "col1"] / df["col1"]
column 이름의 리스트 / df.loc[:, ["col4", "col6", "col3"]] / df[["col4", "col6", "col3"]]
column 이름의 리스트 슬라이싱 / df.loc[:, "col2":"col5"]	
위치로 인덱싱하기 / 기본 형태 / 단축 형태
하나의 row 위치 / df.iloc[8]	
row 위치의 리스트 / df.iloc[[4, 5, 3]]	
row 위치의 리스트 슬라이싱 / df.iloc[2:5] / df[2:5]
하나의 column 위치 / df.iloc[:, 3]	
column 위치의 리스트 / df.iloc[:, [3, 5, 6]]	
column 위치의 리스트 슬라이싱 / df.iloc[:, 3:7]	


### 데이터 변형하기
## DataFrame에 값 쓰기 I

iphone_df.loc['iPhone 8', '출시 버전'] = 'ios 10.3'
iphone_df

iphone_df.loc['iPhone 8'] = ['2016-09-22', '4.7', '2GB', 'ios 11.0', 'No'] // 내용 수정하기

iphone_df['디스플레이'] = ['4.7 in', '4.7 in', '4.3 in', '4.5 in', '4.7 in', '4.7 in', '4.7 in'] // 컬럼 전체 수정하기

iphone_df['Face ID'] = 'Yes' // 특정 항목 컬럼 전체 바꾸기


## DataFrame에 값 쓰기 II

iphone_df[['디스플레이', 'Face ID']] = 'X'

iphone_df.loc[['iPhone 7', 'iPhone X']] = '0'

iphone_df.loc[['iPhone 7':'iPhone X']] = '0'

iphone_df[['디스플레이']] > 5

iphone_df.loc[iphone_df['디스플레이'] > 5]

iphone_df.loc[iphone_df['디스플레이'] > 5] = 'P'

iphone_df.iloc[[1, 3], [1, 4]]

iphone_df.iloc[[1, 3], [1, 4]] = 'V'


## 03. DataFrame에 값 추가/삭제


# 값 추가
iphone_df.loc['iPhone XR'] = ['2016-09-22', '4.7', '2GB', 'ios 11.0', 'No'] // 값 추가

iphone_df['제조사'] = 'Apple'

# 값 삭제
iphone_df.drop('iPhone XR', axis='index', inplace=False)

// inplace=False 는 기존의 데이터를 변경하지 않는다. inplace=True 는 원본 데이터 수정

# columns의 값 삭제
iphone_df.drop('iPhone XR', axis='columns', inplace=True)

# 여러 low를 삭제
iphone_df.drop(['iPhone 7', 'iPhone 8', 'iPhone X'], axis='index', inplace=True)


## 잘못된 DataFrame 고치기 I

# 해설
우리는 크게 세 가지 일을 해야 합니다.

ID 1의 무게를 200으로 변경하세요.
ID 21의 row를 삭제하세요.
ID 20의 row를 추가하세요. ID 20의 키는 70, 무게는 200입니다.
하나씩 해결해 봅시다.

# 1. ID 1의 무게를 200으로 변경하세요.

import pandas as pd
df = pd.read_csv('data/body_imperial1.csv', index_col=0)
먼저 데이터를 불러온 후,


df.loc[1,"Weight (Pound)"] = 200
인덱싱을 해서 값을 써주면 되겠죠.

# 2. ID 21의 row를 삭제하세요.
row나 column을 삭제할 때는 drop 메소드를 사용하면 됩니다.


df.drop(21, axis="index", inplace=True)

# 3. ID 20의 row를 추가하세요. ID 20의 키는 70, 무게는 200입니다.
ID 20의 row를 인덱싱한 후, 키와 무게를 리스트에 넣어서 전달해줍니다.


df.loc[20] = [70,200]
이 작업들을 모두 마치니, 우리가 원하는 결과가 완성되었네요.

# 모범 답안

import pandas as pd
df = pd.read_csv('data/body_imperial1.csv', index_col=0)

# 데이터 고치기
df.loc[1,"Weight (Pound)"] = 200
df.drop(21, axis = "index", inplace = True)
df.loc[20] = [70,200]

# 테스트 코드
df


## 잘못된 DataFrame 고치기 II

# 실습 설명
키와 몸무게가 담겨 있는 한 DataFrame이 있는데요. 몇 가지 잘못된 사항들이 있습니다. 이번 챕터에서 배운 기법들로 DataFrame을 바로잡아 봅시다.

해야 할 일이 두 가지 있습니다.

'비만도' column을 추가하고, 모든 ID에 대해 '정상'으로 설정해주세요.
'Gender' column의 값을 ID 0~10까지는 'Male' 11~20까지는 'Female'로 변경하세요.
실습 결과


주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)

# 해설
주어진 DataFrame에서 두 가지를 수정하면 됩니다.

비만도 column을 추가하고, 모든 ID에 대해 '정상'으로 설정해주세요.
'Gender' column의 값을 ID 0~10까지는 'Male' 11~20까지는 'Female'로 변경하세요.
하나씩 해봅시다.

1. '비만도' column을 추가하고, 모든 ID에 대해 '정상'으로 설정해주세요.
새로운 column을 추가하려면, 다음과 같은 문법을 사용하면 됩니다.


df['column name'] = value
우리의 데이터에 적용해 봅시다.


# 데이터 읽기
import pandas as pd
df = pd.read_csv('data/body_imperial2.csv', index_col=0)

# 데이터 고치기
df["비만도"] = "정상"
2. 'Gender' column의 값을 ID 0~10까지는 'Male' 11~20까지는 'Female'로 변경하세요.
0부터 10까지, 그리고 11부터 20까지를 어떻게 인덱싱할 수 있을까요?

리스트 슬라이싱을 활용하면 됩니다.

처음부터 10까지는 :10, 11부터 마지막까지는 11:로 표현할 수 있습니다.


df.loc[:10,"Gender"] = "Male"
df.loc[11:,"Gender"] = "Female"

# 정답 출력
df
모범 답안

# 데이터 읽기
import pandas as pd
df = pd.read_csv('data/body_imperial2.csv', index_col=0)

# 데이터 고치기
df["비만도"] = "정상"
df.loc[:10,"Gender"] = "Male"
df.loc[11:,"Gender"] = "Female"

# 정답 출력
df


## index/column 설정하기

liverpool_df.rename(columns{'position': 'Position'}, inplace=True)

liverpool_df.rename(columns{'position': 'Position', 'position': 'Position', 'position': 'Position'}, inplace=True)

liverpool_df.index.name = 'Player Name'

liverpool_df.set_index('Number')

liverpool_df.set_index('Number', inplace=True)

liverpool_df.index

liverpool_df['Player Name'] = liverpool_df.index

// 이름이 겹칠수 있으니 고유 등번호를 인덱스 값으로 사용



## 서류 전형 합격 여부
# 실습 설명
토익 시험은 LC(듣기) 파트와 RC(독해) 파트로 이루어져 있습니다. 각 파트가 495점 만점, 총 990점이 만점입니다.

“서울 항공”에 입사하기 위해서는 토익 점수를 제출해야 하는데요. 각 파트가 최소 250점, 총 점수가 최소 600점이 되어야 서류 전형을 합격할 수 있습니다.

기존 DataFrame에 “합격 여부”라는 column을 추가하고, 합격한 지원자는 불린 값 True, 불합격한 지원자는 불린 값 False를 넣어주면 됩니다.
# toeic.csv
Gender,LC,RC
female,315,320
female,430,245
female,430,475
male,180,220
male,325,350
female,295,400
female,405,475
male,155,150
male,280,315
female,215,475

# 해설
두 가지 조건을 모두 만족해야 서류 전형에 합격합니다.

각 조건을 먼저 살펴보고, 두 조건을 합치는 방식으로 접근해 봅시다.

# 1. 두 파트의 합이 600점을 넘어야 한다.
먼저 두 파트의 합이 600점을 넘는지 확인해 봅시다.


pass_total = df['LC'] + df['RC'] > 600
pass_total

0     True
1     True
2     True
3    False
4     True
5     True
6     True
7    False
8    False
9     True
dtype: bool
이 값을 pass_total이라는 변수에 저장해 두었습니다.

# 2. 모든 파트가 최소 250점 이상이어야 한다.
다음으로 두 파트 모두 최소 250점 이상인지도 확인해 봅시다.
이 값은 pass_both라는 변수에 저장해 둡시다.


pass_both = (df['LC'] >= 250) & ( df['RC'] >= 250)
pass_both

0     True
1    False
2     True
3    False
4     True
5     True
6     True
7    False
8     True
9    False
dtype: bool
두 조건을 모두 만족하는 사람을 찾는다.
이 두 변수 pass_total와 pass_both를 사용해서, 두 조건을 모두 만족하는 지원자를 찾아서 합격 여부에 추가합니다.


df['합격 여부'] = pass_total & pass_both
최종적으로 이런 코드가 됩니다.


# 모범 답안

import pandas as pd

df = pd.read_csv('data/toeic.csv')

pass_total = df['LC'] + df['RC'] > 600
pass_both = (df['LC'] >= 250) & (df['RC'] >= 250)
df['합격 여부'] = pass_total & pass_both

# 테스트 코드
df


## 
# 실습 설명
데이터프레임이 있습니다.
코드를 4줄만 써서, 아래 데이터프레임으로 바꿔보세요.

# 실습 결과

어느 부분이 바뀌었을까요?
눈에 잘 들어오지 않는다면, 하나씩 힌트를 열어 확인해보고 코드를 작성해 보세요.

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)


# 해설
두 이미지를 잘 살펴보면, 세 가지의 변화가 있었음을 발견할 수 있습니다.

1. 'A' column이 모두 2배가 되었습니다.
이 변화는 아래와 같이 작성할 수 있습니다.


df['A'] = df['A'] * 2
2. 'B'에서 'E'까지의 column은 80보다 큰 것은 1, 작은 것은 0으로 바뀌었습니다.
이 내용은 리스트와 불린 연산을 활용해서 해결할 수 있습니다.


# 80보다 작은 값은 0으로 변경
lowers = df.loc[:, 'B':'E'] < 80
df[lowers] = 0

# 80 이상의 값은 1로 변경
highers = df.loc[:, 'B':'E'] >= 80
df[highers] = 1
이 코드를 줄여서 이렇게 쓸 수도 있겠죠?


df[df.loc[:, 'B':'E'] < 80] = 0
df[df.loc[:, 'B':'E'] >= 80] = 1
3. 2번 index의 'F' column 값이 29에서 99로 바뀌었습니다.
우리가 잘 아는 DataFrame 기본 인덱싱을 활용하면 됩니다.


df.loc[2, 'F'] = 99
모든 변경을 코드로 작성하면 아래와 같습니다.

# 모범 답안

import pandas as pd

df = pd.read_csv('data/Puzzle_before.csv')

df['A'] = df['A'] * 2
df[df.loc[:, 'B':'E'] < 80] = 0
df[df.loc[:, 'B':'E'] >= 80] = 1
df.loc[2, 'F'] = 99

# 테스트 코드
df



## 큰 DataFrame 살펴보기

laptops_df.head(7) // 데이터 첫부분 7줄 표시

laptops_df.tail(7) // 데이터 마지막 7줄 표시

laptops_df.shape // 데이터의 크기 모양

laptops_df.columns // 어떤 컬럼이 있는 확인

laptops_df.info // 정보를 확인 가능

laptops_df.describe()  // 통계를 확인

laptops_df.sort_values(by='price')  // price 기준으로 정렬 (기본적으로 낮은 순으로 정렬)

laptops_df.sort_values(by='price', ascending=True)  // price가 낮은 순으로 정렬

laptops_df.sort_values(by='price', ascending=False)  // price가 높은 순으로 정렬

laptops_df.sort_values(by='price', ascending=False, inplace=True)  // 원본 데이터를 수정하기 위해서는 inplace를 사용해야 한다.


## 큰 Series 살펴보기

laptops_df['brand']  // brand 전체를 보여준다.

laptops_df['brand'].unique()  // brand가 겹치지 않고 하나씩만 나열한다.

laptops_df['brand'].value_counts()  // brand 별로 카운트 한다.

laptops_df['brand'].describe()  // 요약해서 볼 수 있다.


## 여행지 선정하기 I
# 질문 1
여행을 좋아하는 익중이는 여행지를 알아보고 있습니다. 주어진 데이터에는 총 몇 개의 도시와 몇 개의 나라가 있는지 알아맞혀 보세요.

답안은 도시/나라 형식으로 숫자만 입력해 주세요. (예시: 156/77)

249/61
👏🏻 정답입니다.

퀴즈 해설


df['City / Urban area'].value_counts()
을 실행하면, 'City / Urban area' column의 항목들을 볼 수 있습니다.


df['City / Urban area'].value_counts().shape

(249,)
를 확인하면 총 249개의 도시가 있음을 알 수 있습니다.

마찬가지로 나라의 경우에도


df['Country'].value_counts().shape

(61,)
을 통해 몇 개 나라가 있는지 확인할 수 있습니다.

따라서 249개 도시, 61개 나라가 정답입니다.

# 질문 2
사람 만나기를 좋아하는 익중이는 가장 사람이 붐비는 도시로 여행을 가기로 마음 먹었습니다. 주어진 데이터에서, 인구 밀도(명/sqKm) 가 10000 이 넘는 도시는 총 몇 개인지 알아보세요.

참고로 인구 밀도는 인구 수 / 땅의 면적 (in sqKm) 로 구할 수 있습니다.

답안은 숫자로만 입력해 주세요. (예시: 24)

19
👏🏻 정답입니다.

퀴즈 해설

먼저 인구 밀도를 계산합니다.


df["Density"] = df["Population"] / df["Land area (in sqKm)"]
계산한 인구 밀도에 불린 연산을 적용하여 인덱싱 합니다.


df_high_density = df[df["Density"] > 10000]
불린 연산으로 인덱싱한 df_high_density에 총 몇 개의 데이터가 있는지 info() 메소드를 적용해서 살펴봅시다.


df_high_density.info()

<class 'pandas.core.frame.DataFrame'>
Int64Index: 19 entries, 32 to 129
Data columns (total 5 columns):
City / Urban area      19 non-null object
Country                19 non-null object
Population             19 non-null int64
Land area (in sqKm)    19 non-null int64
Density                19 non-null float64
dtypes: float64(1), int64(2), object(2)
memory usage: 912.0+ bytes
총 19개의 row가 있음을 확인할 수 있습니다.

# 질문 3
이번에는 인구 밀도가 가장 높은 도시를 찾아봅시다.

답안은 데이터에 적힌 이름을 그대로 입력해 주세요. (예시: Seoul)

Mumbai
👏🏻 정답입니다.

퀴즈 해설

이전 문제와 같이 인구 밀도를 먼저 계산합니다.


df["Density"] = df["Population"] / df["Land area (in sqKm)"]
이 인구 밀도를 기준으로 정렬합니다.


density_ranks = df.sort_values(by="Density", ascending = False)
정렬된 데이터에서 도시 정보만 뽑아서 확인해 봅시다.


density_ranks['City / Urban area']

75                      Mumbai
74                     Kolkata
101                    Karachi
                ...           
57                         Pau
220                    Hickory
196            Barnstable Town
Name: City / Urban area, Length: 249, dtype: object
Mumbai가 가장 높은 인구 밀도를 갖는다는 것을 알 수 있습니다.



## 여행지 선정하기 II
# world_cities.csv
,City / Urban area,Country,Population,Land area (in sqKm)
0,Buenos Aires,Argentina,11200000,2266
1,Melbourne,Australia,3162000,2080
2,Sydney,Australia,3502000,1687
3,Brisbane,Australia,1508000,1603
4,Perth,Australia,1177000,964
5,Adelaide,Australia,1002000,729
6,Gold Coast,Australia,422000,383
7,Vienna,Austria,1550000,453
8,Baku/Sumqayit,Azerbaijan,2100000,544
9,Brussels,Belgium,1570000,712
10,Antwerp,Belgium,915000,596
11,Sao Paulo,Brazil,17700000,1968
12,Rio de Janeiro,Brazil,10800000,1580
13,Belo Horizonte,Brazil,4000000,868
14,Curitiba,Brazil,2500000,648
15,Brasilia,Brazil,1625000,583
16,Fortaleza,Brazil,2650000,583
17,Porto Alegre,Brazil,2800000,583
18,Campinas,Brazil,1750000,492
19,Goiania,Brazil,1475000,479
20,Recife,Brazil,3025000,376
21,Phnom Penh,Cambodia,1500000,518
22,Montreal.,Canada,3216000,1740
23,Toronto,Canada,4367000,1655
24,Vancouver,Canada,1830000,1120
25,Edmonton,Canada,782000,850
26,Calgary,Canada,879000,702
27,Quebec,Canada,635000,669
28,Ottawa/Hull,Canada,828000,490
29,Winnipeg,Canada,627000,446
30,St. Catharines,Canada,300000,389
31,Santiago,Chile,5425000,648
32,Beijing,China,8614000,748
33,Shanghai,China,10000000,746
34,Shenzhen,China,8000000,466
35,Shenyang,China,4200000,453
36,Tianjin,China,4750000,453
37,Dalian,China,2750000,389
38,Bogota,Colombia,7000000,518
39,Kinshasa,Congo,5000000,469
40,Lumumbashi,Congo,1200000,427
41,Copenhagen,Denmark,1525000,816
42,Quito,Ecuador,1500000,479
43,Cairo,Egypt,12200000,1295
44,Helsinki,Finland,1000000,479
45,Paris,France,9645000,2723
46,Marseille,France,1350000,1204
47,Bordeaux,France,754000,1057
48,Lyon,France,1349000,954
49,Toulouse,France,761000,808
50,Nice,France,889000,721
51,Toulon,France,520000,713
52,Avignon,France,254000,508
53,Valenciennes,France,357000,507
54,Douai/Lens,France,519000,489
55,Nantes,France,545000,476
56,Lille,France,1050000,474
57,Pau,France,181000,450
58,Tours,France,298000,421
59,Bethune,France,259000,390
60,Essen/Düsseldorf,Germany,7350000,2642
61,Berlin,Germany,3675000,984
62,Frankfurt,Germany,2260000,984
63,Hamburg,Germany,1925000,829
64,Cologne/Bonn,Germany,1960000,816
65,Munich,Germany,1600000,518
66,Stuttgart,Germany,1250000,414
67,Aachen,Germany,585000,401
68,Accra,Ghana,1500000,453
69,Athens,Greece,3685000,684
70,Budapest,Hungary,1800000,702
71,Delhi,India,14300000,1295
72,Hyderabad,India,5300000,583
73,Bangalore,India,5400000,534
74,Kolkata,India,12700000,531
75,Mumbai,India,14350000,484
76,Chennai,India,5950000,414
77,Jakarta,Indonesia,14250000,1360
78,Tehran,Iran,7250000,686
79,Baghdad,Iraq,5500000,596
80,Dublin,Ireland,1075000,365
81,Tel Aviv,Israel,2300000,453
82,Milan,Italy,4250000,1554
83,Rome,Italy,2500000,842
84,Naples,Italy,2400000,583
85,Turin,Italy,1350000,500
86,Tokyo/Yokohama,Japan,33200000,6993
87,Nagoya,Japan,9000000,2875
88,Osaka/Kobe/Kyoto,Japan,16425000,2564
89,Fukuoka,Japan,2150000,544
90,Sapporo,Japan,2075000,414
91,Kuwait,Kuwait,1600000,544
92,Beirut,Lebanon,1800000,648
93,Kuala Lumpur,Malaysia,4400000,1606
94,Mexico City,Mexico,17400000,2072
95,Guadalajara,Mexico,3500000,596
96,Monterey,Mexico,3200000,479
97,Rotterdam,Netherlands,1325000,531
98,Auckland,New Zealand,1050000,531
99,Lagos,Nigeria,13400000,738
100,Lahore,Pakistan,5100000,622
101,Karachi,Pakistan,9800000,518
102,Lima,Peru,7000000,596
103,Manila,Philippines,14750000,1399
104,Katowice,Poland,2200000,544
105,Warsaw,Poland,2000000,466
106,Lisbon,Portugal,2250000,881
107,Porto,Portugal,1035000,389
108,San Juan,Puerto Rico,2217000,2309
109,Aguadilla,Puerto Rico,299000,620
110,Moscow,Russia,10500000,2150
111,St Petersburg,Russia,5300000,622
112,Nizhni Novgorod,Russia,1500000,505
113,Arabia,Saudi,1525000,673
114,Riyadh,Saudi Arabia,4000000,1101
115,Jeddah,Saudi Arabia,2750000,777
116,Singapore,Singapore,4000000,479
117,Johannesburg/East Rand,South Africa,6000000,2396
118,Durban,South Africa,2900000,829
119,Cape Town,South Africa,2700000,686
120,Pretoria,South Africa,1850000,673
121,Vereeniging,South Africa,600000,479
122,Port Elizabeth,South Africa,900000,427
123,Seoul/Incheon,South Korea,17500000,1049
124,Madrid,Spain,4900000,945
125,Barcelona,Spain,3900000,803
126,Khartoum,Sudan,4000000,583
127,Stockholm,Sweden,1400000,518
128,Taichung,Taiwan,2000000,510
129,Taipei,Taiwan,5700000,376
130,Bangkok,Thailand,6500000,1010
131,Istanbul,Turkey,9000000,1166
132,Ankara,Turkey,3100000,583
133,Abu Dhabi,UAE,550000,777
134,Dubai,UAE,1900000,712
135,London,UK,8278000,1623
136,Birmingham,UK,2284000,600
137,Manchester,UK,2245000,558
138,Leeds/Bradford,UK,1499000,370
139,Glasgow,UK,1200000,368
140,Donetsk,Ukraine,1400000,451
141,New York Metro,USA,17800000,8683
142,Chicago,USA,8308000,5498
143,Atlanta,USA,3500000,5083
144,Philadelphia,USA,5149000,4661
145,Boston,USA,4032000,4497
146,Los Angeles,USA,11789000,4320
147,Dallas/Fort Worth,USA,4146000,3644
148,Houston,USA,3823000,3355
149,Detroit,USA,3903000,3267
150,Washington,USA,3934000,2996
151,Miami,USA,4919000,2891
152,Seattle,USA,2712000,2470
153,Minneapolis/St. Paul,USA,2389000,2316
154,Pittsburgh,USA,1753000,2208
155,St. Louis,USA,2078000,2147
156,Tampa//St. Petersburg,USA,2062000,2078
157,Phoenix/Mesa,USA,2907000,2069
158,San Diego,USA,2674000,2026
159,Baltimore,USA,2076000,1768
160,Cincinnati,USA,1503000,1740
161,Cleveland,USA,1787000,1676
162,Kansas City,USA,1362000,1514
163,Indianapolis,USA,1219000,1432
164,San Francisco//Oakland,USA,3229000,1365
165,Virginia Beach,USA,1394000,1364
166,Providence,USA,1175000,1304
167,Denver,USA,1985000,1292
168,Milwaukee,USA,1309000,1261
169,Portland,USA,1583000,1228
170,Hartford,USA,852000,1216
171,Bridgeport//Stamford,USA,889000,1205
172,Orlando,USA,1157000,1174
173,Riverside/San Bernardino,USA,1507000,1136
174,Richmond,USA,819000,1131
175,Charlotte,USA,759000,1126
176,Nashville,USA,750000,1116
177,Jacksonville,USA,882000,1063
178,San Antonio,USA,1328000,1056
179,Memphis,USA,972000,1036
180,Columbus,USA,1133000,1030
182,Louisville,USA,864000,1013
183,Sacramento,USA,1393000,956
184,Buffalo,USA,977000,950
185,Knoxville,USA,420000,879
186,Dayton,USA,703000,838
187,Oklahoma City,USA,747000,835
188,Raleigh,USA,542000,828
189,Austin,USA,902000,824
190,McAllen,USA,523000,813
191,Springfield,USA,574000,800
192,Akron,USA,570000,797
193,Rochester,USA,694000,764
194,Tucson,USA,720000,755
195,Chattanooga,USA,344000,751
196,Allentown/Bethlehem,USA,576000,750
197,Barnstable Town,USA,244000,741
198,Las Vegas,USA,1314000,741
199,New Haven,USA,531000,739
200,Albany,USA,559000,736
201,Baton Rouge,USA,479000,727
202,Sarasota//Bradenton,USA,559000,700
203,Columbia,USA,421000,697
204,Poughkeepsie,USA,352000,686
205,Tulsa,USA,558000,677
206,San Jose,USA,1538000,674
207,Grand Rapids,USA,539000,667
208,Winston/Salem,USA,299000,651
209,Worcester,USA,430000,648
210,Augusta,USA,336000,600
211,Flint,USA,365000,599
212,Charleston,USA,423000,598
213,Salt Lake City,USA,888000,598
214,Youngstown,USA,417000,591
215,Greenville,USA,302000,587
216,Omaha,USA,627000,586
217,Albuquerque,USA,598000,580
218,Palm Bay,USA,393000,569
219,El Paso,USA,675000,568
220,Pensacola,USA,324000,568
221,Hickory,USA,188000,546
222,Mobile,USA,318000,546
223,Harrisburg,USA,363000,540
224,Asheville,USA,222000,536
225,Little Rock,USA,360000,532
226,Toledo,USA,503000,524
227,Lancaster,USA,324000,517
228,New Orleans,USA,1009000,512
229,Colorado Springs,USA,466000,511
230,Cape Coral,USA,330000,497
231,Ogden,USA,418000,466
232,Syracuse,USA,402000,465
233,Wichita,USA,422000,465
234,Concord,USA,553000,457
235,Port St Lucie,USA,271000,438
236,Fayetteville,USA,276000,433
237,Jackson,USA,293000,417
238,Scranton,USA,385000,411
239,Huntsville,USA,213000,407
240,Durham,USA,288000,406
241,South Bend,USA,276000,404
242,Shreveport,USA,275000,401
243,Honolulu,USA,718000,399
244,Bonita Springs / Naples,USA,221000,389
245,Canton,USA,267000,372
246,Spokane,USA,335000,371
247,Tashkent,Uzbekistan,2200000,531
248,Ho Chi Minh City,Vietnam,4900000,518
249,Harare,Zimbabwe,1750000,712


# 질문 1
익중이는 누나에게 여행지를 추천 받으려고 합니다.
그런데 나라 이름이 기억나지 않고, 이 데이터에서 그 나라에 속한 도시가 딱 4개 들어 있었다는 것만 기억이 난다고 하네요. 이 나라는 무엇일까요?

답안은 데이터에 적힌 이름 그대로 적어 주세요. (예시: France)

Italy
👏🏻 정답입니다.

# 퀴즈 해설

먼저, world_cities.csv 파일을 불러 옵니다.
이 데이터의 로우 하나하나는 각 도시의 정보를 담고 있는데요. Country 컬럼을 보면 이 도시가 어떤 나라에 속한 도시인지 알 수 있습니다.


import pandas as pd

world_cities = pd.read_csv("data/world_cities.csv")

world_cities['Country']

0       Argentina
1       Australia
2       Australia
3       Australia
4       Australia
5       Australia
6       Australia
7         Austria
8      Azerbaijan
9         Belgium
10        Belgium
          ...    
220           USA
221           USA
한 번만 등장하는 국가도 있고, 여러 번 등장하는 국가도 있네요. 여기서 익중이가 추천받은 나라를 찾으려면, 각 국가가 데이터에 몇 번씩 등장하는지 확인해 보면 되겠죠?


countries = world_cities['Country'].value_counts()

USA             106
France           15
Brazil           10
Canada            9
               ... 
Vietnam           1
Sudan             1
이제 여기서 총 4번 등장한 나라를 찾으면 됩니다.


countries[countries == 4]

Italy    4
Name: Country, dtype: int64
Italy가 총 4번 등장한다고 되어 있네요. 다시 말해, 이 데이터에는 Italy에 속한 도시가 4번 등장한다고 볼 수 있죠. 즉, 정답은 Italy 입니다.



## 코드잇 대학교: 수강신청 준비하기

# 실습 설명
2,000명의 코드잇 대학교 학생들이 수강신청을 했습니다.

수강신청에는 다음 3개의 조건이 있습니다.

“information technology” 과목은 심화과목이라 1학년은 수강할 수 없습니다.
“commerce” 과목은 기초과목이고 많은 학생들이 듣는 수업이라 4학년은 수강할 수 없습니다.
수강생이 5명이 되지 않으면 강의는 폐강되어 수강할 수 없습니다.
기존 DataFrame에 “status”라는 이름의 column을 추가하고, 학생이 수강 가능한 상태이면 “allowed”, 수강 불가능한 상태이면 “not allowed”를 넣어주세요.

# 실습 결과
주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)

# 해설
총 세 개의 조건이 있습니다.

“information technology” 과목은 심화과목이라 1학년은 수강할 수 없습니다.
“commerce” 과목은 기초과목이고 많은 학생들이 듣는 수업이라 4학년은 수강할 수 없습니다.
수강생이 5명이 되지 않으면 강의는 폐강 되어 수강할 수 없습니다.
이 조건이 “status”라는 column에 “allowed” 또는 “not allowed” 형태로 저장되어야 합니다.

먼저 "status"라는 column을 만들고, 모든 데이터를 "allowed"로 채워봅시다.


import pandas as pd

df = pd.read_csv('data/enrolment_1.csv')

df["status"] = "allowed"
이제 여기에서 각 조건에 해당하는 경우, "not allowed"로 변경해 주면 됩니다.

# 조건 1: “information technology” 과목은 심화과목이라 1학년은 수강할 수 없습니다.
“information technology” 과목을 수강하는 모든 데이터를 boolean1에 저장하고, 1학년에 해당하는 모든 데이터를 boolean2에 저장합시다.


boolean1 = df["course name"] == "information technology"
boolean2 = df["year"] == 1
그리고 이 두 값에 & 불린 연산을 하면, 두 조건을 동시에 만족하는 데이터만 인덱싱할 수 있습니다.


df.loc[boolean1 & boolean2, "status"] = "not allowed"

# 조건 2: “commerce” 과목은 기초과목이고 많은 학생들이 듣는 수업이라 4학년은 수강할 수 없습니다.
“commerce” 과목을 수강하는 모든 데이터를 boolean3에 저장하고, 4학년에 해당하는 모든 데이터를 boolean4에 저장합시다.


boolean3 = df["course name"] == "commerce"
boolean4 = df["year"] == 4
그리고 이 두 값에 & 불린 연산을 하면, 두 조건을 동시에 만족하는 데이터만 인덱싱할 수 있습니다.


df.loc[boolean3 & boolean4, "status"] = "not allowed"

# 조건 3: 수강생이 5명이 되지 않으면 강의는 폐강되어 수강할 수 없습니다.
우선 status가 allowed인 데이터만 모아 봅시다.


allowed = df["status"] == "allowed"
value_counts()를 사용하면, 각 과목별 신청 인원을 확인할 수 있습니다.


course_counts = df.loc[allowed, "course name"].value_counts()
course_counts

arts                                      158
science                                   124
commerce                                  101
english                                    56
education                                  41
                                         ... 
chemstry                                    1
jmc                                         1
b.ed                                        1
pgdca                                       1
sciences                                    1
Name: course name, Length: 296, dtype: int64
각 과목별 신청 인원이 5 이하인 과목의 index만 골라서 리스트로 만들어줍니다.


closed_courses = list(course_counts[course_counts < 5].index)
print(closed_courses)

['refactoring', 'computer architecture', 'engg. & tech.', ... , 'computer science (lateral entry)', 'arts/science', 'pg.diploma', 'aqua culture']
이제 폐강 과목에 대한 리스트가 확보되었습니다.

for문을 통해 not allowed 문구를 넣어줍시다.


for course in closed_courses:
    df.loc[df["course name"] == course, "status"] = "not allowed"
이제 이 세 코드를 모두 합치면 우리가 원하는 결과를 얻을 수 있습니다.

# 모범 답안
import pandas as pd

df = pd.read_csv('data/enrolment_1.csv')
df["status"] = "allowed"

# 조건 1
boolean1 = df["course name"] == "information technology"
boolean2 = df["year"] == 1
df.loc[boolean1 & boolean2, "status"] = "not allowed"

# 조건 2
boolean3= df["course name"] == "commerce"
boolean4= df["year"] == 4
df.loc[boolean3& boolean4, "status"] = "not allowed"

# 조건 3
allowed = df["status"] == "allowed"
course_counts = df.loc[allowed, "course name"].value_counts()
closed_courses = list(course_counts[course_counts < 5].index)
for course in closed_courses:
    df.loc[df["course name"] == course, "status"] = "not allowed"

# 테스트 코드
df


## 코드잇 대학교: 강의실 배정하기 I

# 실습 설명
수강 신청이 완료되었습니다. 이제 각 과목을 수강하는 학생수에 따라 크기가 다른 강의실을 배치하려고 합니다.

강의실은 규모에 따라 “Auditorium”, “Large room”, “Medium room”, “Small room” 총 4가지 종류가 있습니다.

아래 조건에 따라 강의실 종류를 지정해 주세요.

80명 이상의 학생이 수강하는 과목은 “Auditorium”에서 진행됩니다.
40명 이상, 80명 미만의 학생이 수강하는 과목은 “Large room”에서 진행됩니다.
15명 이상, 40명 미만의 학생이 수강하는 과목은 “Medium room”에서 진행됩니다.
5명 이상, 15명 미만의 학생이 수강하는 과목은 “Small room”에서 진행됩니다.
폐강 등의 이유로 status가 “not allowed”인 수강생은 room assignment 또한 “not assigned”가 되어야 합니다.
실습 결과


주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)

# 해설
수강 인원에 따라 총 다섯 개의 조건이 있습니다.

80명 이상의 학생이 수강하는 과목은 “Auditorium”에서 진행됩니다.
40명 이상, 80명 미만의 학생이 수강하는 과목은 “Large room”에서 진행됩니다.
15명 이상, 40명 미만의 학생이 수강하는 과목은 “Medium room”에서 진행됩니다.
5명 이상, 15명 미만의 학생이 수강하는 과목은 “Small room”에서 진행됩니다.
폐강 등의 이유로 status가 “not allowed”인 수강생은 room assignment 또한 “not assigned”가 되어야 합니다.
먼저 각 과목의 수강 인원을 가져와야겠죠?

status가 "allowed"인 course들에 대해서 수강 인원을 가져와봅시다.


import pandas as pd

df = pd.read_csv('data/enrolment_2.csv')

# 과목별 인원 가져오기
allowed = df["status"] == "allowed"
course_counts = df.loc[allowed, "course name"].value_counts()
course_counts

arts                                         158
science                                      124
commerce                                     101
english                                       56
                                            ... 
mca                                            5
interior design                                5
building construction and mangement            5
nanotechnology                                 5
Name: course name, Length: 82, dtype: int64
이제 각 과목별로 수강 인원을 확인할 수 있습니다.
강의실 규모에 따라 각 과목을 리스트로 만들어 줍시다.


# 각 강의실 규모에 해당되는 과목 리스트 만들기
auditorium_list = list(course_counts[course_counts >= 80].index)
large_room_list = list(course_counts[(80 > course_counts) & (course_counts >= 40)].index)
medium_room_list = list(course_counts[(40 > course_counts) & (course_counts >= 15)].index)
small_room_list = list(course_counts[(15 > course_counts) & (course_counts > 4)].index)
이 리스트를 활용해서 for문으로 room assignment 값을 정해줍니다.


# not allowed 과목에 대해 값 지정해주기
not_allowed = df["status"] == "not allowed"
df.loc[not_allowed, "room assignment"] = "not assigned"

# allowed 과목에 대해 값 지정해주기
for course in auditorium_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Auditorium"

for course in large_room_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Large room"
    
for course in medium_room_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Medium room"
    
for course in small_room_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Small room"
이제 코드가 완성되었습니다!

# 모범 답안

import pandas as pd

df = pd.read_csv('data/enrolment_2.csv')

# 과목별 인원 가져오기
allowed = df["status"] == "allowed"
course_counts = df.loc[allowed, "course name"].value_counts()

# 각 강의실 규모에 해당되는 과목 리스트 만들기
auditorium_list = list(course_counts[course_counts >= 80].index)
large_room_list = list(course_counts[(80 > course_counts) & (course_counts >= 40)].index)
medium_room_list = list(course_counts[(40 > course_counts) & (course_counts >= 15)].index)
small_room_list = list(course_counts[(15 > course_counts) & (course_counts > 4)].index)

# not allowed 과목에 대해 값 지정해주기
not_allowed = df["status"] == "not allowed"
df.loc[not_allowed, "room assignment"] = "not assigned"

# allowed 과목에 대해 값 지정해주기
for course in auditorium_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Auditorium"

for course in large_room_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Large room"
    
for course in medium_room_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Medium room"
    
for course in small_room_list:
    df.loc[(df["course name"] == course) & allowed, "room assignment"] = "Small room"
    
# 정답 출력
df


## 코드잇 대학교: 강의실 배정하기 II

# 실습 설명
이전 과제에서 강의실 크기에 따라 room assignment column을 만들어 주었습니다.

이제 이 room assignment에 따라 강의실 이름을 붙여주려고 합니다.

아래 세 가지 조건을 만족하도록 코드를 작성하세요.

같은 크기의 강의실이 필요한 과목에 대해 알파벳 순서대로 방 번호를 배정하세요.

예를 들어 "Auditorium"이 필요한 과목으로 “arts”, “commerce”, “science” 세 과목이 있다면, “arts”는 “Auditorium-1”, “commerce”는 “Auditorium-2”, “science”는 “Auditorium-3” 순서로 방 배정이 되어야 합니다.

방 번호에 room 은 포함되지 않습니다. 아래 스크린샷을 참고하여 작성해주세요.

status column이 “not allowed”인 수강생은 room assignment column을 그대로 “not assigned”로 남겨둡니다. "not allowed" 인 수강생의 room assignment 상태가 변경되지 않도록 유의해주세요.

room assignment column의 이름을 room number로 바꿔주세요.

# 해설
아래 세 가지 조건을 만족해야 합니다.

같은 크기의 강의실이 필요한 과목에 대해 알파벳 순서대로 방 번호를 배정하세요. 예를 들어 Auditorium이 필요한 과목으로 “arts”, “commerce”, “science” 세 과목이 있다면, “arts”는 “Auditorium-1”, “commerce”는 “Auditorium-2”, “science”는 “Auditorium-3” 순서로 방 배정이 되어야 합니다.

“status”가 “not allowed”인 수강생은 “room assignment”를 그대로 “not assigned”로 남겨둡니다.

“room assignment” column의 이름을 “room number”로 바꿔주세요.

먼저, 앞선 과제와 같이 status가 allowed인 수강생들의 과목별 수강 인원을 받아옵니다.


import pandas as pd

df = pd.read_csv('data/enrolment_3.csv')

# 과목별 인원 가져오기
allowed = df["status"] == "allowed"
course_counts = df.loc[allowed, "course name"].value_counts()
그리고 이를 통해 각 강의실 규모에 해당되는 과목 리스트를 만들어 줍니다.


# 각 강의실 규모에 해당되는 과목 리스트 만들기
auditorium_list = list(course_counts[course_counts >= 80].index)
large_room_list = list(course_counts[(80 > course_counts) & (course_counts >= 40)].index)
medium_room_list = list(course_counts[(40 > course_counts) & (course_counts >= 15)].index)
small_room_list = list(course_counts[(15 > course_counts) & (course_counts > 4)].index)
이렇게 만들어진 리스트를 활용해서, 각 강의실의 이름을 붙여줍니다.


# 강의실 이름 붙이기
for i in range(len(auditorium_list)):
    df.loc[(df["course name"] == sorted(auditorium_list)[i]) & allowed, "room assignment"] = "Auditorium-" + str(i + 1)

for i in range(len(large_room_list)):
    df.loc[(df["course name"] == sorted(large_room_list)[i]) & allowed, "room assignment"] = "Large-" + str(i + 1)
    
for i in range(len(medium_room_list)):
    df.loc[(df["course name"] == sorted(medium_room_list)[i]) & allowed, "room assignment"] = "Medium-" + str(i + 1)
    
for i in range(len(small_room_list)):
    df.loc[(df["course name"] == sorted(small_room_list)[i]) & allowed, "room assignment"] = "Small-" + str(i + 1)
마지막으로 column 이름을 바꿔줍니다.


# column 이름 바꾸기
df.rename(columns={"room assignment": "room number"}, inplace = True)
코드가 완성되었습니다!

# 모범 답안

import pandas as pd

df = pd.read_csv('data/enrolment_3.csv')

# 과목별 인원 가져오기
allowed = df["status"] == "allowed"
course_counts = df.loc[allowed, "course name"].value_counts()

# 각 강의실 규모에 해당되는 과목 리스트 만들기
auditorium_list = list(course_counts[course_counts >= 80].index)
large_room_list = list(course_counts[(80 > course_counts) & (course_counts >= 40)].index)
medium_room_list = list(course_counts[(40 > course_counts) & (course_counts >= 15)].index)
small_room_list = list(course_counts[(15 > course_counts) & (course_counts > 4)].index)

# 강의실 이름 붙이기
for i in range(len(auditorium_list)):
    df.loc[(df["course name"] == sorted(auditorium_list)[i]) & allowed, "room assignment"] = "Auditorium-" + str(i + 1)

for i in range(len(large_room_list)):
    df.loc[(df["course name"] == sorted(large_room_list)[i]) & allowed, "room assignment"] = "Large-" + str(i + 1)
    
for i in range(len(medium_room_list)):
    df.loc[(df["course name"] == sorted(medium_room_list)[i]) & allowed, "room assignment"] = "Medium-" + str(i + 1)
    
for i in range(len(small_room_list)):
    df.loc[(df["course name"] == sorted(small_room_list)[i]) & allowed, "room assignment"] = "Small-" + str(i + 1)

# column 이름 바꾸기
df.rename(columns={"room assignment": "room number"}, inplace = True)
    
# 테스트 코드
df


## DataFrame 다루기 복습 해야 할 부분

03. 여행지 선정하기 I

04. 여행지 선정하기 II

05. 코드잇 대학교: 수강신청 준비하기

06. 코드잇 대학교: 강의실 배정하기 I

07. 코드잇 대학교: 강의실 배정하기 II



## 데이터 사이언스

def is_palindrome(word):
    for left in range(len(word) // 2):
        right = len(word) - left -1
        if word[left] != word[right]:
            return False
    return True
# 테스트 코드
print(is_palindrome("racecar"))
print(is_palindrome("stars"))
print(is_palindrome("토마토"))
print(is_palindrome("kayak"))
print(is_palindrome("hello"))


## 개발 환경 설정하기

Integrated Development Environment

C:\Users\codeit> pip install numpy  
C:\Users\codeit> pip install pandas  
C:\Users\codeit> pip install matplotlib  
C:\Users\codeit> pip install seaborn  
C:\Users\codeit> pip install beautifulsoup4  
C:\Users\codeit> pip install selenium

C:\Users\codeit> pip install jupyter
C:\Users\codeit> jupyter notebook

## Jupyter Notebook 사용법
a : 이전 cell 생성
b : 이후 cell 생성
dd : cell 삭제

# mark down // 글 첨삭하기

단축키
# 가장 큰 제목
## 두 번째 큰 제목
### 세 번째 큰 제목
그냥 텍스트 작성

**두껍게**
*기울이기*

* 제목 // 점으로 목록 만들기


## Markdown 정리

아래 사이트들은 Markdown을 실시간으로 사용해볼 수 있는 온라인 서비스입니다.

Dillinger https://dillinger.io/

Stackedit  https://stackedit.io/app

하나씩 배울 때마다 직접 연습해 보세요.

제목

#를 사용하여 단계별로 제목을 표현할 수 있습니다.

#을 하나만 쓰면 1단계 제목이고, # 을 4개 사용하면 4단계 제목이 됩니다. 단계가 높아질수록 글자 크기가 작아집니다.

# 이것은 1단계 제목입니다.
## 이것은 2단계 제목입니다.
### 이것은 3단계 제목입니다.
#### 이것은 4단계 제목입니다.

이것은 일반 글입니다.
이것은 1단계 제목입니다.
이것은 2단계 제목입니다.
이것은 3단계 제목입니다.
이것은 4단계 제목입니다.
이것은 일반 글입니다.

# 번호가 있는 목록
아래처럼 적으면 번호가 있는 목록(리스트)을 만들 수 있습니다.

1. 첫 번째 할일입니다. 
2. 두 번째 할일입니다.
3. 세 번째 할일입니다.

첫 번째 할일입니다.
두 번째 할일입니다.
세 번째 할일입니다.

# 번호가 없는 목록
숫자 대신 *을 사용하면, 번호가 없는 목록을 만들 수도 있습니다.


* 번호가 없는 목록입니다.
* 번호가 없는 목록입니다.
* 번호가 없는 목록입니다.

번호가 없는 목록입니다.
번호가 없는 목록입니다.
번호가 없는 목록입니다.

# 줄 바꿈
아래와 같이 입력해 봅시다.

경찰청 철창살은 외철창살이냐 쌍철창살이냐?
내가 그린 기린 그림은 잘 그린 기린 그림이고 네가 그린 기린 그림은 잘 못 그린 기린 그림이다.
신인 샹송 가수의 신춘 샹송 쇼.
경찰청 철창살은 외철창살이냐 쌍철창살이냐?내가 그린 기린 그림은 잘 그린 기린 그림이고 네가 그린 기린 그림은 잘 못 그린 기린 그림이다.신인 샹송 가수의 신춘 샹송 쇼.

그런데 실행된 값을 보면 줄 바꿈이 안 되어 있죠? 줄 바꾸기를 하려면 문장의 맨 끝에 Space 키를 두 번 눌러야 합니다.

경찰청 철창살은 외철창살이냐 쌍철창살이냐?vv
내가 그린 기린 그림은 잘 그린 기린 그림이고 네가 그린 기린 그림은 잘 못 그린 기린 그림이다.vv
신인 샹송 가수의 신춘 샹송 쇼.
경찰청 철창살은 외철창살이냐 쌍철창살이냐?  
내가 그린 기린 그림은 잘 그린 기린 그림이고 네가 그린 기린 그림은 잘 못 그린 기린 그림이다.  
신인 샹송 가수의 신춘 샹송 쇼.

# 문단 바꿈
문단을 바꾸는 효과를 위해서는 Enter 키를 두 번 눌러 주세요.

경찰청 철창살은 외철창살이냐 쌍철창살이냐?

내가 그린 기린 그림은 잘 그린 기린 그림이고 네가 그린 기린 그림은 잘 못 그린 기린 그림이다.

신인 샹송 가수의 신춘 샹송 쇼.
경찰청 철창살은 외철창살이냐 쌍철창살이냐?

내가 그린 기린 그림은 잘 그린 기린 그림이고 네가 그린 기린 그림은 잘 못 그린 기린 그림이다.

신인 샹송 가수의 신춘 샹송 쇼.

# 문자 강조
*와 _를 사용하면 글자를 꾸밀 수 있습니다.


이탈릭체는 *별표*나 _언더바_로.

볼드체는 **별표 두 개**나 __언더바 두 개__로.

**별표와 _언더바_**를 섞어서.

취소선은 ~~물결~~로.
이탈릭체는 별표나 언더바로.

볼드체는 별표 두 개나 언더바 두 개로.

별표와 언더바를 섞어서.

취소선은 물결로.

# 링크
어떤 사이트로의 링크를 걸 수도 있습니다.


코딩 강의는 역시 [코드잇](https://www.codeit.kr)이죠!
코딩 강의는 역시 코드잇이죠!

# 이미지
이미지를 추가할 수도 있습니다.


![코드잇](https://www.codeit.kr/static/images/brand/logo_original.png) 
코드잇

# 코드 블록
 ` 세 개로 “코드 블록”을 추가해 보세요. 언어도 지정해 줄 수 있습니다!


```python
print('hello world!')
for i in range(10):
    print(i)
```

print('hello world!')
for i in range(10):
    print(i)


# 인라인 코드
``로 코드를 감싸면 “인라인 코드”를 쓸 수 있습니다.


값을 출력하고 싶으면 `print` 함수를 사용하면 되겠죠?
값을 출력하고 싶으면 print 함수를 사용하면 되겠죠?

# 구분선
---으로 구분선을 추가할 수 있습니다.

구분 선 위
---
구분 선 아래


## 스크림 로스터
import pandas as pd
df = pd.read_csv('C:/Users/MisterHaru/AppData/Local/TslGame/Saved/observer/TeamInfo.csv')



## Numerical Python // 숫자와 관련한 파이썬 도구(Numpy)

# Numpy 배열( Numpy array)


## numpy array를 만드는 다양한 방법
# 파이썬 리스트를 통해 생성
numpy 모듈의 array 메소드에 파라미터로 파이썬 리스트를 넘겨주면 numpy array가 리턴됩니다.


array1 = numpy.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31])
    
print(array1)

[ 2  3  5  7 11 13 17 19 23 29 31]

# 균일한 값으로 생성
numpy 모듈의 full 메소드를 사용하면, 모든 값이 같은 numpy array를 생성할 수 있습니다.

array1 = numpy.full(6, 7)
    
print(array1)

[7 7 7 7 7 7]

# 모든 값이 0인 numpy array 생성
모든 값이 0인 numpy array를 생성하기 위해서는 full 메소드를 사용하면 되겠죠. 하지만 사실은 더 간편한 방법이 있습니다.


array1 = numpy.full(6, 0)
array2 = numpy.zeros(6, dtype=int)
    
print(array1)
print()
print(array2)

[0 0 0 0 0 0]

[0 0 0 0 0 0]

# 모든 값이 1인 numpy array 생성

모든 값이 1인 numpy array를 생성하는 것도 비슷합니다. zeros 메소드 대신 ones를 사용하면 됩니다.


array1 = numpy.full(6, 1)
array2 = numpy.ones(6, dtype=int)
    
print(array1)
print()
print(array2)

[1 1 1 1 1 1]

[1 1 1 1 1 1]

# 랜덤한 값들로 생성
어쩔 때는 임의의 값들로 배열을 생성시키고 싶습니다. 그럴 때는 numpy의 random 모듈의 random 함수를 사용하면 됩니다.

numpy 모듈 안에 random이라는 모듈이 있고, 그 안에 또 random이라는 함수가 있는 겁니다!


array1 = numpy.random.random(6)
array2 = numpy.random.random(6)
    
print(array1)
print()
print(array2)

[0.42214929 0.45275673 0.57978413 0.61417065 0.39448558 0.03347601]

[0.42521953 0.65091589 0.94045742 0.18138103 0.27150749 0.8450694 ]

# 연속된 값들이 담긴 numpy array 생성
numpy 모듈의 arange 함수를 사용하면 연속된 값들이 담겨 있는 numpy array를 생성할 수 있습니다.

arange 함수는 파이썬의 기본 함수인 range와 굉장히 비슷한 원리로 동작하는데요. 파라미터가 1개인 경우, 2개인 경우, 3개인 경우 모두 살펴봅시다.

# 파라미터 1개
arange(m)을 하면 0부터 m-1까지의 값들이 담긴 numpy array가 리턴됩니다.


array1 = numpy.arange(6)
print(array1)

[0 1 2 3 4 5]

# 파라미터 2개
arange(n, m)을 하면 n부터 m-1까지의 값들이 담긴 numpy array가 리턴됩니다.

array1 = numpy.arange(2, 7)
print(array1)

[2 3 4 5 6]

# 파라미터 3개
arange(n, m, s)를 하면 n부터 m-1까지의 값들 중 간격이 s인 값들이 담긴 numpy array가 리턴됩니다.


array1 = numpy.arange(3, 17, 3)
print(array1)

[ 3  6  9 12 15]


## 모듈 별명 지어주기

import numpy as np

import matplotlib.pyplot as plt

# 파이썬 연산으로 할 때
for i in range(len(array1)):
    array1[i] = 2 * array1[i]

# Numpy로 연산 할 때
# array1 = array1 * 2 // 값을 지정해 줘야 데이터가 변경 된다.
array1 * 2
array1 / 2
array1 + 2
array1 ** 2

array1 * array2
array1 + array2
array1 / array2




##  신주쿠 흥부부대 찌개
실습 설명
일본에는 한식 열풍이 불고 있습니다. 기회를 엿본 영훈이는 대기업을 퇴사하고 신주쿠에 프랜차이즈 ‘흥부부대찌개’ 가맹점을 냈습니다.

그러나 보수적인 아버지께서는 번듯한 직장을 박차고 나온 영훈이가 못마땅합니다. 아버지를 안심시켜 드리기 위해 매달 매출을 보고하려고 하는데요. 엔화(¥)로 저장한 매출 데이터를 원화(₩)로 변환하는 작업이 필요합니다.

마침 numpy를 배운 우리가 도와줄 수 있겠네요. 엔화 매출이 담겨 있는 파이썬 리스트가 주어졌습니다. 1엔에 10.08원이라고 가정하고, 원화 매출이 담긴 numpy array를 만들어 출력해 주세요.

반복문은 사용하면 안 됩니다!

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print가 없는 방식으로 작성해 주세요. (예시: won_array)

해설 보기

해설
revenue_in_yen에 엔화(¥) 매출 데이터가 담겨 있습니다.


import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]
1엔이 10.08원이라고 가정했으니까, 우리는 이 모든 숫자들에 10.08을 곱해주면 되겠죠.

numpy array를 사용하면, 모든 데이터에 같은 숫자를 한 번에 곱해줄 수 있습니다.

먼저 주어진 리스트를 numpy array로 만듭니다.


yen_array = np.array(revenue_in_yen)
이제 이 numpy array에는 곱셈 연산을 적용할 수 있습니다.


won_array = yen_array * 10.08
won_array # 정답 출력

array([ 3024000.,  3427200.,  3225600.,  3628800.,  4435200.,  1411200.,
        1814400.,  3427200.,  3326400.,  2923200.,  2822400.,  3830400.,
        1713600.,  1411200.,  2318400.,  3931200.,  4032000.,  3528000.,
        3830400.,  1512000.,  1108800.,  2419200.,  3830400.,  3830400.,
        3427200.,  4233600.,  1512000.,  1310400.,  3628800.,  3225600.,
        2520000.])
이제 원화(₩) 데이터를 얻었습니다!


# 하루 정답
import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]

# 여기에 코드를 작성하세요
won = 10.08
won_array = np.array(revenue_in_yen) * won

# 테스트 코드
won_array


## 흥부부대찌개 LA 진출

# 실습 설명
흥부부대찌개 신주쿠점의 흥행에 성공한 영훈이는 여세를 몰아 LA에도 가맹점을 하나 냈습니다.

이제 아버지께 매출을 보고하기 위한 프로세스가 조금 복잡해졌습니다. 각 지점의 매출을 원화로 변환시키고 더해야 하죠. 1엔에 10.08원, 1달러에 1138원이라고 가정하세요. 그리고 두 지점의 매출의 합이 원화로 담긴 numpy array를 만들어 출력해주세요.

반복문은 사용하면 안 됩니다!

주의: 자동 채점 과제입니다. 정답 출력 코드는 print가 없는 방식으로 작성해 주세요. (예시: won_array)

해설 보기

해설
revenue_in_yen에 엔화(¥) 매출 데이터가, revenue_in_dollar에 달러($) 매출 데이터가 담겨 있습니다.


import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]

revenue_in_dollar = [
    1200, 1600, 1400, 1300, 
    2100, 1400, 1500, 2100, 
    1500, 1500, 2300, 2100, 
    2800, 2600, 1700, 1400, 
    2100, 2300, 1600, 1800, 
    2200, 2400, 2100, 2800, 
    1900, 2100, 1800, 2200, 
    2100, 1600, 1800
]
조금 복잡하지만, 앞선 과제와 원리는 같습니다.

원화로 계산하기 위해서는 엔화 데이터에 10.08을, 달러 데이터에 1138을 곱해주어야 합니다.

리스트에 있는 모든 값에 곱셈을 동시에 해주기 위해, 엔화 데이터 revenue_in_yen와 달러 데이터 revenue_in_dollar 모두 numpy array로 만들어 줍시다.


yen_array = np.array(revenue_in_yen)
dollar_array = np.array(revenue_in_dollar)
이제 이 numpy array에 각 환율을 곱한 뒤, 함께 더해주면 원화를 구할 수 있습니다.


won_array = yen_array * 10.08 + dollar_array * 1138
won_array # 테스트 코드 

array([ 4389600.,  5248000.,  4818800.,  5108200.,  6825000.,  3004400.,
        3521400.,  5817000.,  5033400.,  4630200.,  5439800.,  6220200.,
        4900000.,  4370000.,  4253000.,  5524400.,  6421800.,  6145400.,
        5651200.,  3560400.,  3612400.,  5150400.,  6220200.,  7016800.,
        5589400.,  6623400.,  3560400.,  3814000.,  6018600.,  5046400.,
        4568400.])


# 하루 정답
import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]

revenue_in_dollar = [
    1200, 1600, 1400, 1300, 
    2100, 1400, 1500, 2100, 
    1500, 1500, 2300, 2100, 
    2800, 2600, 1700, 1400, 
    2100, 2300, 1600, 1800, 
    2200, 2400, 2100, 2800, 
    1900, 2100, 1800, 2200, 
    2100, 1600, 1800
]

# 여기에 코드를 작성하세요
yen = np.array(revenue_in_yen)
dollar = np.array(revenue_in_dollar)

yen_won = yen * 10.08
dollar_won = dollar * 1138
won_array = yen_won + dollar_won
# 테스트 코드
won_array


b = np.array([])
np.where(b)

np.where(revenue_in_yen < 200000)


## 흥부부대찌개 목표 일 매출

# 실습 설명
영훈이가 창업한 흥부부대찌개 신주쿠점은 이제 직장인들에게 소문난 맛집입니다. 그러나 매일같이 성공적인 것은 아닙니다. 목표 일 매출을 달성하지 못하는 날들이 아직 꽤 있거든요. 영훈이가 생각하는 성공적인 하루 매출은 20만 엔입니다.

성공적이지 않은 날의 매출만 골라서 보고 싶습니다. 20만 엔 이하의 매출만 담긴 numpy array를 출력해주세요.
반복문은 사용하면 안 됩니다!

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: bad_days_revenue)

# 해설
numpy array의 좋은 점은 리스트 안의 데이터를 반복문 없이 한 번에 분석할 수 있다는 것입니다.

우리의 매출이 revenue_in_yen 이라는 변수에 리스트 형태로 들어 있습니다.


import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]
numpy의 도움을 받기 위해서는 numpy array로 만들어 주어야겠죠?


yen_array = np.array(revenue_in_yen)
그리고 주어진 조건에 해당하는 filter를 만들어 줍시다.


filter = np.where(yen_array <= 200000)
filter # 결과 확인

(array([ 5,  6, 12, 13, 19, 20, 26, 27]),)
우리가 원하는 데이터가 어디에 있는지 확인할 수 있네요.

이 filter를 인덱싱에 활용해주면, 우리가 원하는 결과를 얻을 수 있습니다.


bad_days_revenue = yen_array[filter]
bad_days_revenue # 정답 출력

array([140000, 180000, 170000, 140000, 150000, 110000, 150000, 130000])
정리하면 이런 코드가 되겠네요.


import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]

yen_array = np.array(revenue_in_yen)

filter = np.where(yen_array <= 200000)
bad_days_revenue = yen_array[filter]

bad_days_revenue # 정답 출력
아래와 같이 where 메소드로 filter를 만들지 않고 바로 인덱싱하는 방법도 있다는 것! 참고하세요.


import numpy as np

revenue_in_yen = [
    300000, 340000, 320000, 360000, 
    440000, 140000, 180000, 340000, 
    330000, 290000, 280000, 380000, 
    170000, 140000, 230000, 390000, 
    400000, 350000, 380000, 150000, 
    110000, 240000, 380000, 380000, 
    340000, 420000, 150000, 130000, 
    360000, 320000, 250000
]

yen_array = np.array(revenue_in_yen)

bad_days_revenue = yen_array[yen_array <= 200000]

bad_days_revenue # 정답 출력



## numpy array vs. python list


## numpy 기본 통계

numpy 라이브러리는 기본적인 통계 기능도 제공합니다.

# 최댓값, 최솟값
max 메소드와 min 메소드를 사용하면 numpy array의 최댓값과 최솟값을 구할 수 있습니다.


import numpy as np

array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5])

print(array1.max()) # 최댓값
print(array1.min()) # 최솟값

31
5

# 평균값
mean 메소드를 사용하면 numpy array의 평균값을 구할 수 있습니다.


import numpy as np

array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5])

print(array1.mean()) # 평균값

15.25
위 예시에서, 총합(14+6+13+21+23+31+9+5)을 총 개수(8)로 나누면 15.25 입니다.

# 중앙값
median 메소드를 사용하면 중간값을 구할 수 있는데요. 특이하게 median은 numpy array의 메소드가 아니라 numpy의 메소드입니다.


import numpy as np

array1 = np.array([8, 12, 9, 15, 16])
array2 = np.array([14, 6, 13, 21, 23, 31, 9, 5])

print(np.median(array1)) # 중앙값
print(np.median(array2)) # 중앙값

12.0
13.5
array1을 정렬하면 중앙값이 12 입니다.

array2에는 짝수개의 요소가 있기 때문에 중앙값이 13과 14 두 개입니다. 둘의 평균값을 내면 13.5 입니다.

# 표준 편차, 분산
표준 편차와 분산은 값들이 평균에서 얼마나 떨어져 있는지 나타내는 지표입니다. 잘 모르신다면 일단 넘어가셔도 좋습니다.


import numpy as np

array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5])

print(array1.std()) # 표준 편차
print(array1.var()) # 분산

8.496322733983215
72.1875


## Pandas
// 2차원 형태의 데이터를 다루기 위한 자료형 : Pandas의 DataFrame
표 형식 : 열 column, 행 Row / Index

# DataFrame을 만드는 다양한 방법

# From list of lists, array of arrays, list of series
2차원 리스트나 2차원 numpy array로 DataFrame을 만들 수 있습니다. 심지어 pandas Series를 담고 있는 리스트로도 DataFrame을 만들 수 있습니다.

따로 column과 row(index)에 대한 설정이 없으면 그냥 0, 1, 2, ... 순서로 값이 매겨집니다.


import numpy as np
import pandas as pd

two_dimensional_list = [['dongwook', 50, 86], ['sineui', 89, 31], ['ikjoong', 68, 91], ['yoonsoo', 88, 75]]
two_dimensional_array = np.array(two_dimensional_list)
list_of_series = [
    pd.Series(['dongwook', 50, 86]), 
    pd.Series(['sineui', 89, 31]), 
    pd.Series(['ikjoong', 68, 91]), 
    pd.Series(['yoonsoo', 88, 75])
]

# 아래 셋은 모두 동일합니다
df1 = pd.DataFrame(two_dimensional_list)
df2 = pd.DataFrame(two_dimensional_array)
df3 = pd.DataFrame(list_of_series)

print(df1)

          0   1   2
0  dongwook  50  86
1    sineui  89  31
2   ikjoong  68  91
3   yoonsoo  88  75

# From dict of lists, dict of arrays, dict of series
파이썬 사전(dictionary)으로도 DataFrame을 만들 수 있습니다.

사전의 key로는 column 이름을 쓰고, 그 column에 해당하는 리스트, numpy array, 혹은 pandas Series를 사전의 value로 넣어주면 됩니다.


import numpy as np
import pandas as pd

names = ['dongwook', 'sineui', 'ikjoong', 'yoonsoo']
english_scores = [50, 89, 68, 88]
math_scores = [86, 31, 91, 75]

dict1 = {
    'name': names, 
    'english_score': english_scores, 
    'math_score': math_scores
}

dict2 = {
    'name': np.array(names), 
    'english_score': np.array(english_scores), 
    'math_score': np.array(math_scores)
}

dict3 = {
    'name': pd.Series(names), 
    'english_score': pd.Series(english_scores), 
    'math_score': pd.Series(math_scores)
}


# 아래 셋은 모두 동일합니다
df1 = pd.DataFrame(dict1)
df2 = pd.DataFrame(dict2)
df3 = pd.DataFrame(dict3)

print(df1)

       name  english_score  math_score
0  dongwook             50          86
1    sineui             89          31
2   ikjoong             68          91
3   yoonsoo             88          75

# From list of dicts
리스트가 담긴 사전이 아니라, 사전이 담긴 리스트로도 DataFrame을 만들 수 있습니다.


import numpy as np
import pandas as pd

my_list = [
    {'name': 'dongwook', 'english_score': 50, 'math_score': 86},
    {'name': 'sineui', 'english_score': 89, 'math_score': 31},
    {'name': 'ikjoong', 'english_score': 68, 'math_score': 91},
    {'name': 'yoonsoo', 'english_score': 88, 'math_score': 75}
]

df = pd.DataFrame(my_list)
print(df)

   english_score  math_score      name
0             50          86  dongwook
1             89          31    sineui
2             68          91   ikjoong
3             88          75   yoonsoo



## 스타들의 생일은 언제?
# 실습 설명
정답 출력 코드는 print 없이 작성해 주세요. ( 예시: df)
아래와 같은 결과가 나오도록 DataFrame을 만들고 출력해 보세요.  
column은 name, birthday, occupation 총 3개입니다.

name	birthday	occupation
0	Taylor Swift	December 13, 1989	Singer-songwriter
1	Aaron Sorkin	June 9, 1961	Screenwriter
2	Harry Potter	July 31, 1980	Wizard
3	Ji-Sung Park	February 25, 1981	Footballer





힌트5/5

해설 보기

해설
1단계
우선 DataFrame에 들어갈 정보가 담긴 2차원 파이썬 리스트를 만들어 주세요.

2차원 파이썬 리스트는 이렇게 작성할 수 있습니다.


celebrities = [
    ['Taylor Swift', 'December 13, 1989', 'Singer-songwriter'],
    ['Aaron Sorkin', 'June 9, 1961', 'Screenwriter'],
    ['Harry Potter', 'July 31, 1980', 'Wizard'],
    ['Ji-Sung Park', 'February 25, 1981', 'Footballer']
]
2단계
이제 DataFrame을 생성해야겠죠? pandas 라이브러리의 DataFrame 함수(생성자)를 사용하면 됩니다.


df = pd.DataFrame(celebrities)
이렇게 DataFrame을 생성할 수 있는데요. Column 이름은 어떻게 붙여 줄 수 있을까요?

3단계
DataFrame 함수의 columns라는 파라미터로 리스트를 넘겨주면 됩니다.


import pandas as pd

celebrities = [
    ['Taylor Swift', 'December 13, 1989', 'Singer-songwriter'],
    ['Aaron Sorkin', 'June 9, 1961', 'Screenwriter'],
    ['Harry Potter', 'July 31, 1980', 'Wizard'],
    ['Ji-Sung Park', 'February 25, 1981', 'Footballer']
]

df = pd.DataFrame(celebrities, columns=['name', 'birthday', 'occupation'])
df # 테스트 코드



## pandas의 데이터 타입

# 
pandas DataFrame에는 다양한 종류의 데이터를 담을 수 있습니다. dtypes를 사용해서 각 column이 어떤 데이터 타입을 보관하는지 확인할 수 있는데요.


import pandas as pd

two_dimensional_list = [['dongwook', 50, 86], ['sineui', 89, 31], ['ikjoong', 68, 91], ['yoonsoo', 88, 75]]

my_df = pd.DataFrame(two_dimensional_list, columns=['name', 'english_score', 'math_score'], index=['a', 'b', 'c', 'd'])

print(my_df.dtypes)

name             object
english_score     int64
math_score        int64
dtype: object
위 경우 'name' column은 object라는 데이터 타입을 보관하고, 'english_score'와 'math_score' column은 int64라는 데이터 타입을 보관하는 거죠.

보시다시피 한 column 내에서는 모든 값이 동일한 데이터 타입입니다.

# pandas의 dtype들
pandas에 담을 수 있는 dtype(데이터 타입) 몇 가지를 살펴봅시다.

dtype	설명
int64	정수
float64	소수
object	텍스트
bool	불린(참과 거짓)
datetime64	날짜와 시간
category	카테고리


## CSV : Comma-Separated Values

## 

import pandasf as pd
pd.read_csv(파일경로) // 변수 = pd.read_csv('파일경로')

변수 = pd.read_csv('파일경로', header=None)  // csv 파일에 헤더가 없는 경우 

변수 = pd.read_csv('파일경로', index_col=0)  // 특정 컬럼을 인덱스 값으로 설정 가능하다.


## 가장 인기 있는 아기 이름은?
실습 설명
아기의 성별과 어머니의 인종에 따른, 뉴욕에서 가장 인기 있는 아기 이름이 무엇인지 조사를 해 봤습니다.

조사 결과가 data 폴더의 popular_baby_names.csv라는 파일에 담겨 있는데요. 안에 있는 정보를 DataFrame으로 읽어 들이고, DataFrame을 출력해 주세요.



주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)


힌트3/3

해설 보기

해설
CSV 파일을 DataFrame으로 읽어 들이기 위해서는 pandas 라이브러리의 read_csv 함수를 사용하면 됩니다.

read_csv에 파일 경로를 넘겨 줘야 하는데요. 파일 경로는 어떻게 써 줄 수 있을까요?

현재 폴더에 data라는 폴더가 있고, 그 안에 popular_baby_names.csv라는 파일이 있습니다. 그럼 경로를 'data/popular_baby_names.csv'라고 작성할 수 있겠네요.

코드로 표현하면 다음과 같습니다.


import pandas as pd

df = pd.read_csv('data/popular_baby_names.csv')
df # 테스트 코드


코드잇 사이트가 아니라 본인의 컴퓨터에서 직접 실행하는 경우, CSV 파일을 저장한 경로를 동일하게 넣어주면 됩니다.


import pandas as pd

df = pd.read_csv('컴퓨터에 저장한 CSV 파일 경로')
df # 테스트 코드


##  메가밀리언 로또 당첨 번호
# 실습 설명
‘메가밀리언’은 ‘파워볼’과 더불어 미국에서 양대산맥을 이루는 복권입니다. 당첨될 확률이 약 3억분의 1 정도로 굉장히 낮은 대신, 당첨시 금액이 어마어마하죠. 2018년에는 한 명이 무려 1.8조원을 가져가기도 했습니다.

메가밀리언 측에서 2002년부터 현재(2/15/19)까지의 당첨 번호가 담긴 mega_millions.csv 파일을 공개했는데요. 이 데이터를 DataFrame에 넣어 봅시다.

날짜(Draw Date)가 이 DataFrame의 인덱스가 되도록 해 주세요!

56c6gw2p7-4-9-1.png

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)


# 해설
메가밀리언 로또 당첨 번호가 저장된 CSV 파일을 읽어들이고 싶습니다.

CSV 파일을 DataFrame으로 읽어들이기 위해서는 pandas 라이브러리의 read_csv 함수를 사용하면 됩니다.

read_csv에 파일 경로를 넘겨 줘야 하는데요. 파일 경로는 어떻게 써 줄 수 있을까요?

현재 폴더에  data라는 폴더가 있고, 그 안에 mega_millions.csv 파일이 있습니다. 그럼 경로를 'data/mega_millions.csv'라고 작성할 수 있겠네요. 직접 읽어봅시다.


import pandas as pd

df = pd.read_csv('data/mega_millions.csv')
df # 테스트 코드


하지만 우리가 의도한 모양과는 조금 다르죠.

Draw Date를 인덱스로 지정하고 싶습니다.

파일을 읽어 들일 때 특정 column을 인덱스로 지정하고 싶으면, index_col 파라미터를 써 주면 됩니다.

Draw Date가 0번 column에 있으니, index_col=0 이라고 적어주면 됩니다.


import pandas as pd

df = pd.read_csv('data/mega_millions.csv', index_col=0)
df # 테스트 코드


이제 우리가 원하는 형태로 출력이 되었습니다.

몇 번째 Column인지 명확하지 않을 때는 숫자 0 대신 index_col='Draw Date' 라고 이름을 직접 적용해도 됩니다.


import pandas as pd

df = pd.read_csv('data/mega_millions.csv', index_col='Draw Date')
df # 테스트 코드


결과가 동일하게 출력됩니다.


############# 데이터 분석과 시각화  #######################

## 시각화의 두 가지 목적

# 이상점(Outlier)

## 선 그래프
matplotlib inline

import pandas as pd

df = pd.read_csv('data/broadcast.csv', index_col=0)
df

df.plot()    // 그래프를 표시한다.

df.plot(kind='line')    // kind 옵션은 기본이다. 작성하지 않아도 기본 적용

df.plot(y='KBS')
df.plot(y='KBS', 'JTBC')

df[['KBS', 'JTBC']]
df[['KBS', 'JTBC']].plot()

df['KBS']  // 1차원이지만 plot를 이용해서 그래프 표시
df['KBS'].plot()

# 숫자 데이터 일 경우 그래표 표시가 가능하지만 문자열인 경우 그래표 표시에 에러 발생


## 국가별 경제 성장
#실습 설명
주어진 데이터를 이용해서 한국(Korea_Rep), 미국(United_States), 영국(United_Kingdom), 독일(Germany), 중국(China), 일본(Japan)의 GDP 그래프를 그려 보세요.

# 해설
원하는 나라에 대해서만 그래프를 그리려면, y값으로 리스트를 넘겨주면 됩니다.


df.plot(y=["Korea_Rep", "United_States", "United_Kingdom", "Germany", "China", "Japan"])
모범 답안

%matplotlib inline
import pandas as pd

df = pd.read_csv('data/gdp.csv', index_col=0)

df.plot(y=["Korea_Rep", "United_States", "United_Kingdom", "Germany", "China", "Japan"])

# 하루 정답
df[['Korea_Rep', 'United_States', 'United_Kingdom', 'Germany', 'China', 'Japan']].plot()


## 그래프 선 색상 변경하기
%matplotlib inline
import pandas as pd

df = pd.read_csv('data/gdp.csv',index_col=0)
countries = ["Korea_Rep", "United_States", "United_Kingdom", "Germany", "China", "Japan"]

# 각 열에 대한 색상 지정하여 그래프 그리기
df.plot(y=countries, color=["blue", "green", "orange", "red", "purple", "pink"])


## 막대 그래프
df.plot(kind='barh')

df.plot(kind='bar', stacked=True)

df['Female'].plot(kind='bar')



## 실리콘 밸리에는 누가 일할까? I

data = df[(df['job_category'] == 'Managers') & (df['gender'] == 'Male') & (df['race_ethnicity'] != 'All')]
data.plot(kind='bar', x='race_ethnicity', y='count')


## 실습 설명
실리콘 밸리에서 일하는 사람들의 정보가 있습니다.

직업 종류, 인종, 성별 등이 포함되어 있는데요.

실리콘 밸리에서 일하는 남자 관리자 (Managers)에 대한 인종 분포를 막대 그래프로 다음과 같이 그려보세요.

# 해설
우선 데이터 내용을 확인해 봅시다.


%matplotlib inline
import pandas as pd

df = pd.read_csv('data/silicon_valley_summary.csv')
df


우리는 관리자 (Manager) 직군의 남자에 대한 그래프를 그리려고 합니다.

따라서 'job_category'는 'Manager'이고, 'gender'는 'Male'인 데이터만 뽑아봅시다.


boolean_male = df['gender']=='Male'
boolean_manager = df['job_category'] == 'Managers'

df[boolean_male & boolean_manager]


각 인종에 대한 데이터만 그래프로 그리고 싶으니, 'race_ethnicity'가 'All'인 경우는 제외해야겠죠?


boolean_male = df['gender']=='Male'
boolean_manager = df['job_category'] == 'Managers'
boolean_not_all = df['race_ethnicity'] != 'All'

df[boolean_male & boolean_manager & boolean_not_all]


이제 이 데이터를 plot 메소드를 이용해서 그래프로 그려주면 됩니다.


df[boolean_male & boolean_manager & boolean_not_all].plot(kind='bar', x='race_ethnicity',  y='count')


# 모범 답안

%matplotlib inline
import pandas as pd

df = pd.read_csv('data/silicon_valley_summary.csv')
boolean_male = df['gender']=='Male'
boolean_manager = df['job_category'] == 'Managers'
boolean_not_all = df['race_ethnicity'] != 'All'

df[boolean_male & boolean_manager & boolean_not_all].plot(kind='bar', x='race_ethnicity',  y='count')

## 하루 정답
%matplotlib inline
import pandas as pd

df = pd.read_csv('data/silicon_valley_summary.csv')

# 여기에 코드를 작성하세요
data = df[(df['job_category'] == 'Managers') & (df['gender'] == 'Male') & (df['race_ethnicity'] != 'All')]
data.plot(kind='bar', x='race_ethnicity', y='count')



## 파이 그래프
// 원형의 비율 그래프

df.loc[2017].plot(kind='pie')



## 실리콘 밸리에는 누가 일할까?
# 실습 설명
이번에는 어도비 (Adobe)의 직원 분포를 한번 살펴봅시다.

어도비 전체 직원들의 직군 분포를 파이 그래프로 그려보세요.

(인원이 0인 직군은 그래프에 표시되지 않아야 합니다.)


# 해설
먼저 주어진 데이터를 살펴봅시다.


%matplotlib inline
import pandas as pd

df = pd.read_csv("data/silicon_valley_details.csv")
df


이제 여기에서 회사 'company'는 'Adobe', 인종 'race' 는 'Overall_totals'인 데이터만 골라내 봅시다.


boolean_adobe = df['company'] == 'Adobe'
boolean_all_races = df['race'] == 'Overall_totals'
df[boolean_adobe & boolean_all_races]


좀 더 깔끔하게 정리하기 위해서, 'count'가 0인 데이터는 제거하고, 'job_category'가 'Totals' 혹은 'Previous_totals' 인 데이터도 제거합시다.


boolean_adobe = df['company'] == 'Adobe'
boolean_all_races = df['race'] == 'Overall_totals'
boolean_count = df['count'] != 0
boolean_job_category = (df['job_category'] != 'Totals') & (df['job_category'] != 'Previous_totals')

df_adobe = df[boolean_adobe & boolean_all_races & boolean_count & boolean_job_category]
df_adobe


이제 그래프를 그려봅시다.


df_adobe.plot(kind='pie', y= 'count')


그런데 그래프가 조금 이상하죠?

파이 그래프는 index를 기준으로 이름표를 붙여주게 됩니다.

우리가 원하는 이름표는 직업 카테고리이니까, set_index를 활용해서 index를 바꿔줍시다.


df_adobe.set_index('job_category', inplace=True)


이제 plot 메소드로 그래프를 그려주면 됩니다.


df_adobe.plot(kind='pie', y= 'count')


# 모범 답안

%matplotlib inline
import pandas as pd

df = pd.read_csv("data/silicon_valley_details.csv")

boolean_adobe = df['company'] == 'Adobe'
boolean_all_races = df['race'] == 'Overall_totals'
boolean_count = df['count'] != 0
boolean_job_category = (df['job_category'] != 'Totals') & (df['job_category'] != 'Previous_totals')

df_adobe = df[boolean_adobe & boolean_all_races & boolean_count & boolean_job_category]
df_adobe.set_index('job_category', inplace=True)
df_adobe.plot(kind='pie', y= 'count')



## 히스토그램
구간으로 범위를 정해서 그래프를 표시

df.plot(kind='hist', y='Height')

df.plot(kind='hist', y='Height', bins=15)


## 스타벅스 음료의 칼로리는?
# 실습 설명
스타벅스 음료의 칼로리 및 영양소 정보가 있습니다.

스타벅스 음료의 칼로리 분포는 어떻게 되는지, 히스토그램을 그려서 확인해 봅시다.

원하는 결과가 나오도록 df.plot() 메소드의 괄호를 채워 보세요!


df.plot() # 괄호를 채워 주세요.
칼로리의 구간은 총 20개로 나누어 주세요.

# 해설
주어진 데이터의 'Calories' column에는 칼로리 정보가 저장되어 있습니다.
확인해 봅시다.


%matplotlib inline
import pandas as pd

df = pd.read_csv("data/starbucks_drinks.csv")
df['Calories']

0        3
1        4
2        5
3        5
4       70
      ... 
237    320
238    170
239    200
240    180
241    240
Name: Calories, Length: 242, dtype: int64
DataFrame의 .plot() 메소드를 사용해서 히스토그램을 그리면 됩니다.

히스토그램을 그리기 위해서 kind를 hist로,  
칼로리에 대한 데이터를 그리기 위해서 y를 Calories로,  
총 20개의 구간으로 나누기 위해 bins를 20으로 설정해 줍니다.

# 모범 답안

%matplotlib inline
import pandas as pd

df = pd.read_csv("data/starbucks_drinks.csv")
df.plot(kind='hist', y='Calories', bins=20)


## 박스 플롯

최댓값
75% 지점(Q3)
중간 값: 50% 지점(Q2)
25% 지점(Q1)
최솟값

이상점(outliers)

# 박스 플롯 인사이트

df['math score'].describe()  //통계 요약

df.plot(kind='box', y='math score')

df.plot(kind='box', y=['math score', 'reading score', 'writing score'])



## 스타벅스 음료의 칼로리는? II
# 해설
박스 플롯도 히스토그램과 유사하게 그릴 수 있습니다.

'Calories' column의 칼로리 정보를 확인해 봅시다.


%matplotlib inline
import pandas as pd

df = pd.read_csv("data/starbucks_drinks.csv")
df['Calories']

0        3
1        4
2        5
3        5
4       70
      ... 
237    320
238    170
239    200
240    180
241    240
Name: Calories, Length: 242, dtype: int64
여기에서 plot 메소드를 사용해서 박스 플롯을 그리면 됩니다.

# 모범 답안

%matplotlib inline
import pandas as pd

df = pd.read_csv("data/starbucks_drinks.csv")
df['Calories'].plot(kind='box')


## 산점도 scatter plot
연관성을 분석 할 수 있다.(몸무게&키, 행복&등수, 몸무게&행복)

df.plot(kind='scatter', x='math score', y='reading score')




## 
%matplotlib inline
import pandas as pd

df = pd.read_csv("data/world_indexes.csv")
Life = df['Life expectancy at birth- years']
users = df['Internet users percentage of population 2014']
Forest = df['Forest area percentage of total land area 2012']
Carbon = df['Carbon dioxide emissionsAverage annual growth']

df.plot(kind='scatter', x='Life', y='users')



df.plot(kind='scatter',x='math score',y = 'writing score')

df[['math score','writing score']].corr()



## Seaborn 소개


## KDE Plot (Kernel Density Estimation)

!pip install seaborn==0.9.0

import pandas as pd
import seaborn as sns

body_df = pd.read_csv('data/body.csv', index_col=0)
body_df.head()

body_df['Height']  // 키의 정보를 가져온다.

body_df['Height'].value_counts()  //키가 항목별 몇번씩 나오는지 확인.

body_df['Height'].value_counts().sort_index()  //index순으로 정렬이 된다.


# 그래프 그리기
body_df['Height'].value_counts().sort_index().plot()

# 그래프를 곡선으로 만들기
sns.kdeplot(body_df['Height'])

# 곡선의 정도 변경하기
//bw 옵션으로 변경 (0.05, 0.5 등 수치 입력, 높은 수치로 입력시 부정확한 그래프 표시)
sns.kdeplot(body_df['Height'], bw=0.5)


## 서울 지하철 승차 인원
# 해설

%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/subway.csv')
주어진 DataFrame을 살펴보면, 호선 이름 ('line'), 역 이름 ('station'), 승차인원 ('in'), 하차인원('out') 등의 정보가 있습니다.

이 중 우리가 원하는 데이터인 승차인원은 다음과 같이 인덱싱할 수 있습니다.


df['in']

0         37
1       2064
2        375
3       4338
4        321
...  
586    31245
587    38099
588    52310
589    29150
590    58391
Name: in, Length: 591, dtype: int64
이 값을 다음과 같이 sns.kdeplot() 안에 넣어주면 됩니다.


%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/subway.csv')

sns.kdeplot(df['in'])


## KDE 활용 예시

body_df.plot(kind='hist', y='Height', bins=15)

sns.kdeplot(body_df['Height'], bins=15)

# box
body_df.plot(kind='box', y='Height')

# violinplot
sns.violinplot(y=body_df['Height'])

# scatter
body_df.plot(kind='scatter', x='Height', y='Weight')

# 등고선
sns.kdeplot(body_df['Height'], body_df['Weight'])



## 교수님의 연봉은?

# 해설
%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/salaries.csv')
주어진 DataFrame을 살펴보면, 급여에 대한 정보가 'salary' column에 있습니다.

이 값을 살펴봅시다.


df['salary']

0     186960
1      93000
2     110515
3     131205
4     104800
...  
73    105450
74    104542
75    124312
76    109954
77    109646
Name: salary, Length: 78, dtype: int64
이 값으로 violin plot을 그리기 위해서는 다음과 같이 sns.violinplot()을 사용하면 됩니다.

# 모범 답안

%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/salaries.csv')

sns.violinplot(df['salary'])



## LM Plot
body_df.plot(kind='scatter', x='Height', y='Weight')


#회귀선 
sns.lmplot(data=body_df, x='Height', y='Weight')

//키나 몸무게 값으로 예측해 볼 수 있다. 하지만 정확한 수치 예측은 불가하다.


## 카테고리별 시각화
laptops_df['os'].unique()
// 어떤 값들이 있는지 추려서 보여준다.

sns.catplot(data=laptops_df, x='os', y='price', kind='box')
sns.catplot(data=laptops_df, x='os', y='price', kind='violin')
a
sns.catplot(data=laptops_df, x='os', y='price', kind='strip')
// 수치별로 표a시가 되어 인식 하기가 좋다.
a
# 프로세서 항목a에 따라 다르게 표시
laptops_df['proacessor_brand'].unique()
a
sns.catplot(data=laptops_df, x='os', y='price', kind='strip', hue='processor_brand')
a
# swarm (점들을a 겹치지지 않고 펼쳐서 보여준다)
sns.catplot(data=laptops_df, x='os', y='price', kind='swarm', hue='processor_brand')


## 보험금 분석하기
# 해설
먼저 주어진 데이터를 살펴봅시다.

%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/insurance.csv')
주어진 DataFrame을 살펴보면, 흡연 여부에 대한 정보가 'smoker', 보험금에 대한 정보가 'charges' column에 있습니다.

이 두 column을 활용하여  seaborn의 카테고리별 시각화를 해 봅시다.

카테고리별 시각화는 seaborn의 catplot을 사용하면 됩니다.

주어진 그래프는 violin 형태이기 때문에 다음과 같이 코드를 작성할 수 있습니다.

# 모범 답안
sns.catplot(data=df, x='smoker', y='charges', kind='violin')


## 평균(Mean) = 데이터들의 합 / 데이터 개수

## 중간값(Median) : 데이터셋에서 딱 중간에 있는 값
// 잘못된 값을 입력하더라도 중간값을 구하는데 문제가 없다.


##  중간값 계산하기

# 1번 문제
list_1 = [44, 42, 43, 28, 46, 33, 42, 37, 29]
list_1.sort()
print(list_1)
length = len(list_1)
print(f"리스트의 길이는 {length}")
if length % 2 == 0:
    print(f"list_1의 중간값은 {(list_1[length // 2] + list_1[length // 2 - 1]) /2}, 중간값은 {length // 2 - 1}과 {length // 2} 사이 ")
else:
    print(f"list_1의 중간값은 {list_1[length // 2]}, 중간값 인덱스는 {length // 2}")
    


# 2번 문제
list_1 = [33, 45, 98, 38, 21, 49, 51, 58, 82, 75]
list_1.sort()
print(list_1)
length = len(list_1)
print(f"리스트의 길이는 {length}")
if length % 2 == 0:
    print(f"list_1의 중간값은 {(list_1[length // 2] + list_1[length // 2 - 1]) /2}, 중간값은 {length // 2 - 1}과 {length // 2} 사이 ")
else:
    print(f"list_1의 중간값은 {list_1[length // 2]}, 중간값 인덱스는 {length // 2}")
    


## Q1, Q3와 이상값

# Q1, Q3 구하기
지금까지 평균과 중간값에 대해 알아봤습니다. 데이터의 50% 지점을 의미하는 중간값을 구할 줄 안다면 Q1(데이터의 25% 지점)과 Q3(데이터의 75% 지점)도 쉽게 구할 수 있는데요.

일단 아래와 같은 데이터셋이 있다고 합시다. 여기서 중간값은 딱 가운데에 있는 86이겠죠?

그리고 이 데이터를 중간값을 기준으로 둘로 나눠 볼게요. 아래 이미지처럼 첫 번째 값부터 중간값까지, 중간값부터 가장 마지막 값까지 이렇게 둘로 나눠진다고 보시면 됩니다. 여기 왼쪽 묶음에서 중간값은 56인데요. 이건 50%의 중간, 즉 25% 지점에 해당하는 Q1 값이라고 볼 수 있습니다. 반대로 오른쪽 묶음에 있는 중간값 100은 75% 지점, 즉 Q3이 되겠죠.

그런데, 아래와 같이 Q1, Q2, Q3의 위치를 딱 정할 수 없는 경우도 있습니다. 지금처럼 데이터의 개수가 짝수일 때 중간값(Q2)을 구하려면 가운데에 있는 두 값의 합을 2로 나눠 주면 된다고 배웠었죠? 즉, 여기서 중간값은 24와 35, 두 값의 중간 지점에 있다고 볼 수 있기 때문에 24 더하기 35를 2로 나눈 29.5가 됩니다.

이번엔 Q1을 구해 볼게요. 사실 데이터에서 특정 퍼센트 지점에 있는 값을 구하는 방법은 여러 가지가 있지만, numpy와 pandas에서 기본으로 사용되는 방식을 대표로 설명드리겠습니다.

먼저 Q1이 몇 번 인덱스에 있는 값인지 구해야 하는데요. 데이터에서 특정 퍼센트 지점의 인덱스를 구하려면 데이터의 개수에서 1을 뺀 뒤, 여기에 원하는 숫자를 곱해주면 됩니다. 지금은 데이터의 개수가 총 8개니까, 
(8−1)∗0.25
를 해서 1.75라는 결과물을 얻을 수 있습니다. (참고로 데이터 개수에서 1을 빼는 건, 파이썬에서 인덱스가 0부터 시작하니까 이를 보정해 주기 위한 거라고 보시면 됩니다!)

그런데 0번, 2번, 4번… 이런 정수 값이 아니라 1.75번 인덱스에 있는 숫자를 어떻게 구해야 할까요? 일단 1.75번 인덱스는 1번 인덱스에 있는 15와 2번 인덱스에 있는 20 사이에 있다고 볼 수 있습니다. 이제 1.75에서 정수 부분 1이 무엇을 의미하는지 알았으니까, 0.75라는 소수 부분을 고려할 차례네요.

쉽게 말하자면, 소수 부분은 두 값 사이에서 정확히 어느 정도 위치에 있는지를 의미한다고 볼 수 있습니다. 0.75를 분수로 표현하면 4분의 3인데요. 즉, Q1은 아래 그림과 같이 15와 20 사이에서 4분의 3 지점에 위치하고 있습니다. 이 값을 구하려면 
15∗(1−0.75)+20∗0.75
를 계산하면 됩니다. 즉, Q1은 18.75가 됩니다.

그러면 Q3도 계산해 볼게요. Q3의 인덱스는 
(8−1)∗0.75를 해서 5.25가 되고요. 즉, 57(5번 인덱스)과 63(6번 인덱스) 사이에서 4분의 1 지점에 위치한 값이라고 할 수 있습니다. 이번에는 
57∗(1−0.25)+63∗0.25를 계산해 주면, Q3은 58.5가 되죠.

# 이상값 구하기
박스 플롯에서 박스와 위스커 바깥에 있는 점들을 이상점(outlier)라고 부른다고 배웠는데요. 딱 봤을 때 다른 값들에 비해 유독 크거나 작은 값들을 이상점이라고 판단할 수도 있겠지만, 이게 이상점인지 아닌지 애매한 경우도 있겠죠?

이상점을 구분하기 위한 명확한 기준이 몇 가지 있는데, 가장 대표적인 것 중 하나로 위에서 구한 Q1과 Q3 값을 활용하는 방식이 있습니다. 이 방식을 사용하려면 일단, Q3과 Q1 사이의 거리를 알아야 하는데요. 그냥 Q3에서 Q1을 빼주면 되고, 이 값을 바로 IQR(Interquartile Range)라고 부릅니다. 방금 전에 사용한 데이터를 다시 예로 들면, IQR은 Q3(58.5)에서 Q1(18.75)를 뺀 39.75가 됩니다.

그리고 보통 Q1 지점에서 아래로 1.5 IQR 더 떨어져 있거나, Q3 지점에서 위로 1.5 IQR 더 떨어져 있는 값은 이상점이라고 판단합니다. 아까 Q1과 Q3을 구한 데이터를 예로 들면, 이상값으로 판단하는 기준점은 -40.875와 118.125가 됩니다. 다 0보다 큰 숫자들이니까, -40.875보다 작은 값은 없는 것 같고 125는 118.125보다 크니까 이상점이라고 볼 수 있겠네요.


## 상관 계수
피어슨 Pearson
상관 계수 Correlation Coefficient


## 상관 계수 시각화

학생들의 시험 점수 데이터가 있습니다. 데이터 사이의 상관 계수를 살펴봅시다.

DataFrame의 corr() 메소드를 사용하면, 숫자 데이터 사이의 상관 계수를 보여 줍니다.


%matplotlib inline
import pandas as pd

df = pd.read_csv('data/exam.csv')

df.corr()


상관 계수도 DataFrame 형태로 출력됩니다.  
하지만 숫자가 많다보니 한눈에 잘 들어오지 않을 수 있는데요. 이럴 때 히트맵을 사용합니다.

히트맵은 상관 계수를 시각화하는 대표적인 방법입니다. 상관 계수는 시각화해서 보는 경우가 많습니다.  
Seaborn을 이용하면 히트맵을 그릴 수 있습니다.

상관 계수의 결과를 Seaborn의 heatmap 메소드에 넘겨주면 됩니다.


%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/exam.csv')

sns.heatmap(df.corr())


색이 밝을수록 상관 계수가 더 높다는 의미입니다.  
읽기 점수(reading score)와 쓰기 점수(writing score) 사이의 상관 관계가 가장 강하다는 것을 한 눈에 알 수 있네요.

annot=True 옵션을 추가해주면, 색상 뿐 아니라 숫자도 함께 보여줍니다.


%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/exam.csv')

sns.heatmap(df.corr(), annot=True)


## EDA (Exploratory Data Analysis) - 탐색적 데이터 분석
데이터셋을 다양한 관점에서 살펴보고 탐색하면서 인사이트를 찾는 것!

- 각 row는 무엇을 의미하는가?

- 각 column은 무엇을 의미하는가?

- 각 column은 어떤 분포를 보이는가?

- 두 column은 어떤 연관성이 있는가?


# 기본 정보 파악하기
basic_info = df.iloc[:, 140:]
basic_info.head()

basic_info.describe() //통계적으로 파악 가능(숫자 columns들만 표시)

basic_info['Gender'].value_counts()
basic_info['Handedness'].value_counts()
basic_info['Education'].value_counts()

sns.violinplot(data=basic_info, y='Age')

sns.violinplot(data=basic_info, x='Gender', y='Age')

# jointplot
sns.jointplot(data=basic_info, x='Height', y='Weight')


## 요즘 인기 직업은?

# 질문 1
occupations.csv 파일(다운로드)을 보고, 여성분들이 가장 많이 종사하고 있는 직종이 무엇인지 파악해 보세요.

상위 직종 3개를 골라 보세요.

1 student, programmer, scientist
2 student, librarian, educator
3 student, administrator, other
4 student, administrator, librarian
5 student, homemaker, marketing

👏🏻 정답입니다.

# 퀴즈 해설
import pandas as pd

df = pd.read_csv('data/occupations.csv', index_col=0)
df.head(5)
성별이 'gender'라는 column에 'M' 혹은 'F'로 들어 있네요.
성별이 여성인 데이터만 받아와 봅시다.


women = df[df['gender'] == 'F']
이제 여성 분들의 직업만 인덱싱해서 받아올 수 있겠죠?


women['occupation']
각 직업이 얼마나 많이 있는지 확인해봅시다.


women['occupation'].value_counts().sort_values(ascending=False)
student, administrator, other, librarian 순으로 많다는 것을 알 수 있네요.

# 정답
import pandas as pd

df = pd.read_csv('data/occupations.csv', index_col=0)
df.head(5)
women = df[df['gender'] == 'F']
women['occupation'].value_counts().sort_values(ascending=False)


# 남자 일 경우
man = df[df['gender'] == 'M']
man['occupation'].value_counts().sort_values(ascending=False)


## 상관 관계 분석 (Correlation Analysis)

music = df.iloc[:, :19]
music.head()

sns.heatmap(music.corr())

df.corr()
df.corr()['Age']
df.corr()['Age'].sort_values(ascending=False)



# 브런치 카페 음악 셀렉션

대위는 신촌에서 대학생들을 대상으로 브런치 카페를 운영합니다.

손님들의 취향에 딱 맞는 음악을 틀고 싶은데요. 브런치 카페이기 때문에, 일찍 일어나는 사람들이 좋아할 만한 음악이 무엇인지 분석해 보려고 합니다.

주어진 데이터(다운로드)의 “Getting up”이라는 column을 보면 사람들이 아침에 일어나는 걸 얼마나 어려워하는지 알 수 있습니다. 5라고 대답한 사람들은 아침에 일어나는 걸 아주 어려워 하는 사람들이고, 1이라고 대답한 사람들은 아침에 쉽게 일어난다는 거죠.

이 데이터로 봤을 때, 아침에 일찍 일어나는 사람들이 가장 좋아할 만한 음악 장르는 무엇인가요?

1 Rock n roll
2 Dance
3 Classical music
4 Opera
5 Swing, Jazz

# 정답

%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/young_survey.csv')

brunch_df = df.corr()['Getting up']
brunch_df[1:19].sort_values(ascending=True)



# 06. 스타트업 아이템 탐색하기

# 하루 정답
%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/young_survey.csv')

A_1 = df.corr().loc['Musical instruments', 'Writing']
A_2 = df.corr().loc['Spending on looks', 'Branded clothing']
A_3 = df.corr().loc['Writing notes', 'New environment']
A_4 = df.corr().loc['Workaholism', 'Healthy eating']
A_5 = df.corr().loc['Prioritising workload', 'Healthy eating']

ans = [A_1, A_2, A_3, A_4, A_5]
ans.sort()
print(ans)

# 퀴즈 해설

주어진 column들의 상관 관계 값을 통해, 각 가설에 대해 판단할 수 있습니다.

다음과 같이 상관 계수를 구할 수 있습니다.


import pandas as pd

df = pd.read_csv('young_survey.csv')

df.corr()
이 결과를 통해 각 column 사이의 상관 계수를 알 수 있습니다.

먼저 1번 가설에 대해 알아봅시다.

"악기를 다루는 사람들은 시 쓰기를 좋아하는 경향이 있을 것이다."


df.corr().loc['Musical instruments', 'Writing']

0.3441930532038243
2번 가설에 대해 알아봅시다.

"외모에 돈을 많이 투자하는 사람들은 브랜드 의류를 선호할 것이다."


df.corr().loc['Spending on looks', 'Branded clothing']

0.41839894464589139
3번 가설입니다.

"메모를 자주 하는 사람들은 새로운 환경에 쉽게 적응할 것이다."


df.corr().loc['Writing notes', 'New environment']

-0.07745384812012578
4번 가설입니다.

"워커홀릭들은 건강한 음식을 먹으려는 경향이 있을 것이다."


df.corr().loc['Workaholism', 'Healthy eating']

0.23678959462190083

df.corr().loc['Prioritising workload', 'Healthy eating']

0.24259007871256624
이 중에서 가장 낮은 상관 관계를 보이는 가설은 3번 가설입니다.


## 클러스터 분석 (Cluster Analysis)

interests = df.loc[:, 'History':'Pets']
interests.head()

corr = interests.corr()
corr

corr['History'].sort_values(ascending=False)

sns.Clustermap(corr)


## 영화 카페 운영하기

# 하루 정답
%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/survey.csv')

# 여기에 코드를 작성하세요

DVD = df.loc[:, 'Horror':'Action']
DVD.head()

corr = DVD.corr()
corr

sns.clustermap(corr)

# 해설
먼저 데이터를 불러옵시다.

%matplotlib inline
import pandas as pd
import seaborn as sns

df = pd.read_csv('data/survey.csv')
주어진 데이터셋에서 영화 장르에 대한 column은 'Horror'부터 'Action'까지 입니다. 이 두 column 사이의 정보만 인덱싱해서 'genres'라는 변수에 저장해 봅시다.

genres = df.loc[:, 'Horror':'Action']
여기에 .corr()을 붙여주면, 영화 장르별 상관 관계를 구할 수 있겠죠?

이제 genres.corr()을 sns.clustermap()에 넣어주면,  clustermap을 그릴 수 있습니다.

sns.clustermap(genres.corr())




# 타이타닉 EDA
질문 1
RMS 타이타닉은 1912년에 빙산과 충돌해 침몰한 여객선입니다. 타이타닉호의 침몰은 무려 1514명 정도가 사망한 비운의 사건이죠. 영화 ‘타이타닉’으로 인해 이름이 가장 널리 알려진 여객선이기도 합니다.

우리에게 주어진 titanic.csv 파일(다운로드)에는 당시 탑승자들의 정보가 담겨 있습니다. 생존 여부, 성별, 나이, 지불한 요금, 좌석 등급 등의 정보가 있는데요.

생존 여부는 'Survived' column에 저장되어 있습니다. 0이 사망, 1이 생존을 의미합니다.
좌석 등급은 'Pclass' column에 저장되어 있습니다. 1은 1등실, 2는 2등실, 3은 3등실을 의미합니다.
지불한 요금은 'Fare' column에 저장되어 있습니다.
다양한 방면으로 EDA(탐색적 데이터 분석)를 한 후, 다음 보기 중 맞는 것을 모두 고르세요.

1
타이타닉의 승객은 30대와 40대가 가장 많다.

가장 높은 요금을 낸 사람은 30대이다.

3
생존자가 사망자보다 더 많다.

1등실, 2등실, 3등실 중 가장 많은 사람이 탑승한 곳은 3등실이다.

가장 생존율이 높은 객실 등급은 1등실이다.

6
나이가 어릴수록 생존율이 높다.

나이보다 성별이 생존율에 더 많은 영향을 미친다.

👏🏻 정답입니다.

퀴즈 해설

우선 보기를 하나씩 살펴보기에 앞서, 데이터의 전반적인 사항을 살펴봅시다.


titanic = pd.read_csv('data/titanic.csv')
titanic.info()

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
    PassengerId    891 non-null int64
    Survived       891 non-null int64
    Pclass         891 non-null int64
    Name           891 non-null object
    Sex            891 non-null object
    Age            714 non-null float64
    SibSp          891 non-null int64
    Parch          891 non-null int64
    Ticket         891 non-null object
    Fare           891 non-null float64
    Cabin          204 non-null object
    Embarked       889 non-null object
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.6+ KB
이 외에도 titanic.head() 등을 통해 살펴볼 수 있겠죠?

이제 보기를 하나씩 살펴봅시다.

보기 1: "타이타닉의 승객은 30대와 40대가 가장 많다."
히스토그램으로 살펴봅시다.


# 나이 분포
titanic.plot(kind='hist', y='Age', bins=30)


30대와 40대보다는 20대가 더 많기 때문에 틀린 설명입니다.

보기 2: "가장 높은 요금을 낸 사람은 30대이다."
이번에는 나이와 요금 사이의 산점도를 그려서 확인해 봅시다.


# 나이별 요금 분포
titanic.plot(kind='scatter', x='Age', y='Fare')


가장 높은 요금을 낸 사람은 30대가 맞네요.

보기 3: "생존자가 사망자보다 더 많다."
생존 여부는 'Survived '라는 column에 저장되어 있습니다.  
value_counts() 메소드를 이용해서 살펴봅시다.


titanic['Survived'].value_counts()

    0    549
    1    342
    Name: Survived, dtype: int64
0이 사망, 1이 생존을 의미하니까 사망자가 더 많네요. 아쉽지만 틀린 보기였습니다.

보기 4: "1등실, 2등실, 3등실 중 가장 많은 사람이 탑승한 곳은 3등실이다."
이 보기는 'Pclass' column의 value_counts()로 알 수 있겠죠?


titanic['Pclass'].value_counts()

    3    491
    1    216
    2    184
    Name: Pclass, dtype: int64
3등실의 사람이 491명으로 가장 많다는 것을 알 수 있습니다.

보기 5: "가장 생존율이 높은 객실 등급은 1등실이다."
객실 등급과 생존 여부에 대해서 그래프를 그려봐야겠네요.  
그런데 그냥 산점도를 그려보면 그림이 겹쳐져서 알아보기 힘듭니다.


# 객실 등급별 생존율 분포
titanic.plot(kind='scatter', x='Pclass', y='Survived')


이런 경우, KDE Plot을 활용하면 겹쳐진 정도를 알 수 있습니다.


# 객실 등급별 생존율 분포
sns.kdeplot(titanic['Pclass'], titanic['Survived'])


그래프를 보면 3등실은 확실히 아래쪽에 더 밀집되어 있고, 2등실은 위쪽과 아래쪽이 비슷하게 분포하고 있습니다. 
하지만 1등실은 위쪽이 더 밀집되어 있음을 알 수 있습니다.

따라서, 1등실의 생존율이 더 높다는 것을 알 수 있네요.

보기 6: "나이가 어릴수록 생존율이 높다."
생존율에 대한 카테고리별 그래프를 그려서 확인해 봅시다.  
왼쪽이 사망자 그룹, 오른쪽이 생존자 그룹입니다.


# 생존 여부에 따른 나이 분포
sns.stripplot(data=titanic, x="Survived", y="Age")


아직 잘 보이지 않으면 바이올린 플롯으로도 그려볼까요?


# 생존 여부에 따른 나이 분포
sns.violinplot(data=titanic, x="Survived", y="Age")


생존한 사람들의 나이 분포와 사망한 사람들의 나이 분포 사이에는 큰 차이가 보이지 않습니다.  
따라서 나이가 어릴수록 생존율이 높다고 하긴 어려울 것 같네요.

보기 7: "나이보다 성별이 생존율에 더 많은 영향을 미친다."
나이 뿐 아니라 성별도 한번 같이 파악해 봅시다.


# 생존 여부에 따른 나이 및 성별 분포
sns.stripplot(data=titanic, x="Survived", y="Age", hue="Sex")


나이 분포는 비슷한 데 비해, 성별의 분포는 확연히 차이가 나죠?

나이보다는 확실히 성별에 영향을 많이 받았다는 걸 알 수 있습니다.


### 새로운 인사이트 발견하기
## 새로운 값 계산하기

df['MBN'] + df['TV CHOSUN'] + df['JTBC'] + df['Channel A'] + df['SBS'] + df['MBC'] + df['KBS']

df.sum(axis='columns')

df['Total'] = df.sum(axis='columns')  //Total 항목을 만들고 df.sum값을 넣는다.

df.plot(y='Total')


df['KBS'] + df['MBC'] + df['SBS']

df['Group 1'] = df.loc[:, 'KBS':'SBS'].sum(axis='columns')
df['Group 2'] = df.loc[:, 'TV CHOSUN':'MBN'].sum(axis='columns')

df.plot(y=['Group 1', 'Group 2'])



## 문자열 필터링
df[df['Genre'] == 'Blues']  //Blues 장르만 표시

df[df['Genre'].str.contains('Blues')]  //Blues 장르가 포함이 된 모든 항목 표시

df[df['Genre'].str.startswith('Blues')]  //Blues가 앞에 오는 항목만 표시


df['Contains Blues'] = df['Genre'].str.contains('Blues')
df
// Contains Blues 항목이 생기고 Blues인 경우만 True로 표시가


## 박물관이 살아 있다I
# 실습 설명
한국에서 잘나가는 동양예술전문가 솔희는 최근 “박물관이 살아 있다” 프로젝트를 시작했습니다.

“박물관이 살아 있다” 프로젝트는 점점 떨어져가는 문화예술공간의 방문율을 높이기 위해 시작되었습니다.
김솔희씨는 먼저 예술의 흥행을 위해선 젊은이들의 참여가 시급하다고 판단하여, 대학교 박물관을 먼저 개선하기로 하였습니다.

대학 박물관을 개선하기 위해 다음과 같이 박물관을 분류하기로 하였습니다.

박물관은 대학/일반 박물관으로 나뉜다.
시설명에 '대학'이 포함되어 있으면 '대학', 그렇지 않으면 '일반'으로 나누어 '분류' column에 입력한다.
'분류' column을 만들어서 솔희를 도와주세요!

해설 보기

해설
먼저 데이터를 불러옵시다.


import pandas as pd

df = pd.read_csv('data/museum_1.csv')
주어진 데이터에서 '시설명' column은 박물관의 이름을 나타냅니다.
'시설명' column에서 '대학교' 를 포함한 데이터는 '대학'으로, 그렇지 않으면 '일반'으로 분류하여 '분류' column에 넣어주고자 합니다.

먼저 '시설명' column에서 '대학'을 포함한 데이터를 골라봅시다.


is_university = df['시설명'].str.contains('대학')
is_university
'대학교'가 문자열에 포함되어 있으면 True로, 그렇지 않으면 False로 나옵니다.


0      False
1      False
2      False
3      False
4       True
       ...  
896    False
897    False
898    False
899    False
900    False
Name: 시설명, Length: 901, dtype: bool
이 결과를 바탕으로 다음과 같이 '분류' column에 데이터를 넣어주면 됩니다.

df.loc[is_university == True, '분류'] = '대학'
df.loc[is_university == False, '분류'] = '일반'

df
데이터를 출력해보면, 잘 분류된 것을 확인할 수 있습니다.


# 모범 답안
import pandas as pd

df = pd.read_csv('data/museum_1.csv')
is_university = df['시설명'].str.contains('대학')

df.loc[is_university == True, '분류'] = '대학'
df.loc[is_university == False, '분류'] = '일반'

df

# 하루 정답
import pandas as pd

df = pd.read_csv('data/museum_1.csv')

df['분류'] = '일반'
# 대학이 포함된 시설의 인덱스를 univ로 명명
univ = df[df['시설명'].str.contains('대학')].index

# univ에 포함되는 '분류'컬럼을 '대학'으로  수정
df.loc[univ, '분류'] = '대학'
df


## 문자열 분리
address = df['소재지도로명주소'].str.split(n=1, expand=True)

address

df['관할구역'] = address[0]

# 박물관이 살아 있다 II

# 하루 정답
import pandas as pd

df = pd.read_csv('data/museum_2.csv')

# 여기에 코드를 작성하세요
area = df['운영기관전화번호'].str.split('-', n=0, expand=True)
df['지역번호'] = area[0]

df


# 실습 설명
솔희는 어느 지역에 박물관이 많은지 분석해보려고 합니다.

하지만 주어진 데이터에는 주소가 없네요.
그러던 도중, 전화번호 앞자리가 지역을 나타낸다는 것을 깨달았습니다.

솔희가 박물관의 위치를 파악할 수 있게 '운영기관전화번호' column의 맨 앞 3자리를 추출하고, '지역번호' column에 넣어주세요.

# 해설
'운영기관전화번호' column에서 지역번호를 분리해 봅시다.

지역번호는 전화번호에서 -을 기준으로 가장 앞에 있는 세 자리 숫자를 말합니다.  
예를 들어, 061-390-7224에서는 지역번호가 061입니다.

column에 .str.split(pat='-') 메소드를 적용하면, 문자열을 '-' 기준으로 분리할 수 있습니다.


phone_number = df['운영기관전화번호'].str.split(pat='-')

phone_number

0       [061, 390, 7224]
1       [033, 737, 4371]
2       [033, 730, 9000]
3       [033, 746, 5256]
...        
897     [053, 744, 5500]
898     [053, 740, 2061]
899     [053, 768, 6051]
900     [043, 835, 4161]
Name: 운영기관전화번호, Length: 901, dtype: object
조금 더 자세히 접근해 봅시다.

n=2 옵션을 적용하면, n을 기준으로 문자열을 나누되 최대 2번 분리한다는 의미입니다.
전화번호는 -을 기준으로 총 2번 (3가지 영역으로) 나눌 수 있죠? 그래서 n에 2을 넣어줬습니다.

expand=True 옵션을 활용하면 결과를 DataFrame으로 보여줍니다.

따라서 이런 코드가 되겠네요.


phone_number = df['운영기관전화번호'].str.split(pat='-', n=2, expand=True)

phone_number


이제 phone_number의 첫 번째 값을 지역번호 column에 넣어주면 됩니다.


df['지역번호'] = phone_number[0]

# 모범 답안
import pandas as pd

df = pd.read_csv('data/museum_2.csv')

phone_number = df['운영기관전화번호'].str.split(pat='-', n=2, expand=True)
df['지역번호'] = phone_number[0]

df



## 웹 자동화
# 웹의 확장

Uniform Resource Locator(URL)

# 서버와 클라이언트

request - response

pycham / setting / inter / requests 추가


## 파이썬으로 요청 보내기
import requests

response = requests.get("https://google.com")
print(response)

성공 하면 <Response [200]>
실패 하면 <Response [500]>


# 미스터하루 홈페이지 리퀘스트
import requests

response = requests.get("http://misterharu.com")
print(response.text)



# TV 시청률 데이터 가져오기

# 모범 답안

import requests

response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text
    
# 테스트 코드
print(rating_page)


## 박물관이 살아 있다 III
# 해설
지역번호를 이용해서 지역명을 알아내고자 합니다.

각 지역번호에 해당하는 지역명을 만들기 위해서는, 지역번호와 지역명에 대한 파이썬 사전 (dictionary)이 필요한데요.

지역번호와 지역을 참고해서, .map() 메소드에 넣어줄 사전을 만들어 봅시다.


region = {
    '02': '서울시',
    '031': '경기도', '032': '경기도',
    '033': '강원도', 
    '041': '충청도', '042': '충청도', '043': '충청도', '044': '충청도',
    '051': '부산시', 
    '052': '경상도', '053': '경상도', '054': '경상도', '055': '경상도',
    '061': '전라도', '062': '전라도', '063': '전라도',
    '064': '제주도',
    '1577': '기타', '070': '기타'
}
이 파이썬 사전을 map 메소드에 넣어주면, '지역번호' column의 값을 지역명으로 매칭해줍니다.


df["지역번호"] = df["지역번호"].map(region)
이제 마지막으로 .rename() 메소드를 활용하여 column의 이름을 "지역명"으로 바꿔줍니다.


df.rename(columns={"지역번호": "지역명"}, inplace=True)
이제 완성되었습니다. df 를 입력하여 결과를 확인해 보세요.

# 모범 답안
import pandas as pd

df = pd.read_csv("data/museum_3.csv", dtype={'지역번호': str})

region = {
    '02': '서울시',
    '031': '경기도', '032': '경기도',
    '033': '강원도', 
    '041': '충청도', '042': '충청도', '043': '충청도', '044': '충청도',
    '051': '부산시', 
    '052': '경상도', '053': '경상도', '054': '경상도', '055': '경상도',
    '061': '전라도', '062': '전라도', '063': '전라도',
    '064': '제주도',
    '1577': '기타', '070': '기타'
}

df["지역번호"] = df["지역번호"].map(region)
df.rename(columns={"지역번호": "지역명"}, inplace=True)

# 테스트 코드
df 


## groupby


## 여러 웹사이트 한꺼번에 가져오기
import requests

rating_pages = []
# https://https://workey.codeit.kr/ratings/index?year=2010&month=1&weekIndex=0

for i in range(5):
    url = "https://workey.codeit.kr/ratings/index?year=2010&month=1&weekIndex={}".format(i)
    response = requests.get(url)
    rating_page = response.text
    rating_pages.append(rating_page)
    
print(len(rating_pages))
print(rating_pages[0])





##TV 시청률 데이터 가져오기 II

# 실습 설명
티비랭킹닷컴 사이트에서 처음 3년(2010~2012)간의 데이터를 가져와 주세요.

웹사이트 주소의 구조를 활용해서, 모든 페이지의 HTML 코드를 rating_pages에 저장해 주세요.

2010년 1월부터 2012년 12월까지 모든 달에 대해, 1주차~5주차 페이지를 순서대로 리스트에 넣어주시면 됩니다.

이전 레슨에서 봤듯이, 5주차가 없는 달은 데이터가 없는 페이지가 나오는데요. 그런 페이지들도 리스트에 넣어주세요.

(코드를 실행하는데 다소 시간이 걸릴 수 있습니다.)


# 해설
티비랭킹닷컴의 주소는 year, month, weekIndex 총 3개의 조건에 따라서 페이지 결과가 달라지죠?

예를 들어, 2011년 6월 셋째 주의 결과는 아래 주소에서 확인할 수 있습니다.


https://workey.codeit.kr/ratings/index?year=2011&month=6&weekIndex=2
우리는 2010년 1월부터 2012년 12월까지 모든 달에 대해, 1주차~5주차 페이지를 순서대로 리스트에 넣어야 합니다. 그러면 아래와 같은 모든 주소에 대해 페이지를 받아 와야 합니다.


https://workey.codeit.kr/ratings/index?year=2010&month=1&weekIndex=0
https://workey.codeit.kr/ratings/index?year=2010&month=1&weekIndex=1
https://workey.codeit.kr/ratings/index?year=2010&month=1&weekIndex=2
...
https://workey.codeit.kr/ratings/index?year=2012&month=12&weekIndex=2
https://workey.codeit.kr/ratings/index?year=2012&month=12&weekIndex=3
https://workey.codeit.kr/ratings/index?year=2012&month=12&weekIndex=4
이 모든 웹 페이지 주소를 requests.get() 함수 괄호 안에 넣어서 실행하면 되는데요.

이런 반복적인 동작을 해결하기 위해서, 이전 레슨처럼 반복문을 활용하면 되겠죠? 그런데 이번엔 weekIndex 뿐만이 아니라 year, month도 바꿔 줘야 하기 때문에 중첩된 반복문을 사용해야 합니다.

year는 2010~2012,
month는 1~12,
weekIndex는 0~4까지 반복되면 되겠죠?
range() 함수는 다양한 사용법이 있는데요:


# range(b) -> 0에서 b-1을 뜻함
# range(a, b) -> a에서 b-1을 뜻함
두 번째 방식을 사용해서 반복문을 이렇게 쓸 수 있습니다.


for year in range(2010, 2013):
    for month in range(1, 13):
        for weekIndex in range(0, 5):
이제 빈 리스트를 만들고, 주어진 year, month, weekIndex 값을 통해 페이지를 요청하고 빈 리스트에 추가하면 됩니다.

코드를 완성해 봅시다.


ratings_pages = []

for year in range(2010, 2013):
    for month in range(1, 13):
        for weekIndex in range(0, 5):
            url = "https://workey.codeit.kr/ratings/index?year={}&month={}&weekIndex={}".format(year, month, weekIndex)
            response = requests.get(url)
            rating_pages.append(response.text)
이렇게 작성하면 우리가 원하는 전체 기간에 대해 페이지를 요청하고, rating_pages라는 리스트에 추가할 수 있습니다.

테스트 코드를 실행해 보면, 원하는 결과를 얻을 수 있습니다.

# 모범 답안

import requests

rating_pages = []

for year in range(2010, 2013):
    for month in range(1, 13):
        for weekIndex in range(0, 5):
            url = "https://workey.codeit.kr/ratings/index?year={}&month={}&weekIndex={}".format(year, month, weekIndex)
            response = requests.get(url)
            rating_pages.append(response.text)

# 테스트 코드
print(len(rating_pages)) # 가져온 총 페이지 수 
print(rating_pages[0]) # 첫 번째 페이지의 HTML 코드



## PUBG api

import requests

url = "https://api.pubg.com/shards/$kakao/players?filter[playerNames]=$Mister_Haru"

header = {
    "Authorization": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI2MmQzYThiMC1kOWJkLTAxM2EtMWQyNC0xM2EwNDg1M2ZlMTEiLCJpc3MiOiJnYW1lbG9ja2VyIiwiaWF0IjoxNjU2NDk1NjA3LCJwdWIiOiJibHVlaG9sZSIsInRpdGxlIjoicHViZyIsImFwcCI6Im1pc3RlcmhhcnUtbGl2In0.skQJ8TWf1UJj32bItfBMm34ajL41rdMAj2yJInjtn24",
    "Accept": "application/vnd.api+json"
}

r = requests.get(url, headers=header)


## 

import json
import urllib.request

key = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI2MmQzYThiMC1kOWJkLTAxM2EtMWQyNC0xM2EwNDg1M2ZlMTEiLCJpc3MiOiJnYW1lbG9ja2VyIiwiaWF0IjoxNjU2NDk1NjA3LCJwdWIiOiJibHVlaG9sZSIsInRpdGxlIjoicHViZyIsImFwcCI6Im1pc3RlcmhhcnUtbGl2In0.skQJ8TWf1UJj32bItfBMm34ajL41rdMAj2yJInjtn24'

URL = 'https://api.pubg.com/shards/$kakao/players?filter[playerNames]=$Mister_Haru'

json_page = urllib.request.urlopen(url)
json_data = json_page.read().decode("utf-8")
json_array = json.loads(jason_data)

print(json_array)


## chatGPT 수정
import json
import urllib.request

key = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJqdGkiOiIxNGY3YTdmMC0wZmY5LTAxM2MtYTIyYS00YTczMDBjOWZhM2UiLCJpc3MiOiJnYW1lbG9ja2VyIiwiaWF0IjoxNjkwNjA2MTA3LCJwdWIiOiJibHVlaG9sZSIsInRpdGxlIjoicHViZyIsImFwcCI6Ii03MzE1Njc4YS0yZjQ1LTRhZjAtOWRkYS1hZDYyYjJlNTNiNTgifQ.DrFSSWDqEv2sqdIRdbiWpA9RBrGXjxyrxi2dPchtbUM'  # 여기에 실제 API 키를 입력해야 합니다.
url = 'https://api.pubg.com/shards/$kakao/players?filter[playerNames]=$Mister_Haru'

# Request 헤더에 API 키 추가
headers = {'Authorization': f'Bearer {key}'}

# urllib.request.Request 객체를 사용하여 요청 생성
request = urllib.request.Request(url, headers=headers)

# 요청 실행 및 응답 저장
with urllib.request.urlopen(request) as response:
    json_data = response.read().decode("utf-8")

# JSON 데이터 파싱
json_array = json.loads(json_data)

# 결과 출력
print(json_array)


##
curl -g "https://api.pubg.com/shards/kakao/players?filter[playerIds]=Mister_Haru" \ -H "Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI2MmQzYThiMC1kOWJkLTAxM2EtMWQyNC0xM2EwNDg1M2ZlMTEiLCJpc3MiOiJnYW1lbG9ja2VyIiwiaWF0IjoxNjU2NDk1NjA3LCJwdWIiOiJibHVlaG9sZSIsInRpdGxlIjoicHViZyIsImFwcCI6Im1pc3RlcmhhcnUtbGl2In0.skQJ8TWf1UJj32bItfBMm34ajL41rdMAj2yJInjtn24"  -H "accept: application/vnd.api+json"

curl -X GET -H "Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI2MmQzYThiMC1kOWJkLTAxM2EtMWQyNC0xM2EwNDg1M2ZlMTEiLCJpc3MiOiJnYW1lbG9ja2VyIiwiaWF0IjoxNjU2NDk1NjA3LCJwdWIiOiJibHVlaG9sZSIsInRpdGxlIjoicHViZyIsImFwcCI6Im1pc3RlcmhhcnUtbGl2In0.skQJ8TWf1UJj32bItfBMm34ajL41rdMAj2yJInjtn24" "https://api.pubg.com/shards/kakao/players?filter[playerIds]=Mister_Haru" -H "accept: application/vnd.api+json"


# API-preface.py
# chicken.dinner 라이브러리 사용하지 않고 가져올 때
# Change points

import requests
import pandas as pd

# point 1 your API KEY
api_key = "your API Key"

header = {
  "Authorization": api_key,
  "Accept": "application/vnd.api+json"
}

# point 2 change URL parameters
url = "https://api.pubg.com/shards/tournaments/{match_id}" 
r = requests.get(url, headers=header)

# point 3 now you can see the results
json_r = r.json()


### example ###

# Find info of "GEN_pio"

import requests

# 1. filter[playerNames]={playerName}
url = "https://api.pubg.com/shards/steam/players?filter[playerNames]=GEN_Pio"

# 2. players/{account Id}
# url = "https://api.pubg.com/shards/steam/players/account.e57c514acd19440bbc8a52233e482d93"

header = {
  "Authorization": "your API KEY",
  "Accept": "application/vnd.api+json"
}

r = requests.get(url, headers=header)
json_r = r.json()
json_r

# tip. you can get --
# 'id' - accountId (account.e57c514acd19440bbc8a52233e482d93)
# 'Match ID' - Used to lookup the full match object on the '/matches' endpoint


# get-data-matches.py
import pandas as pd

from chicken_dinner.models.tournament import Tournament
from chicken_dinner.pubgapi import PUBG
from chicken_dinner.pubgapi import PUBGCore

api_key = 'PERSONAL API KEY'
PUBG = PUBG(api_key=api_key, shard='pc-tournament', gzip=True)
PUBGCore = PUBGCore(api_key=api_key, shard='pc-tournament', gzip=True)

id = pd.read_csv("tournaments_list.csv")
id = id['tournament_id']
id

# ''안에 아이디 하나씩 넣기
id_tor = 'kr-bsc20'

# 괄호에 아이디 하나씩 넣기 *22
tor = PUBG.tournament(id_tor)
matchlist = tor.match_ids

## NEEDED LIST
# FIRST TABLE
# match_info
match_id = []
created_at = []
map_name = []
duration = []
telemetry_link = []

# SECOND TABLE
# match_participant
player_id = []
team_roster_id = []
team_id = []
team_rank = []
match_id_2 = []
# match_participant_stats
participant_stats = []


# EXTRACTING 'FOR'
#for i in matchid_list :
for i in matchlist :
  # match_info
  # match_id, created_at, map_name, duration, telemetry_link
  match = PUBG.match(i)
  match_id.append(match.id)
  created_at.append(match.created_at)
  map_name.append(match.map_name)
  duration.append(match.duration)
  telemetry_link.append(match.telemetry_url)

  # match_participant
  # match_id, player_id, team_roster_id, team_rank, team_id
  rosters = match.rosters
  for i in range(len(rosters)):
    roster = rosters[i]
    roster_participant = roster.participants
    for i in range(len(roster_participant)):
      participant = roster_participant[i]
      match_id_2.append(match.id)
      player_id.append(participant.name)
      team_roster_id.append(roster.id)
      team_rank.append(roster.stats['rank'])
      team_id.append(roster.stats['team_id'])
      # match_participant_stats
      stats = participant.stats
      participant_stats.append(stats)

# MAKE DATAFRAME USING LISTS
#match_pariticpant
match_participant = pd.DataFrame({'match_id': match_id_2, 'player_id': player_id, 'team_roster_id': team_roster_id, 'team_id': team_id, 'team_rank': team_rank})

#match_participant_stats
match_participant_stats = pd.DataFrame(participant_stats).drop(columns='player_id')
match_participant_stats['team_name'] = match_participant.player_id.str.split('_').str[0]
match_participant_stats['player_name'] = match_participant.player_id.str.split('_').str[1]
match_participant_stats['player_name'] = [i.upper() for i in match_participant_stats['player_name']]

match_participant_stats = match_participant_stats.rename(columns={'name':'player_id'})

#인덱스 기준으로 join
match_participant_all = pd.merge(match_participant, match_participant_stats, how='inner', left_index=True, right_index=True)
match_participant_all = match_participant_all.rename(columns={'player_id_x': 'player_id'})

#[최종]matches_info
match_info = pd.DataFrame({'match_id': match_id, 'created_at': created_at, 'map_name' : map_name, 'duration': duration})

#[최종] result
tournament_id = pd.DataFrame({'tournament_id': [id_tor for i in range(len(match_participant_all))]})

match_participant_all = pd.merge(match_participant_all, tournament_id, how = "left", left_index=True, right_index=True)
result = pd.merge(match_participant_all, match_info, on = 'match_id', how='left').drop_duplicates()

result = result[['tournament_id', 'match_id', 'created_at', 'map_name', 'duration','team_rank', 'team_name', 'player_id', 'player_name', 'time_survived', 'death_type', 'kill_place', 'kills', 'dbnos', 'assists','damage_dealt', 'headshot_kills','longest_kill', 'road_kills', 'vehicle_destroys', 'weapons_acquired', 'boosts', 'heals', 'revives','ride_distance', 'swim_distance', 'walk_distance']].sort_values('created_at')


## get-matchid.py
# 한 tournament의 모든 match_id와 created_at을 데이터프레임으로 만들어주는 코드

import requests
import pandas as pd

api_key = "your API Key"
header = {
  "Authorization": api_key,
  "Accept": "application/vnd.api+json"
}

# {as-pcs3as} 부분에 esports_tournaments_summary.csv의 tournament_id
url = "https://api.pubg.com/tournaments/as-pcs3as"

r = requests.get(url, headers=header)
json_r = r.json()
json_r

# 모든 match_id와 created_at을 꺼내주는 반복문
matchId_dict = {match['attributes']['createdAt']: match['id'] for match in json_r['included']}
matchId_dict

# 데이터프레임화
pcs3_matchId_df = pd.DataFrame(sorted(matchId_dict.items(), key=lambda x: x[0]), columns=['createdAt', 'matchId'])
pcs3_matchId_df

############ https://github.com/dataitgirls4/team_5




### 웹 스타일링
## 웹 페이지 살펴보기
# 기본 HTML 태그 정리 노트
앞선 레슨에서 아래와 같은 코드를 보았습니다.


<!DOCTYPE html>
<html>
    <head>
      <title>Codeit</title>
      <meta charset="utf-8">
    </head>

    <body>
        <img src="https://www.codeit.kr/static/images/brand/logo_original.png">
        <h2>본인의 커리어 목표를 달성해 보세요.</h2>
        <p><b>코드잇</b>은 미국 아이비리그 컴퓨터 사이언스 전공자, 국내 대기업 IT 출신 등으로 구성된 팀을 통해, 모든 수업들을 자체 제작하여 큰 분야를 쪼개서 유기적으로 학습할수 있게 만들었어요. 코드잇 가이드를 통해 원하는 목표를 빠르게 달성해 보세요</p>

        <hr>

        <h2><i>코드잇 커뮤니티</i>에서 함께 궁금한 점을 해결해 봐요.</h2>
        <p>궁금한 게 생겼거나 함께 의견을 나누고 싶다면? 서로의 질문과 답변을 통해, 코딩 실력을 더욱 향상시켜 보세요</p>

        <hr>

        <a href="https://www.codeit.kr/">새로운 코딩 교육의 시작, 코드잇</a>

        <ul>
          <li>프로그래밍 기초 <i>in Python</i></li>
            <li>프로그래밍 기초 <span><i>in Java</i></span></li>
          <li>컴퓨터 개론</li>
          <li>웹 퍼블리싱</li>
          <li>업무 자동화</li>
            <li>데이터 사이언스</li>
          <li>머신 러닝</li>
          <li>알고리즘의 정석</li>
        </ul>
    </body>
</html>
어떤 HTML 요소들이 있는지, 여러분이 자주 마주치게 될 태그 몇 개만 살펴보겠습니다!

물론 이전 레슨에서도 말씀드린 것처럼, 태그를 다 알아야하는 것은 아니며, 암기할 필요는 더더욱 없습니다.

‘HTML에는 이런 코드가 있구나’ 정도만 대략적으로 파악하시면 됩니다.

!DOCTYPE 선언
HTML 파일을 쓸 때는 가장 먼저 <!DOCTYPE> 선언을 해야 합니다. 이전의 HTML 버전을 사용하려면 조금 복잡하게 써야 하지만, 가장 최신 버전인 HTML 5를 사용하기 위해서는 이렇게만 쓰면 됩니다:


<!DOCTYPE html>
title 태그
페이지의 제목은 <title> 태그에 써주면 됩니다. 브라우저의 탭이나 방문 기록에 나와 있는 제목이 바로 <title>에 들어갑니다.


<title>Sample Website</title>
icok8rt60-screenshot(rev).png

h1~h6 태그
한 페이지에 여러 개의 머리말이 있을 수 있는데요. 그 중 가장 중요한 머리말은 <h1>(heading 1), 그 다음으로 중요한 머리말은 <h2>(heading 2). 이런 식으로 <h6>(heading 6)까지 작성할 수 있습니다.

htmlcss

<h1>머리말 1</h1>
<h2>머리말 2</h2>
<h3>머리말 3</h3>
<h4>머리말 4</h4>
<h5>머리말 5</h5>
<h6>머리말 6</h6>

결과 확인
각 머리말의 크기는 나중에 마음대로 설정할 수 있지만, 따로 설정해주지 않으면 <h1>부터 순서대로 작아집니다.

p 태그
보통 문단은 <p>(paragraph) 태그 안에 넣습니다. 물론 직접 설정할 수도 있지만 <p> 태그 위, 아래에는 기본적으로 여백이 조금씩 있습니다.

htmlcss

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>

<p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>

<p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

결과 확인
b 태그, i 태그
<b> 태그는 글씨를 볼드체로 바꿔줍니다.

<i> 태그는 글씨를 이탤릭체로 바꿔줍니다.

htmlcss

<b>볼드체</b>와 <i>이탤릭체</i>

결과 확인
a 태그
<a> 태그는 하이퍼링크를 만들 때 사용됩니다. 태그의 href 속성 안에 링크를 통해 이동하고 싶은 주소를 써 줍니다.

htmlcss

<a href="https://www.codeit.kr/">코드잇 웹사이트</a>

결과 확인
div 태그
<div> 태그는 HTML에서 가장 많이 쓰이는 태그 중 하나인데요. 'div'는 'division'의 약자로, 웹 요소에 특정 효과를 줄 때 사용하는 것이 아니라, 웹사이트의 구역을 정해서 레이아웃을 정하고, 디자인을 입히는데 사용합니다.

<div> 태그의 원리를 모르셔도 웹 스크래핑을 하는 데는 크게 지장이 없습니다. 앞으로 HTML 코드를 보다 보면 <div> 태그를 정말 많이 접하실 겁니다.

htmlcss

<div>
  <h2>div 안에 있는 h2</h2>
  <p>div 안에 있는 p</p>
</div>


## CSS 선택자 정리 노트
# CSS 선택자란?
HTML 코드 어느 요소에 스타일을 입힐지를 정하는 부분을 "CSS 선택자"라고 합니다. CSS 선택자를 통해 원하는 태그를 선택하는 거죠.

우리가 웹 요소에 스타일을 입힐 것은 아니고, CSS 선택자를 통해 필요한 태그를 선택해서, 태그에 있는 정보를 추출해 낼 겁니다.

그럼 다양한 CSS 선택자를 정리해 봅시다. 선택자를 정리할 때 드는 예시는 아래 코드에 대한 예시입니다.


<!DOCTYPE html>
<html>
    <head>
      <title>Codeit</title>
      <meta charset="utf-8">
        <link rel="stylesheet" href="styles.css">
    </head>

    <body>
        <img src="https://www.codeit.kr/static/images/brand/logo_original.png">
        <h2 id="first-heading">본인의 커리어 목표를 달성해 보세요.</h2>
        <p><b class="codeit">코드잇</b>은 미국 아이비리그 컴퓨터 사이언스 전공자, 국내 대기업 IT 출신 등으로 구성된 팀을 통해, 모든 수업들을 자체 제작하여 큰 분야를 쪼개서 유기적으로 학습할수 있게 만들었어요. 코드잇 가이드를 통해 원하는 목표를 빠르게 달성해 보세요</p>

        <hr>

        <h2><i class="codeit">코드잇</i> 커뮤니티에서 함께 궁금한 점을 해결해 봐요.</h2>
        <p>궁금한 게 생겼거나 함께 의견을 나누고 싶다면? 서로의 질문과 답변을 통해, 코딩 실력을 더욱 향상시켜 보세요</p>

        <hr>

        <a href="https://www.codeit.kr/">새로운 코딩 교육의 시작, 코드잇</a>

        <ul>
          <li class="beginner Python">프로그래밍 기초 <i>in Python</i></li>
            <li class="beginner">프로그래밍 기초 <span><i>in Java</i></span></li>
          <li>컴퓨터 개론</li>
          <li>웹 퍼블리싱</li>
          <li>업무 자동화</li>
            <li>데이터 사이언스</li>
          <li>머신 러닝</li>
          <li>알고리즘의 정석</li>
        </ul>
    </body>
</html>
태그 이름
태그 이름을 선택자로 사용할 수 있습니다.


/* 선택자 문법 */
tagname

/* 예시: h2 태그만 선택 */
h2
태그 이름을 그대로 쓰면 됩니다.

ID (아이디)
Id는 문서에서 딱 하나의 태그에만 스타일을 입히고 싶을 때 사용됩니다.


/* 선택자 문법 */
#id-name

/* 예시: id 값이 first-heading인 태그만 선택 */
#first-heading
샵(#) 뒤에 아이디 값을 써 주면 됩니다.

아이디는 딱 하나의 태그에 스타일을 입힐 때 사용되기 때문에 동일한 아이디를 여러 태그가 가질 수 없습니다. 하나의 아이디는 HTML 코드 전체에 단 한 번만 나올 수 있습니다.

Class (클래스)
여러 태그에 동시에 스타일을 입히려면 class라는 속성을 사용하면 됩니다.

Class는 여러 HTML 태그에 공통으로 적용할 수 있습니다. 꼭 같은 종류의 태그일 필요도 없습니다.


/* 선택자 문법 */
.class-name

/* 예시: class 이름이 beginner인 태그만 선택 */
.beginner
점(.) 뒤에 class 이름을 써 주면 됩니다.

한 태그는 여러 class 이름을 가질 수 있습니다. Class 이름들 사이에 공백을 추가하면 됩니다.


<li class="beginner Python">프로그래밍 기초 <i>in Python</i></li>
예를 들어 위 li 태그는 class 값 두 개, 'beginner'와 'Python'을 가지고 있는 겁니다.

ID, Class를 제외한 나머지 속성
id와 class를 제외한 나머지 속성도 CSS 선택자로 사용할 수 있습니다.


/* 선택자 문법 */
[attr="value"]

/* 예시: href 속성 값이 "https://www.codeit.kr/"인 태그만 선택 */
[href="https://www.codeit.kr/"]
네모 괄호([]) 안에 속성 이름과 속성 값을 넣어 주면 됩니다.

OR 연산
여러 선택자를 OR 연산으로 조합할 수 있습니다.


/* 선택자 문법 */
selector1, selector2, ...

/* 예시: h2 태그 또는 p 태그 선택 */
h2, p
선택자 사이에 쉼표(,)를 넣어주면 됩니다.

AND 연산
여러 선택자를 AND 연산으로도 조합할 수 있습니다.


/* 선택자 문법 */
selector1selector2...

/* 예시: class 이름이 codeit인 b 태그 선택 */
b.codeit
선택자를 붙여서 써 줍니다.

참고로 태그 이름처럼 알파벳으로 시작하는 선택자가 있고, class, id처럼 기호로 시작하는 선택자가 있으면 알파벳으로 시작하는 선택자를 먼저 써 줘야겠죠?


/* OK */
b.codeit

/* 'class 이름이 codeitb'로 인식됨 */
.codeitb
태그 이름 선택자 두 개에 AND 연산을 할 일은 없습니다. 태그 이름이 두 개일 수는 없기 때문이죠.

중첩된 요소
HTML 태그 안에 또 다른 HTML 태그가 있을 수 있습니다. 이런 경우 태그가 중첩되어 있다고 부르는데요. 일반적인 웹 페이지의 HTML 태그는 많이 중첩되어 있기 때문에, 중첩에 대한 조건을 추가하는 경우가 많습니다.


/* 선택자 문법 */
selector1 selector2 ...

/* 예시: li 태그 안에 중첩된 i 태그 선택 */
li i
선택자를 띄어 써 주면 됩니다. 안에 중첩된 요소에 대한 선택자를 오른쪽에 써 줍니다.

직속 자식
태그에 중첩된 요소들 중 태그의 직속 자식만 선택할 수도 있습니다.


/* 선택자 문법 */
selector1 > selector2 > ...

/* 예시: li 태그의 직속 자식인 i 태그 선택 */
li > i
부등호(>)를 써 주고 직속 자식에 대한 선택자를 부등호 오른쪽에 써 줍니다.

*
마지막으로 별표(*)입니다. 별표 문자는 모든 태그를 뜻합니다. 웹 스크래핑을 할 때는 꽤나 유용할 수 있습니다.


/* 예시: li 태그 안에 있는 모든 태그 선택 */
li * 



## Beautiful Soup 라이브러리

## 설치 하기
# pip3 install beautifulsoup4


import requests
from bs4 import BeautifulSoup


response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')


# print(soup.prettify())  
# print(soup.select('title')) 원하는 태그를 가져온다.
print(soup.select('table'))


## 원하는 태그 선택하기 I

import requests
from bs4 import BeautifulSoup


response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

program_title_tags = soup.select('td.program')

# program_titles = []

# for tag in program_title_tags:
#     program_titles.append(tag.get_text())

# print(program_titles)

print(soup.select_one('td.program'))






import requests
from bs4 import BeautifulSoup


response = requests.get("https://esports.pubgrank.org/tournament/KCSScrim")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

leaderboard_tags = soup.select('table')

leaderboards = []

for tag in leaderboard_tags:
    leaderboards.append(tag.get_text())

print(leaderboards)

# print(soup.select_one('td.program'))


===============

; import requests
; from bs4 import BeautifulSoup


; response = requests.get("https://esports.pubgrank.org/tournament/KCSScrim")
; rating_page = response.text

; soup = BeautifulSoup(rating_page, 'html.parser')

; program_title_tags = soup.select('div.v-data-table__wrapper')

; program_titles = []

; for tag in program_title_tags:
;     program_titles.append(tag.get_text())

; print(program_titles)

; # print(soup.select_one('td.program'))

=====================


## 꿈의 직장 전화번호 모으기 I
# 실습 설명
오렌지 보틀에서 일하고 싶다는 꿈을 갖고 있습니다. 각 지점에 전화를 해서 아르바이트 자리가 없는지 물어보려고 하는데요. 오렌지 보틀의 웹사이트에 가서, 모든 지점의 전화번호를 모아봅시다.

https://workey.codeit.kr/orangebottle/index

모든 지점의 전화번호를 phone_numbers 리스트에 담아주세요.

# 해설
가장 먼저 오렌지 보틀 웹사이트에 요청을 보내고, BeautifulSoup을 이용해서 받아온 HTML 코드를 정리해 줘야겠죠?


# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')
이 코드는 웹사이트 주소만 바뀔 뿐 항상 똑같습니다.

이제 soup을 만들었으면, 필요한 태그를 가져와야 합니다. 전화번호 부분을 검사해 보면, 이렇게 나오는데요.



전화번호는 모두 class 이름이 phoneNum인 span 태그 안에 있습니다.


<span class="phoneNum">070 3460 5076</span>
이것에 대한 CSS 선택자는 아래와 같이 짤 수 있습니다.


span /* span 태그 선택 */
.phoneNum /* class="phoneNum"인 태그 선택 */
span.phoneNum /* class="phoneNum"인 span 태그 선택 */
AND 연산은 선택자를 붙여서 쓰는 것 기억하시죠?

그리고 이 CSS 선택자를 soup.select()안에 써 주시면 됩니다.


# "phoneNum" 클래스를 가진 span 태그 선택하기
phone_num_tags = soup.select('span.phoneNum')
사실 class 이름이 phoneNum인 태그는 전화번호를 담고 있는 span 태그밖에 없기 때문에 .phoneNum을 선택자로 쓰셔도 됩니다.


phone_num_tags = soup.select('.phoneNum')
그럼 이제 태그에서 전화번호를 빼와서, 리스트에 담아야 하는데요. 태그 안에 있는 텍스트를 가져오려면 태그에 get_text()를 쓰면 된다고 했죠?

일단 phone_numbers 리스트를 만들어 주고, 반복문을 써서 모든 태그에서 텍스트를 phone_numbers에 추가해 줍니다.


phone_numbers = []

# 텍스트 추출해서 리스트에 추가하기
for tag in phone_number_tags:
    phone_numbers.append(tag.get_text())
모범 답안

import requests
from bs4 import BeautifulSoup

# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 타입으로 변환
soup = BeautifulSoup(response.text, 'html.parser')

# "phoneNum" 클래스를 가진 span 태그 선택하기
phone_number_tags = soup.select('span.phoneNum')

phone_numbers = []

# 텍스트 추출해서 리스트에 추가하기
for tag in phone_number_tags:
    phone_numbers.append(tag.get_text())

# 테스트 코드
print(phone_numbers)



## 원하는 태그 선택하기 II

import requests
from bs4 import BeautifulSoup


response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

td_tags = soup.select('td')[:4]

for tag in td_tags:
    print(tag.get_text())



## 슬라이싱과 range 함수

# 리스트 인덱스
리스트는 여러 요소를 저장하는데 사용됩니다. 리스트 요소의 위치를 인덱스라고 하는데요. 인덱스는 0부터 시작합니다.


a = [5, 2, 9]  
# a[0] = 5
# a[1] = 2
# a[2] = 9
그리고 파이썬에서는 인덱스가 음수가 될 수도 있습니다. -i는 리스트 끝에서 i 번째 요소를 뜻합니다.


a = [5, 2, 9]
# a[-1] = 9
# a[-2] = 2
# a[-3] = 5
하지만 인덱스는 리스트 길이를 초과할 수 없습니다.


a = [5, 2, 9]
# a[3] = 오류
# a[-4] = 오류
어쩔 때는 리스트의 모든 요소가 필요한 것이 아니라, 일부 요소만 필요합니다. 리스트 요소의 일부만 접근하는 방법은 두 가지가 있습니다.

슬라이싱
인덱스를 활용해서 리스트에서 원하는 부분을 잘라낼 수 있습니다. 이것을 슬라이싱이라고 합니다. 문법은 아래와 같습니다.


lst[start:stop]
lst[start:stop:step]
start, stop, step은 모두 선택적 파라미터인데요 (즉 값을 주지 않아도 됩니다):

start는 시작 인덱스 (디폴트 값 0)
stop은 끝 인덱스 (디폴트 값 리스트 마지막 인덱스)
step은 인덱스 간의 간격을 뜻합니다 (디폴트 값 1).
lst[start:stop]은 start 인덱스에서 stop-1까지의 요소를 가져온다(잘라낸다)는 뜻입니다. 그리고 step도 있으면  step씩 건너뛰면서 요소를 가져온다는 뜻입니다.

아마 예시를 몇 개 보시면 쉽게 이해가 되실 겁니다.


lst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

print(lst[:]) # 리스트 모든 요소
print(lst[:8]) # 처음 8개 요소
print(lst[:-3]) # 인덱스 -3까지 모든 요소
print(lst[1:]) # 인덱스 1부터 모든 요소
print(lst[-4:]) # 마지막 4개 요소
print(lst[2:7]) # 인덱스 2~6에 있는 요소
print(lst[::2]) # 리스트 모든 요소, 인덱스 2씩 건너뛰면서 
print(lst[:8:2]) # 처음 8개 요소, 인덱스 2씩 건너뛰면서
print(lst[1::2]) # 인덱스 1부터 모든 요소, 인덱스 2씩 건너뛰면서
print(lst[2:7:2]) # 인덱스 2~6에 있는 요소, 인덱스 2씩 건너뛰면서

[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
[1, 2, 3, 4, 5, 6, 7, 8]
[1, 2, 3, 4, 5, 6, 7]
[2, 3, 4, 5, 6, 7, 8, 9, 10]
[7, 8, 9, 10]
[3, 4, 5, 6, 7]
[1, 3, 5, 7, 9]
[1, 3, 5, 7]
[2, 4, 6, 8, 10]
[3, 5, 7]
리스트에서 필요한 부분만 잘라냈으니, 이걸 새로운 변수에 저장해서 쓰면 됩니다.


sliced_lst = lst[:8]

for element in sliced_lst:
    print(element)

1
2
3
4
5
6
7
8
range() 함수
range() 함수는 보통 아래와 같이 사용되는데요.


lst = [1, 2, 3, 4, 5]
n = len(lst)

for i in range(n):
    print(lst[i])

1
2
3
4
5
range() 함수는 필요한 인덱스 범위를 만들어 주는데 사용됩니다. range() 함수를 잘 이용하면 원하는 인덱스에 있는 요소들만 접근할 수 있습니다.

range() 함수 문법은 슬라이싱과 비슷한데요, 세 가지 버전이 있습니다.


range(stop)
range(start, stop)
range(start, stop, step)
슬라이싱과 마찬가지로

start는 시작 인덱스
stop은 끝 인덱스
step은 인덱스 간의 간격을 뜻합니다.
(range 함수를 쓸 때는 슬라이싱과 달리 항상 파라미터 값을 주는 것을 권장드립니다. 파라미터 값을 안 주는 경우 함수의 동작이 직관적이지 아닐 수 있습니다.)

그러니까 range(start, stop, step)은 start부터 stop-1까지 step의 간격을 둔 인덱스 범위를 만들어 주는 거죠.

range() 함수를 이용해서 특정 요소들만 접근할 수 있습니다.


lst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
n = len(lst)

print("파라미터 1개")
for i in range(n):
    print(lst[i])

print("파라미터 2개")
for i in range(3, n):
    print(lst[i])

print("파라미터 3개")
for i in range(3, n, 2):
    print(lst[i])

파라미터 1개
1
2
3
4
5
6
7
8
9
10
파라미터 2개
4
5
6
7
8
9
10
파라미터 3개
4
6
8
10



## 원하는 태그 선택하기 III
import requests
from bs4 import BeautifulSoup


response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

tr_tag = soup.select('tr')[1]
td_tags = tr_tag.select('td')
# td_tags = tr_tag.select('*')  // 자식요소의 태그들을 모두 불러 온다.
# print(td_tags)

for tag in td_tags:
    print(tag.get_text())




## 꿈의 직장 전화번호 모으기 II
# 실습 설명
생각해 보니 오렌지 보틀 지점의 전화번호에 더해서, 지점 이름(지점이 위치한 도시)과 주소도 있으면 도움이 될 것 같습니다. 오렌지 보틀의 웹사이트에 가서, 모든 지점의 이름, 주소, 그리고 전화번호를 branch_infos 리스트에 저장해 주세요.

한 지점에 대한 지점 이름, 주소, 전화번호를 리스트 형식으로 branch_infos에 넣어 주세요. 리스트도 보통 값과 다를 것 없이 append()해 주면 됩니다.


branch_infos.append([branch_name, address, phone_number])
a7y0og18d-prac3-1(rev).png

참고로 지점 이름은 주황색 박스 안에 있는 'San Jose', 'GREAT BARTON' 같은 부분이고, 주소는 검은색 글씨로 되어있는 '4823 Fairway Drive', '106 Lammas Street' 같은 부분입니다.

# 해설
먼저 저번처럼 오렌지 보틀 웹사이트의 코드를 가져온 다음, soup을 만들어 줍니다. branch_infos 리스트도 만들어 줄게요.


# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')

branch_infos = []
그리고 웹사이트 코드를 좀 살펴볼게요.

a7y0og18d-prac3-1(rev).png

지점 이름과 전화번호를 가져오는 방법은 여러 가지가 있습니다.

지점 이름 리스트와 전화번호 리스트를 따로 만들고 나중에 branch_infos리스트로 합쳐주는 방법
각 지점의 이름과 전화번호를 바로 branch_infos에 넣어주는 방법
두 번째 방법을 살펴보도록 할게요.

먼저 지점 정보 전체를 담고 있는 <div> 태그를 가져와 봅시다.


<div class="branch">
    ...
</div>

# 모든 지점에 대한 태그 가져오기
branch_tags = soup.select('div.branch')
<div> 태그에 대한 선택자(div)와 class="branch"에 대한 선택자(.branch)를 붙여 써 줬습니다.

이제 반복문을 활용해서 각 지점의 이름과 전화번호를 가져오면 됩니다. 지점 이름은 class 이름이 city인 <p> 태그 안에 있고요.


<p class="city">GREAT BARTON</p>
주소는 class 이름이 address인 <p> 태그 안에 있고요.


<p class="address">106 Lammas Street</p>
전화번호는 전에 봤던 것처럼 class 이름이 phoneNum인 span 태그 안에 있습니다.


<span class="phoneNum">070 3460 5076</span>
이걸 코드로 짜면 아래와 같은데요.


for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city')
    address = branch_tag.select_one('p.address')
    phone_number = branch_tag.select_one('span.phoneNum')
먼저 각 선택자로 하나의 태그만 가져오면 되니까 select_one()을 써 줬고요, 문서 전체가 아닌 div 태그 안에서 태그를 가져오는 거기 때문에 soup.select_one() 대신 branch_tag.select_one()을 써 줬습니다.

그리고 사실 우리가 원하는 건 태그가 아니라 태그 안에 있는 텍스트죠? .get_text()를 추가해 줍시다.


for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
이제 branch_infos에 branch_name, address, phone_number를 추가해 주면 됩니다.


for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    branch_infos.append([branch_name, address, phone_number])

# 모범 답안

import requests
from bs4 import BeautifulSoup

# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')

branch_infos = []

# 모든 지점에 대한 태그 가져오기
branch_tags = soup.select('div.branch')

for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    branch_infos.append([branch_name, address, phone_number])

# 테스트 코드
print(branch_infos)



## find, find_all 메소드

Beautiful Soup에는 필요한 태그를 선택하는 방법이 크게 두 가지 있습니다:

.select_one(), .select() 메소드 활용
.find(), .find_all() 메소드 활용
1번, 2번 다 기능이 비슷하기 때문에 둘 중 하나만 아셔도 무방한데요, 2번 방법도 많이 쓰이기 때문에 이번 레슨에서 설명드리겠습니다.

find 와 find_all
find()는 select_one()과 비슷하고, find_all()은 select()와 비슷합니다.

find()는 매칭되는 태그를 (최대) 하나만 가져오는데 쓰이고 find_all()은 매칭되는 태그를 모두 가져오는데 쓰이는 거죠. 리턴값의 형식도 똑같습니다. find()는 태그 자체나 None을 리턴하고 find_all()은 리스트를 리턴합니다.

하지만find(), find_all()은 CSS 선택자를 쓰지 않고 여러 파라미터를 활용합니다. 지금부터 find(), find_all()의 문법을 설명드릴 텐데, 둘 다 문법이 똑같기 때문에 find_all() 위주로 예시를 들게요.

태그 이름 사용
find(), find_all()도 soup에도 쓸 수 있고, 태그에도 쓸 수 있습니다.


soup.find_all('tagname')
# 예: soup.find_all('p')

soup.find_all(['tagname1', 'tagname2'])
# 예: soup.find_all(['p', 'a'])
태그 이름으로 태그를 선택하려면 태그 이름을 파라미터로 넣어주거나, 여러 태그 이름을 리스트로 넣어줍니다.

첫 번째 예시는 HTML 문서에서 모든 p 태그를 찾는 것이고, 두 번째 예시는 HTML 문서에서 모든 p 태그와 a 태그를 찾는 겁니다.

참고로 모든 태그를 선택할 수도 있는데 이건 태그 안에 있는 모든 태그를 가져올 때 유용하겠죠?


tag.find_all(True)
모든 태그를 가져오려면 find_all() 안에 True를 넣어 줍니다.

속성 사용

soup.find_all('tagname', attr1='val1', attr2='val2', ...)
# 예: soup.find_all('p', id="some-id", class_="some-class")
태그를 찾는데 태그의 속성을 이용해서 필터를 하고 싶다면 속성 이름과 속성 값을 쭉 써 주면 됩니다.

id와 class도 일반 속성처럼 써 주면 되는데 (CSS 선택자는 id와 class 속성을 위한 특수 문법 #과 .이 있었던 것 기억하시죠?) 유일하게 속성 중 class는 class_로 써 줘야 합니다. 파이썬에서 class는 클래스를 만드는데 사용되는 이미 예약된 단어이기 때문이죠.

위 예시는 id가 some-id이고 class가 some-class인 p 태그를 모두 찾는다는 뜻입니다.

참고로 속성만으로 필터를 할 수도 있습니다. 그냥 태그 이름을 안 써 주면 됩니다.


soup.find_all(attr1='val1', attr2='val2', ...)
# 예: soup.find_all(id="some-id", class_="some-class")
위 예시는 id가 some-id이고 class가 some-class인 모든 태그를 찾는다는 뜻입니다.

마지막으로 find(), find_all()은 특정 속성이 있는지 없는지를 조건으로 사용할 수 있습니다. 속성 값 대신 True나 False를 써 주면 됩니다.


# 예: soup.find_all('p', id=True, class_=False)
위 예시는 id 속성은 있지만 class 속성은 없는 p 태그를 모두 찾는다는 뜻입니다.

중첩 태그
find(), find_all()은 중첩에 대한 문법이 없기 때문에, 태그 안에 중첩된 태그를 찾으려면 태그에 find(), find_all()을 사용해야 합니다.


...
    <div class="some-class">
      <p>Paragraph 1</p>
      <p>Paragraph 2</p>
      <p>Paragraph 3</p>
    </div>
...

# select - 선택자로 중첩된 p 태그 선택
soup.select('div.some-class p')

# find_all - 먼저 div 태그를 가져온 후 div 태그에 find_all()
div_tag = soup.find('div', class_='some-class')
div_tag.find_all('p')
앞서 말씀드렸듯이 두 방식의 기능은 거의 똑같기 때문에 더 편하신 방식을 사용하시면 됩니다. 하지만 다음 챕터에서는 Selenium이라는 도구에 대해 배워볼 건데, Selenium을 사용할 때는 CSS 선택자를 계속 쓸 겁니다. 이 점 유의해 주세요.


##### 웹페이지 내용 스크랩 #####
import requests
from bs4 import BeautifulSoup


response = requests.get("https://ashasleti.tistory.com/875")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

td_tags = soup.select('div.tt_article_useless_p_margin')

for tag in td_tags:
    print(tag.get_text())


###############################


## 태그 중복성 확인하기

import requests
from bs4 import BeautifulSoup

response = requests.get("https://workey.codeit.kr/music")
music_page = response.text

soup = BeautifulSoup(music_page, 'html.parser')

print(soup.select('ul.popular__order li'))


## 태그에서 텍스트 빼오기
import requests
from bs4 import BeautifulSoup

response = requests.get("https://workey.codeit.kr/music")
music_page = response.text

soup = BeautifulSoup(music_page, 'html.parser')

popular_artists = []

# for tag in soup.select('ul.popular__order li'):
#     popular_artists.append(tag.get_text().strip())

for tag in soup.select('ul.popular__order li'):
    popular_artists.append(list(tag.stripped_strings)[1]) //줄바꿈 공백 없애기
    
print(popular_artists)    


## 파이썬 문자열 다루기

# 자르고 더하기
파이썬 문자열은 슬라이싱을 통해 자를 수 있습니다.

각 인덱스에 하나의 글자(문자)가 있다고 생각하시면 됩니다.


message = "I love coding"

message1 = message[:7] # message 앞에 7글자 잘라내기
print(message1)

message2 = message[-6:] # message 뒤에 6글자 잘라내기
print(message2)

message3 = message[::2] # message에서 2글자씩 건너뛰기
print(message3)

I love 
coding
Ilv oig
+ 연산자를 사용하면 문자열을 서로 더할 수도 있습니다.


message1 = "I love "
message2 = "coding"

print(message1 + message2)

I love coding

# Strip
strip()은 문자열 양 옆에 공백을 제거해 줍니다.

의도치 않게 데이터의 시작이나 끝에 공백이 들어있을 때, 이걸 제거하기 위해 사용합니다.

문자열 중간에 있는 공백은 그대로 남아있습니다.


message = " 새로운 코딩 교육의 시작, 코드잇 "

print(message.strip())

새로운 코딩 교육의 시작, 코드잇

# Split
split()은 특정 문자(구분자)를 기준으로 문자열을 분리해 줍니다.

데이터에서 특정 부분을 추출해 낼 때 유용합니다.


phone_number = "010-0000-1111"

print(phone_number.split("-")) 
print(phone_number.split("-")[2]) # 전화번호 마지막 4자리만 

["010", "0000", "1111"]
1111
구분자를 아무것도 넣지 않으면, 기본적으로 공백이 구분자가 됩니다. 여기서 공백은 공백 한 칸이 아닌 연속되는 공백을 뜻합니다.


address = "서울특별시 종로구   세종로\n 청와대로      1"

print(address.split())

["서울특별시", "종로구", "세종로", "청와대로", "1"]
(위에서 \n은 줄바꿈을 뜻하는 특수문자입니다)

# Join
join()을 활용하면 여러 문자열을 합칠 수 있습니다.

리스트에 있는 모든 문자열을 join() 대상의 문자열 기준으로 합쳐줍니다.


phone_number_segments = ["010", "0000", "1111"]

print("-".join(phone_number_segments))

010-0000-1111

# Replace
replace()라는 걸 사용하면 문자열의 일부분을 다른 문자열로 바꿀 수 있습니다.

파라미터로 두 문자열을 받는데요, 첫 번째 문자열을 모두 두 번째 문자열로 바꿔줍니다.

예를 들어 아래 코드는 메시지의 'Codeit'이라는 부분을 다 한글 '코드잇'으로 바꿔줍니다.


message = "Codeit은 여러분을 응원합니다. Codeit은 여러분과 함께합니다. 새로운 코딩 교육의 시작, Codeit!"

print(message.replace("Codeit", "코드잇")) 

코드잇은 여러분을 응원합니다. 코드잇은 여러분과 함께합니다. 새로운 코딩 교육의 시작, 코드잇!

# Lower, Upper, Capitalize
이번에는 영어 처리인데요.

lower()는 모두 소문자로,

upper()는 모두 대문자로,

capitalize()는 앞 글자만 대문자로 바꾸고 나머지는 소문자로 바꾸는 겁니다.


word = "codeIt" # I만 대문자

print(word.lower())
print(word.upper())
print(word.capitalize())

codeit
CODEIT
Codeit

# 유의사항
모든 문자열 메소드는 문자열 자체를 변형하는 것이 아니라, 변형된 문자열을 리턴하는 겁니다.

예)


word = "codeIt" # I만 대문자

print(word.lower())
print(word.upper())
print(word.capitalize())
print(word) # word는 그대로임

codeit
CODEIT
Codeit
codeIt
변형된 값을 계속 사용하고 싶다면 리턴값을 새로운 변수에 저장해 줘야겠죠?


word = "codeIt" # I만 대문자

lower_case_word = word.lower()
upper_case_word = word.upper()
capitalized_word = word.capitalize()

print(lower_case_word)
print(upper_case_word)
print(capitalized_word)

codeit
CODEIT
Codeit


## 검색어 순위 받아오기

# 실습 설명
이번에는 음악 사이트(https://workey.codeit.kr/music/index)의 검색어 순위를 받아오려 합니다.

'인기 아티스트' 아래에 있는 '검색어 순위'의 1위~10위 데이터를 popular_searches 리스트에 담아 주세요.

순위, 순위 변동을 제외한 검색어만 리스트에 담아 주세요.

# 실습 결과
['Queen', '방탄소년단', '아이유', '거미', '폴킴', '김범수', '헤이즈', '트와이스', '박효신', '신용재']

# 해설
먼저 soup을 만들어 줍니다.


response = requests.get("https://workey.codeit.kr/music/index")

soup = BeautifulSoup(response.text, 'html.parser')


검색어 순위 리스트는

<ul class="rank__order">
    ...
</ul>
class 이름이 rank__order인 ul 태그에 있죠?

그리고 ul 태그 안에 있는 li 태그에 원하는 정보가 있습니다.

이전 영상에서 봤던 것처럼, 중첩 요소 선택자를 이용해서 li 태그들을 가져와 봅시다.


# <ul class="rank__order">에 중첩된 <li> 태그 선택
for tag in soup.select('ul.rank__order li'):
CSS 선택자에서 띄어쓰기는 중첩 요소를 뜻합니다.

이제 각 li 태그 안에서 검색어를 가져와야 하는데요. 다시 위에 사진을 보시면 li 태그 안에는 총 3가지 텍스트 요소가 있습니다:

그리고 검색어에는 공백도 있고요. 태그의 텍스트 요소를 따로 가져오려면 .stripped_strings를 쓰면 됩니다 (.stripped_strings는 각 텍스트의 공백도 제거해 줍니다).

그런데 .stripped_strings를 사용할 때는, 그걸 리스트로 만들어 줘야 합니다.


list(tag.stripped_strings)
첫 번째 tag의 stripped_strings 리스트를 출력해 보면 아래와 같이 나옵니다.


print(list(tag.stripped_strings))

['1', '0', 'Queen']
우리는 세 번째 요소만 가져오면 되겠죠?


search_word = list(tag.stripped_strings)[2]
그럼 이걸 popular_searches에 추가해 주면, 코드가 완성됩니다.


# <ul class="rank__order">에 중첩된 <li> 태그 선택
for tag in soup.select('ul.rank__order li'):

    # li 태그의 텍스트 요소 중 세 번째 가져오기
    search_word = list(tag.stripped_strings)[2]
    popular_searches.append(search_word)

# 해답 코드

import requests
from bs4 import BeautifulSoup

response = requests.get("https://workey.codeit.kr/music/index")

soup = BeautifulSoup(response.text, 'html.parser')

popular_searches = []

# <ul class="rank__order">에 중첩된 <li> 태그 선택
for tag in soup.select('ul.rank__order li'):

    # li 태그의 텍스트 요소 중 세 번째 가져오기
    search_word = list(tag.stripped_strings)[2]
    popular_searches.append(search_word)

# 출력 코드
print(popular_searches)

['Queen', '방탄소년단', '아이유', '거미', '폴킴', '김범수', '헤이즈', '트와이스', '박효신', '신용재']


# 하루 정답
import requests
from bs4 import BeautifulSoup

# 여기에 코드를 작성하세요
response = requests.get("https://workey.codeit.kr/music/index")
music_page = response.text

soup = BeautifulSoup(music_page, 'html.parser')

popular_searches = []

for tag in soup.select('ul.rank__order li'):
    popular_searches.append(list(tag.stripped_strings)[2])

# 테스트 코드
print(popular_searches)


## 태그에서 속성 빼오기

import requests
from bs4 import BeautifulSoup

response = requests.get("https://workey.codeit.kr/music")
music_page = response.text

soup = BeautifulSoup(music_page, 'html.parser')

print(soup.select_one('img').attrs) 
# 태그의 모든속성(사전 형식으로 저장되어 있다.)

# print(soup.select_one('img')['src'])



## Beautiful Soup 더 간결하게

이번 레슨에서는 Beautiful Soup 코드를 조금 더 간결하게 쓰는 팁을 몇 가지 드릴게요.

# .tagname 문법
첫 번째는 .select_one('tagname'), 즉 태그 이름으로 태그를 찾는 코드를 더 간결하게 쓰는 방법입니다.

태그 이름으로 태그를 찾는 방법은 워낙 많이 쓰이기 때문에 더 간결한 문법이 존재하는데요. 바로 .tagname입니다. 아래와 같이 쓰면 됩니다.


soup.tagname # soup.select_one('tagname')과 동일
tag.tagname # tag.select_one('tagname')과 동일
예를 들어, 아래 div 태그에서 h2, p, a 태그의 정보를 가져와야 한다고 합시다.


...
    <div class="data">
      <h2>제목</h2>
      <p>내용</p>
      <a href="www.example.com">링크</a>
    </div>
...
select를 이용하면 아래와 같이 코드를 써야 하는데요.


div_tag = soup.select_one('div.data')
title = div_tag.select_one('h2').get_text()
content = div_tag.select_one('p').get_text()
link = div_tag.select_one('a')['href']
.tagname 문법을 쓰면, 아래와 같이 코드가 바뀌는 거죠.


div_tag = soup.select_one('div.data')
title = div_tag.h2.get_text()
content = div_tag.p.get_text()
link = div_tag.a['href']
코드가 더 간단해졌죠?

# 메소드 체이닝
두 번째는 메소드 체이닝 (Method Chaining)입니다.

별로 어려운 개념은 아닌데요. 체인(Chain)은 사슬을 뜻하죠? 메소드 체이닝은 메소드의 리턴값을 변수에 저장하지 않고, 리턴값에 바로 또 다른 메소드를 호출하는 것을 뜻합니다. 메소드를 사슬처럼 연결하기 때문에 메소드 체이닝이라고 부르는 거죠.

예를 들어 select_one() 메소드는 태그를 리턴하는데요. select_one() 바로 뒤에 태그에 쓰는 메소드를 호출해 주면 됩니다.  select_one(), select(), get_text() 같은 것들이 있겠죠?


# 1. selector1에 매칭되는 태그를 찾은 다음 그 안에서 selector2에 매칭되는 태그 선택
soup.select_one(selector1).select_one(selector2)

# 2. selector1에 매칭되는 태그를 찾은 다음 그 안에서 selector2에 매칭되는 모든 태그 선택
soup.select_one(selector1).select(selector2) 

# 3. selector1에 매칭되는 태그를 찾은 다음 그 안에서 텍스트 추출
soup.select_one(selector1).get_text()
메소드 체이닝을 사용하면 코드가 더 간결해 지지만, 코드의 가독성은 떨어질 수 있으니까 주의해 주세요. 특히 체인이 너무 길어지면 코드의 가독성이 떨어질 겁니다.

참고로 이번 레슨 첫 부분에서 본 .tagname 문법도 체이닝을 할 수 있습니다. 태그를 계속 타고 들어가야 할 때 .tagname을 계속 붙여 써 주면 됩니다.


...
    <div class="data">
      <div>
        <h2>제목</h2>
        <p>내용<b>키워드</b>내용</p>
        <a href="www.example.com">링크</a>
      </div>
    </div>
...
예를 들어 위와 같은 HTML 구조에서 b 태그 안에 있는 키워드를 가져와야 한다면. 아래와 같은 코드를 쓸 수 있습니다.


div_tag = soup.select_one('div.data')
keyword = div_tag.div.p.b.get_text()
print(keyword)

키워드


### Beautiful Soup 정리 노트

## Beautiful Soup 라이브러리
Beautiful Soup은 HTML 코드에서 필요한 정보를 추출해 주는 라이브러리입니다. Beautiful Soup을 이용하면 HTML 문서에서 필요한 태그를 쉽게 찾을 수 있고, 태그 안에 있는 다양한 정보도 쉽게 가져올 수 있습니다.

Beautiful Soup 공식 문서 

## Beautiful Soup 임포트

import requests
from bs4 import BeautifulSoup
파이썬에서는 Beautiful Soup 라이브러리를 bs4라고 하는데요. bs는 Beautiful Soup을 줄인 것이고, 4는 라이브러리 버전을 뜻합니다. 4가 가장 최신 버전입니다.

bs4에서 BeautifulSoup이라는 걸 가져와 줍니다.

Beautiful Soup 라이브러리로는 웹에 요청을 보낼 수 없기 때문에 requests 라이브러리도 임포트해 줍니다.

## BeautifulSoup 만들기

# URL에 요청 보내기
response = requests.get(URL) 

# 받아온 HTML 코드로 BeautifulSoup 만들기
soup = BeautifulSoup(response.text, 'html.parser')
라이브러리를 임포트해 주었으면, 원하는 웹사이트에 요청을 보내고, 받아온 HTML 코드로 BeautifulSoup이라는 걸 만들어 줍니다. BeautifulSoup을 만들 때 쓰이는 html.parser는 HTML 코드를 정리해 줍니다. 일반적으로 parser는 문서를 읽어 들여서 프로그램이 쉽게 사용할 수 있는 형식으로 변환시켜 주는 툴입니다.

이제 soup을 통해 필요한 태그를 쉽게 가져올 수 있습니다.

## 필요한 태그 가져오기
.select()와 .select_one() 메소드는 선택자를 통해 필요한 태그를 가져옵니다.

# select
.select()는 선택자에 매칭되는 모든 태그를 리스트 형식으로 리턴해 줍니다. 매칭되는 태그가 없거나, 하나뿐이어도 리스트를 리턴합니다.


soup.select(selector)
# 리턴값: [tag1, tag2, ...]

# select_one
.select_one()은 선택자에 매칭되는 태그 중 HTML 문서에서 가장 먼저 나타나는 태그를 리턴해 줍니다. .select()와 달리 태그 자체를 리턴해 줍니다 (매칭되는 태그가 없는 경우 None을 리턴합니다).


soup.select_one(selector)
# 리턴값: tag or None
웹사이트의 HTML 구조가 복잡해서 필요한 태그를 가져오는 것이 어려운 경우:

태그 선택 범위를 넓게 잡고 슬라이싱 등으로 필요한 태그들만 가져오거나
태그 선택 과정을 나눠서 바깥쪽 태그를 선택하고, 태그 안에서 또 태그를 찾습니다. 태그에도 .select()와 .select_one()을 사용할 수 있습니다.

## 태그에서 필요한 정보 가져오기
# 텍스트 가져오기
태그의 텍스트를 가져오는 방법은 여러 가지가 있습니다.


tag.get_text()
list(tag.strings)
list(tag.stripped_strings)

# 1. get_text
.get_text()는 태그 안에 있는 모든 텍스트를 하나로 합쳐서 리턴해 줍니다. 사실 .get_text()안에는 파라미터를 넣을 수도 있는데요. 파라미터를 넣어주면, 텍스트를 합칠 때 파라미터로 넣은 문자열을 텍스트 사이에 넣어 줍니다.


...
    <tag>
      Hello <b>World<b>!
    </tag>
...

# tag 안에 텍스트: 'Hello ', 'World', '!'

tag.get_text()
# 리턴값: 'Hello World!'

tag.get_text(' ')
# 리턴값: 'Hello  World !'

tag.get_text(',')
# 리턴값: 'Hello ,World,!'

# 2. strings
.strings는 태그 안에 있는 텍스트를 따로 리턴해 줍니다. 태그 안에 있는 텍스트 일부만 필요할 때 유용하겠죠? .strings를 쓸 때는 결괏값을 list()로 감싸줘야 합니다.


# tag 안에 텍스트: 'Hello ', 'World', '!'

list(tag.strings)
# 리턴값: ['Hello ', 'World', '!']

# 3. stripped_strings
.stripped_strings는 .strings와 똑같지만 각 텍스트 요소의 양옆의 공백을 제거해 줍니다. 아예 공백인 텍스트는 .stripped_strings에 포함되지 않습니다.


# tag 안에 텍스트: 'Hello ', 'World', '!'

list(tag.stripped_strings)
# 리턴값: ['Hello', 'World', '!']

## 속성 가져오기

tag['attr']
태그의 속성은 tag[]를 사용해서 가져옵니다. []안에 속성 이름을 넣어줍니다.


tag.attrs
태그의 모든 속성은 .attrs로 확인할 수 있습니다.


## 데이터를 엑셀 파일로

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook

wb = Workbook(write_only=True)
ws = wb.create_sheet('TV ratings')
ws.append(['순위', '채널', '프로그램', '시청률'])

response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

for tr_tag in soup.select('tr')[1:]:
    td_tags = tr_tag.select('td')
    row = [
        td_tags[0].get_text(), # 순위
        td_tags[1].get_text(), # 채널
        td_tags[2].get_text(), # 프로그램
        td_tags[3].get_text(), # 시청률
    ]
    ws.append(row)
    
wb.save('시청률_2010년1월1주차'.xlsx)


## openpyxl 정리 노트

설치 : pip3 install openpyxl

openpyxl 라이브러리로 엑셀 파일을 만들 때 사용되는 문법을 정리해 드릴게요.

# 라이브러리 임포트
from openpyxl import Workbook
먼저 라이브러리를 임포트해 줍니다. 라이브러리에서 필요한 도구는 Workbook입니다.

# 워크북 생성
wb = Workbook()
wb = Workbook(write_only=True)
엑셀 파일을 워크북이라고 합니다. 워크북을 만들기 위해선 Workbook()을 쓰면 되는데요. 파일에서 데이터를 읽어오지 않고, 데이터를 저장하기만 하는 경우 write only 모드를 써 주는것이 퍼포먼스에 도움이 됩니다. write only 모드는 write_only=True로 설정해 줄 수 있습니다.

# 워크시트 생성
ws = wb.create_sheet()
ws = wb.create_sheet('워크시트_이름')
다음은 워크시트입니다. 워크시트는 데이터가 저장돼 있는 시트(sheet) 하나를 뜻합니다. 워크북에는 여러 워크시트가 있을 수 있습니다. 워크시트를 만들려면 wb.create_sheet() 메소드를 사용합니다. 파라미터로 워크시트 이름을 넣어줄 수 있고, 안 넣어주면 디폴트 이름 'Sheet1' (혹은 순서에 따라 'SheetN')이 사용됩니다.

# 행 추가
ws.append([data1, data2, data3])
워크시트에 행을 추가하려면 행 요소를 리스트로 만들어서 ws.append()에 넣어줍니다. append는 '무언가를 붙이다'라는 뜻을 가지고 있습니다.

# 워크북 저장
wb.save('워크북_이름.xlsx')
워크북에 모든 데이터를 써 줬으면, 워크북을 저장해 줘야 합니다. wb.save() 메소드를 쓰고, 파일 이름을 파라미터로 넣어줍니다. 파일 이름에 .xlsx 확장자도 잊지 마세요!


## 데이터를 CSV파일로

# CSV란?
CSV는 Comma Separated Values의 약자입니다. '값이 쉼표(,)로 구분되어 있다' 이런 뜻인데요. CSV는 표 데이터를 저장하는데 많이 쓰이는 파일 형식입니다.

엑셀(.xlsx)은 Microsoft Excel 전용 파일 형식이라면 CSV(.csv)는 일반적으로 표 데이터를 저장하는 파일 형식입니다. CSV 파일은 메모장(Windows), Numbers(Mac), Microsoft Excel 등 여러 소프트웨어에서 잘 열립니다.

엑셀을 안 쓰시는 분들은 표 데이터를 CSV 형식으로 저장하시면 됩니다.

# CSV 형식
data.csv 다운로드

사람 이름, 키, 몸무게가 들어가 있는 예시 csv 파일입니다. 파일을 열어보면 아래와 같은 내용이 있는데요.

5cjn9z71n-a.png

각 데이터가 쉼표로 구분돼 있고, 행은 줄 단위로 구분돼 있습니다. 아주 직관적이죠? 이게 바로 CSV 형식입니다.

CSV 파일을 Microsoft Excel, Numbers, 또는 pandas (파이썬 데이터 분석 라이브러리)같은 데이터 분석 툴로 열면 표로 인식이 됩니다. 아래는 data.csv 파일을 Numbers로 열은 모습입니다.

zr9wv59jw-b.png

## 파이썬으로 CSV 파일 작성하기
그럼 파이썬으로는 어떻게 CSV 파일을 만들까요?

# CSV 모듈 임포트
csv라는 모듈을 사용하면 쉽게 만들 수 있습니다. 스탠다드 모듈이기 때문에 따로 설치를 안 해주셔도 됩니다.

import csv

# CSV 파일 생성
먼저 CSV 파일을 만들어 주고, 파일에 데이터를 CSV 형식으로 전달해 주는 csv.writer라는 도구를 활용할 겁니다.


# CSV 파일 생성
csv_file = open('file_name.csv', 'w')
csv_writer = csv.writer(csv_file)
파일을 만들려면 open()을 쓰는데, 파이썬으로 파일을 다뤄보셨다면 전에 만나보셨을 겁니다. open() 안에 파일 이름과 모드(mode)를 써 줍니다. 우리는 데이터를 파일에 써 줄 거니까 'w' (write)를 써 줍니다.

그리고 csv.writer()안에 방금 만들어준 csv_file을 넣어 줍니다.

# 행 추가
CSV 파일에 행을 추가하기 위해서는 우리가 만든 csv_writer를 사용합니다.


# CSV 파일에 행 추가
csv_writer.writerow([data1, data2, ...])
.writerow() 메소드를 활용하면 됩니다. 엑셀 파일에 행을 추가할 때 썼었던 ws.append()와 아주 비슷하죠? 행에 들어갈 데이터를 리스트 형식으로 넘겨줍니다.

헤더 행, 데이터 행 상관없이 csv_writer.writerow()를 사용하시면 됩니다.

# CSV 파일 닫기
파이썬에서 파일을 열었으면, 사용이 끝난 후 닫아주는 것이 좋은 습관입니다.


# CSV 파일 닫기
csv_file.close()
사실 프로그램이 종료되면 파일이 자동으로 닫히기 때문에 지금 같이 단순한 프로그램을 짤 때는 큰 상관이 없는데요. 복잡한 프로그램에서 파일을 닫아주지 않으면, 프로그램이 실행되는 동안 그 파일을 다른 곳에서 사용할 수 없게 됩니다. (csv_writer 가 아닌 csv_file을 닫아주셔야 합니다.)

# 티비랭킹닷컴 데이터를 CSV 파일로
이전 영상에서는 티비랭킹닷컴의 데이터를 엑셀 파일로 저장해 봤는데, 이번에는 똑같은 데이터를 CSV 파일로 저장하는 코드를 보여드릴게요.


## 엑셀 파일:

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook

# 워크북 생성
wb = Workbook(write_only=True)
ws = wb.create_sheet('Data')

# 헤더 행 추가
ws.append(['순위', '채널', '프로그램', '시청률'])

response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')

for tr_tag in soup.select('tr')[1:]:
    td_tags = tr_tag.select('td')
    row = [
        td_tags[0].get_text(),
        td_tags[1].get_text(),
        td_tags[2].get_text(),
        td_tags[3].get_text()
    ]
    # 데이터 행 추가
    ws.append(row)

# 워크북 저장
wb.save('시청률_2010년1월1주차.xlsx')


## CSV 파일:

import csv
import requests
from bs4 import BeautifulSoup

# CSV 파일 생성
csv_file = open('시청률_2010년1월1주차.csv', 'w')
csv_writer = csv.writer(csv_file)

# 헤더 행 추가
csv_writer.writerow(['순위', '채널', '프로그램', '시청률'])

response = requests.get("https://workey.codeit.kr/ratings/index")
rating_page = response.text

soup = BeautifulSoup(rating_page, 'html.parser')


### 꿈의 직장 전화번호 모으기 III
시작하기 전에
지금부터는 데이터를 엑셀 파일로 저장하는 과제가 몇 개 있을 건데요. 엑셀을 안 쓰시는 분들은, CSV 파일로 저장하셔도 좋습니다.

이번 과제의 해설에서는 엑셀, CSV를 둘 다 설명드릴 텐데요. 엑셀과 CSV 문법이 비슷하기 때문에, 다음부터는 엑셀 문법 중심으로만 설명드릴게요.

# 실습 설명
이번 과제는 사이트 내 실행이 지원되지 않습니다. 직접 과제를 해결하고, 코드잇 실행기의 '채점' 버튼을 눌러 직접 채점해 보세요.

오렌지 보틀 지점들에 대한 정보를 엑셀 파일에 저장하려고 합니다.

모든 오렌지 보틀 지점의 이름, 주소, 전화번호를 엑셀 파일 (또는 CSV 파일)에 저장해 주세요.

파일 이름은 '오렌지_보틀.xlsx' (CSV는 '오렌지_보틀.csv')로 해주시고, 엑셀의 경우 워크시트 이름은 따로 설정 안 해주셔도 됩니다.


ws = wb.create_sheet()
이렇게 괄호 안에 워크시트 이름을 안 넣어주면 'Sheet1'이라는 워크시트가 생깁니다.

헤더 행은 '지점 이름', '주소', '전화번호'로 해 주시고, 모든 오렌지 보틀 지점의 데이터를 저장해 주세요.

템플릿에 있는 코드는 '꿈의 직장 전화번호 모으기 II' 과제의 해답 코드입니다.

# 실습 결과
셀프 채점
다음 항목들이 제대로 구현되었는지 확인해 보세요!

'오렌지_보틀.xlsx' ('오렌지_보틀.csv') 파일이 잘 생성돼요.

헤더 행 '지점 이름', '주소', '전화번호'가 제일 위에 들어가 있어요.

데이터 행 총 57개가 알맞은 형식으로 저장돼 있어요.

# 해설
먼저 필요한 라이브러리를 가져옵니다.


# 엑셀
from openpyxl import Workbook 

# CSV
import csv
엑셀은 워크북과 워크시트를 만들어 주고, CSV는 CSV 파일과 csv writer를 만들어 줍니다.


# 엑셀
wb = Workbook(write_only=True)
ws = wb.create_sheet()

# CSV
csv_file = open('오렌지_보틀.csv', 'w')
csv_writer = csv.writer(csv_file)
엑셀의 경우 create_sheet() 안에 이름을 주지 않으면 'Sheet1'이라는 워크시트가 생성됩니다.

그리고 헤더 행을 추가해 줍니다.


# 엑셀
wb = Workbook(write_only=True)
ws = wb.create_sheet()
ws.append(['지점 이름', '주소', '전화번호'])

# CSV
csv_file = open('오렌지_보틀.csv', 'w')
csv_writer = csv.writer(csv_file)
csv_writer.writerow(['지점 이름', '주소', '전화번호'])
그리고 데이터를 추가해 주면 되는데요. 이전 과제에서 필요한 정보는 모두 가져왔으니, 이전 과제의 코드에 정보를 파일에 추가해 주는 코드만 써 주면 됩니다.

branch_infos.appned() 부분을 파일에 데이터를 추가하는 코드로 바꿔줍니다. (그리고 branch_infos = []도 지워주세요.)


# 엑셀
for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    ws.append([branch_name, address, phone_number])

# CSV
for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    csv_writer.writerow([branch_name, address, phone_number])

그리고 print(branch_infos) 부분은 지워주고, 마무리 작업을 해 줍니다.


# 엑셀
wb.save('오렌지_보틀.xlsx')

# CSV
csv_file.close()
모범 답안

## 엑셀

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook

# 워크북 생성
wb = Workbook(write_only=True)
ws = wb.create_sheet()
ws.append(['지점 이름', '주소', '전화번호'])

# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')

# 모든 지점에 대한 태그 가져오기
branch_tags = soup.select('div.branch')

for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    ws.append([branch_name, address, phone_number])

wb.save('오렌지_보틀.xlsx')


## CSV

import requests
from bs4 import BeautifulSoup
import csv

# CSV 파일 생성
csv_file = open('오렌지_보틀.csv', 'w')
csv_writer = csv.writer(csv_file)
csv_writer.writerow(['지점 이름', '주소', '전화번호'])

# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')

# 모든 지점에 대한 태그 가져오기
branch_tags = soup.select('div.branch')

for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    csv_writer.writerow([branch_name, address, phone_number])

csv_file.close()


## 연도별 시청률 데이터 엑셀 파일로 정리해 보기

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook


wb = Workbook(write_only=True)

for year in range(2010, 2019):
    ws = wb.create_sheet("{}년".format(year))
    ws.append(['기간', '순위', '채널', '프로그램', '시청률'])

    for month in range(1, 13):
        for weekIndex in range(0, 5):
            url = "https://workey.codeit.kr/ratings/index?year={}&month={}&weekIndex={}".format(year, month, weekIndex)
            response = requests.get(url)
            rating_page = response.text
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
                td_tags = tr_tag.select('td')
                period = "{}년 {}월 {}주차".format(year, month, weekIndex + 1)
                row = [
                    period, # 기간
                    td_tags[0].get_text(),  # 순위
                    td_tags[1].get_text(),  # 채널
                    td_tags[2].get_text(),  # 프로그램
                    td_tags[3].get_text(),  # 시청률
                ]
                ws.append(row)


wb.save('시청률'.xlsx)



## SBS 프로그램 엑셀 파일로 저장해 보기

# 실습 설명
이번 과제는 사이트 내 실행이 지원되지 않습니다. 직접 과제를 해결하고, 코드잇 실행기의 '채점' 버튼을 눌러 직접 채점해 보세요.

신입 PD로 입사한 나PD는 선배들과 함께 새로운 여행 예능 버라이어티 프로그램을 기획하게 되었습니다.

프로그램 기획 도중 선배로부터 SBS의 모든 프로그램들의 정보를 모아서 엑셀 파일로 보내달라는 요청을 받게 되었는데요.

나PD를 도와서 아래 조건에 따라서 티비랭킹닷컴의 SBS 프로그램 정보들만 모은 다음, 'SBS_데이터.xlsx' (또는 'SBS_데이터.csv') 파일에 저장해 주세요.

다른 방송사는 제외하고 채널 정보가 SBS인 프로그램의 정보들만 모아주세요.
티비랭킹닷컴에 있는 모든 기간의 데이터를 모아주세요. 기간은 '_년 _월 _주차' 형식으로 저장해 주세요.
연도별로 워크시트를 만들 필요 없이 하나의 워크시트에 쭉 저장해 주시면 됩니다. 워크시트 이름은 따로 설정 안 해주셔도 됩니다.
헤더 행, 즉 테이블 열은 '기간', '순위', '프로그램', '시청률'로 해 주시고, 열에 맞게 데이터를 저장해 주세요.
실습 결과


위 사진은 2010년 1월 데이터가 저장돼 있는 모습입니다.

셀프 채점
다음 항목들이 제대로 구현되었는지 확인해 보세요!

'SBS_데이터.xlsx' ('SBS_데이터.csv') 파일이 잘 생성돼요.


헤더 행 '기간', '순위', '프로그램', '시청률'이 제일 위에 들어가 있어요.


데이터 행 총 907개가 알맞은 형식으로 저장돼 있어요.

# 해설
먼저 필요한 라이브러리들을 가져오고, 워크북을 만들어 줍시다.


import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
세 가지 라이브러리가 필요하고요,


# 워크북 생성
wb = Workbook(write_only=True)
ws = wb.create_sheet()
ws.append(['기간', '순위', '프로그램', '시청률'])
워크북을 만들 때는 Workbook(), 워크시트를 만들 때는 wb.create_sheet(), 워크시트에 행을 추가할 때는 ws.append().

티비랭킹닷컴 데이터에서 SBS 프로그램들만 걸러 낸다고 해도, 결국 모든 웹 페이지를 가져와야겠죠? 일단 모든 웹 페이지를 가져와서 각 페이지를 BeautifulSoup으로 만들어 줍시다.


for year in range(2010, 2019):
    for month in range(1, 13):
        for weekIndex in range(0, 5):
            # 웹 페이지 가져와서 BeautifulSoup 생성
            url = "https://workey.codeit.kr/ratings/index?year={}&month={}&weekIndex={}".format(year, month, weekIndex)
            response = requests.get(url)
            rating_page = response.text
            soup = BeautifulSoup(rating_page, 'html.parser')
이 코드는 몇 번 보셨죠? 반복문을 활용해서 year, month, weekIndex를 바꿔주면서, 모든 url에 요청을 보내고, 응답으로 받은 HTML 코드를 BeautifulSoup으로 만들어 줍니다.

그다음에는, 모든 데이터 행을 가져와서 데이터 행의 '채널'이 SBS 인지 확인하는 코드를 작성해 봅시다.



행은 <tr> 태그로 이루어져 있는데, 데이터 행은 두 번째 행 부터니까 [1:]로 슬라이싱을 해 줍니다.


            ...            
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
그리고 태그 구조를 보시면 '채널'에 해당하는 태그는 <td class="channel">입니다. 각 <tr> 태그 안에서 <td class="channel">을 가지고 와서, 안에 있는 텍스트를 빼와 줍시다.


            ...            
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
                channel = tr_tag.select_one('td.channel').get_text()
어차피 각 <tr> 태그 안에는 <td class="channel">이 하나밖에 없으니까 select_one을 써 줬습니다.

이제 나머지 코드의 로직을 짜 보면 이렇게 되겠죠.


            ...            
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
                # 데이터 행의 채널이 SBS인 경우만 데이터 행 저장
                channel = tr_tag.select_one('td.channel').get_text()
                if channel == 'SBS':
                    # 데이터 행의 기간, 순위, 프로그램, 시청률 저장
우선 기간은 year , month, weekIndex를 활용하면 됩니다.


            ...            
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
                # 데이터 행의 채널이 SBS인 경우만 데이터 행 저장
                channel = tr_tag.select_one('td.channel').get_text()
                if channel == 'SBS':
                    # 데이터 행의 기간, 순위, 프로그램, 시청률 저장
                    period = "{}년 {}월 {}주차".format(year, month, weekIndex + 1)
format()을 사용하면 {} 괄호 안에 format의 파라미터가 순서대로 들어갑니다.



이제 나머지 정보를 가져오면 되는데, 아래 방법 중 두 번째 방법을 활용하도록 할게요.

tr_tag.select_one('td.rank')처럼 선택자를 이용해서 필요한 <td> 태그를 하나씩 가져온다.
tr_tag.select('td')처럼 모든 td 태그를 가져와서 인덱스를 활용한다.

            ...            
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
                # 데이터 행의 채널이 SBS인 경우만 데이터 행 저장
                channel = tr_tag.select_one('td.channel').get_text()
                if channel == 'SBS':
                    # 데이터 행의 기간, 순위, 프로그램, 시청률 저장
                    period = "{}년 {}월 {}주차".format(year, month, weekIndex + 1)
                    td_tags = tr_tag.select('td')
                    rank = td_tags[0].get_text()
                    program = td_tags[2].get_text()
                    percent = td_tags[3].get_text()
순위는 첫 번째, 프로그램은 세 번째, 시청률은 네 번째 <td> 태그입니다.

이제 이걸 워크시트에 저장해 줍니다.


                    ...
                    percent = td_tags[3].get_text()
                    ws.append([period, rank, program, percent])
다음으로 워크북을 저장해 주면 코드가 완성됩니다.


wb.save('SBS_데이터.xlsx')

## 모범 답안

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook

# 워크북 생성
wb = Workbook(write_only=True)
ws = wb.create_sheet()
ws.append(['기간', '순위', '프로그램', '시청률'])

for year in range(2010, 2019):
    for month in range(1, 13):
        for weekIndex in range(0, 5):
            # 웹 페이지 가져와서 BeautifulSoup 생성
            url = "https://workey.codeit.kr/ratings/index?year={}&month={}&weekIndex={}".format(year, month, weekIndex)
            response = requests.get(url)
            rating_page = response.text
            soup = BeautifulSoup(rating_page, 'html.parser')

            for tr_tag in soup.select('tr')[1:]:
                # 데이터 행의 채널이 SBS인 경우만 데이터 행 저장
                channel = tr_tag.select_one('td.channel').get_text()
                if channel == 'SBS':
                    # 데이터 행의 기간, 순위, 프로그램, 시청률 저장
                    period = "{}년 {}월 {}주차".format(year, month, weekIndex + 1)
                    td_tags = tr_tag.select('td')
                    rank = td_tags[0].get_text()
                    program = td_tags[2].get_text()
                    percent = td_tags[3].get_text()
                    ws.append([period, rank, program, percent])

wb.save('SBS_데이터.xlsx')


### 웹사이트 제어하기
## Selenium이란?

## Selenium 준비하기

Selenium을 작동시키려면 두 가지를 준비해야 합니다.

Selenium과 웹 드라이버 (Web Driver)입니다.

설치해 봅시다.

# 1. Selenium
Selenium 역시 외부 라이브러리입니다. 우리가 설치해 봤던 requests와 bs4처럼 pip을 이용해서 설치할 수 있습니다.

강의에서는 3.141 버전을 사용합니다. 원활한 진행을 위해 가능한 동일 버전 사용을 권장합니다.


pip3 install selenium==3.141
(PyCharm의) 터미널이나 cmd에 위 명령어를 입력해 주세요.
(PyCharm - 설정의 Project Interpreter 탭에서 설치하셔도 됩니다)
현재 설치된 버전은 터미널이나 cmd 에서 아래 명령으로 확인할 수 있습니다.


pip3 show selenium
만약 버전이 높거나 다르다면 아래와 같이 uninstall 후 install 명령으로 재설치 할 수 있습니다.


pip3 uninstall selenium

# 2. 웹 드라이버
웹 드라이버란?
웹 드라이버가 뭘까요? 일반적으로 드라이버(driver)는 운전자를 뜻하죠? 같은 의미입니다. 웹 드라이버는 웹 브라우저를 운전한다는 겁니다.

보통은 우리가 직접 마우스로 클릭을 한다거나, 키보드를 입력한다거나 하는 방식으로 웹 브라우저를 사용하는데, 이런 우리의 동작을 대신해서 웹 드라이버라는 것이 웹 브라우저를 움직이도록 한다는 거죠.

그리고 각 웹 브라우저에 해당하는 웹 드라이버가 있는데요. 이번 토픽에서는 구글 크롬을 사용하니까 크롬 드라이버를 설치해야겠죠?

크롬 드라이버 설치
크롬 드라이버는 크롬 버전마다 다르기 때문에 먼저 크롬 버전을 확인해야 합니다.



크롬 브라우저 우측 상단에 메뉴 버튼을 클릭한 다음 Help → About Google Chrome (도움말 → Chrome 정보)을 클릭하시면 크롬 버전을 확인하실 수 있습니다.



위 사진은 107 버전이죠?

만약 최신 버전이 아니라면 업데이트를 해달라는 메시지가 있을 겁니다. 클릭해서 업데이트를 해 주세요.

크롬 드라이버는 이 웹사이트에서 다운로드 받을 수 있습니다: https://chromedriver.chromium.org/downloads



크롬 버전에 맞는 링크를 클릭해 주세요.



다음과 같이 사용하는 OS와 프로세서에 맞게 다운로드를 해주세요.

Windows: chromedriver_win32.zip
Mac 인텔 칩: chromedriver_mac64.zip
Mac M 시리즈 칩: chromedriver_mac_arm64.zip
다운로드가 완료되면 좌측 하단의 버튼을 눌러주세요.



Windows는 아래와 같이 압축 풀기 버튼을 눌러주세요.



Mac은 자동으로 압축이 풀립니다.

4eniblqsn-a.png

그리고 Mac은 chromedriver를 열면 아래와 같은 오류가 날 수 있습니다.

dvxjmj31y-b.png

그렇다면 설정 메뉴로 가셔서 Security & Privacy (보안 및 개인 정보 보호) 아이콘을 클릭해 주세요.

6yzljbtwl-c.png

그런 다음 아래 Open Anyway (확인 없이 열기) 버튼을 눌러주세요.

rs79jhcap-d.png

아래와 같이 chromedriver가 잘 실행되면 셋업이 끝난 겁니다. 창을 닫아 주세요.

8zfbpwwge-e.png

크롬 드라이버 경로
이제 chromedriver를 사용하기 편한 위치로 옮기셔도 됩니다. 하지만 경로를 반드시 본인이 알고 있어야 합니다.

만약 경로를 모르겠다면?

Windows:

chromedriver를 선택한 후 홈 탭에서 경로 복사 버튼을 눌러 주면 chromedriver 경로가 복사됩니다. 웹 드라이버를 생성할 때 그대로 붙여넣으시면 됩니다.



Mac:

chromedriver를 우클릭 하세요.



이 상태에서 option 키를 누르면 Copy "chromedriver" → Copy "chromedriver" as Pathname ("chromedriver" 복사 → "chromedriver"의 경로 이름 복사) 이렇게 바뀝니다. 그걸 클릭해 주시면 경로가 복사됩니다. 웹 드라이버를 생성할 때 그대로 붙여넣으시면 됩니다.

# Selenium 실행
이제 준비가 모두 끝났습니다. 잘 되는지 간단한 동작을 한번 시켜 봅시다.


# Selenium 임포트
from selenium import webdriver

# 크롬 드라이버 생성
driver = webdriver.Chrome('chromedriver 경로를 넣어 주세요')

# 사이트 접속하기
driver.get('https://codeit.kr')
먼저 selenium 라이브러리에서 webdriver를 가져오고, 크롬 드라이버를 생성한 후 코드잇 웹사이트에 접속하는 코드입니다. 이 코드를 복사해서 실행해 주세요. 'chromedriver 경로를 넣어 주세요' 이 부분을 꼭 자신의 컴퓨터의 chromedriver 경로로 바꿔주세요 (경로를 모르시겠다면 위에 크롬 드라이버 경로 부분 참고하세요). 경로 예시: '/Users/codeit/Desktop/chromedriver'.

구글 크롬이 실행되고, 코드잇 웹사이트에 자동으로 접속되면 잘 실행이 되는 겁니다!

그럼 마지막으로 한 가지만 더 해 줄게요. driver를 다 사용했으면 driver를 종료해 주는 것이 좋습니다 (파이썬으로 파일을 열었으면, 사용이 끝난 후 닫아주는 것과 비슷합니다).


# 크롬 드라이버 종료
driver.quit() 
하지만 이 코드를 써 주면 웹 브라우저가 너무 빨리 종료되어서 무슨 동작이 있었는지 파악하기가 힘들 수 있기 때문에, 이번 챕터의 영상들에서는 종종 웹 드라이버를 종료하지 않을 때도 있을 겁니다.

경로를 매번 설정해 주는 것이 귀찮다면?
웹 드라이버를 생성할 때 경로를 매번 설정해 주는 것이 귀찮을 수도 있는데요. chromedriver를

윈도우는 C:\Windows
맥은 /usr/local/bin
경로로 옮겨주시면 웹 드라이버를 생성할 때 따로 경로를 설정해 주지 않아도 됩니다.


 # Selenium 임포트
from selenium import webdriver

# 크롬 드라이버 생성 - 경로 설정 필요 없음
driver = webdriver.Chrome()

# 사이트 접속하기
driver.get('https://codeit.kr')
유의사항
크롬은 자동으로 업데이트되기 때문에 잘 실행되던 Selenium 코드에서 갑자기 오류가 날 수 있습니다.


selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XX
크롬 버전과 크롬 드라이버 버전이 안 맞는 거죠. 이런 오류가 나면 현재 크롬 버전에 맞는 크롬 드라이버를 다운로드해 주셔야 합니다.



## 
# Selenium 임포트
from selenium import webdriver

# 크롬 드라이버 생성
driver = webdriver.Chrome()

# 사이트 접속하기
driver.get('https://codeit.kr')




# 크롬 드라이버 종료
driver.quit() 



## 자동으로 아이디 비밀번호 입력후 로그인하기
from selenium import webdriver


driver = webdriver.Chrome('/Users/codeit/web_automation/chromedriver')    # chromedriver 경로
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')

driver.find_element_by_css_selector('.top-nav__login-link').click()
driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()



## 크롬 개발자 도구로 CSS 선택자 가져오기

이번 레슨에서는 우리가 지금까지 사용하지 않았던 크롬 개발자 도구 기능을 하나 소개해 드릴게요.

이전 영상에서는 코스타그램 웹사이트에 가서 로그인 하는 동작을 자동화했었죠?

qiyd3igpl-Untitled(rev).png

로그인 링크를 검사해서 필요한 CSS 선택자를 작성했습니다.

크롬 개발자 도구를 활용하면 웹 요소(태그)의 CSS 선택자를 바로 복사해 올 수 있습니다. 웹 요소에 대한 CSS 선택자를 우리가 작성하지 않아도 되는 거죠.

로그인 링크에 대한 CSS 선택자를 가져와 보겠습니다.



필요한 웹 요소 위에 마우스를 올리고 우클릭을 한 다음 Copy → Copy selector를 클릭하시면 됩니다. 복사된 선택자를 붙여넣어 보면,


#app > nav > div > a
이렇게 나옵니다. 이걸 그대로 find_element_by_css_selector()의 파라미터로 쓰면 되는 거죠. 나머지 요소들(아이디 입력 창, 비밀번호 입력 창, 로그인 버튼)도 똑같이 선택자를 가져올 수 있겠죠?

선택자를 가져와서 find_element_by_css_selector() 안에 붙여넣어 주면 아래 코드가 완성됩니다.


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')

driver.find_element_by_css_selector('#app > nav > div > a').click()

driver.find_element_by_css_selector('#app > div > div > div > form > input.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('#app > div > div > div > form > input.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('#app > div > div > div > form > input.login-container__login-button').click()
코드를 실행하면, 자동 로그인이 잘 되는 것을 확인할 수 있습니다.

저번 챕터에서 이 기능을 소개해드리지 않은 이유는, 어차피 웹 스크래핑을 하려면 CSS 선택자를 잘 이해하고 다룰 줄 알아야 하기 때문입니다.

예를 들어, 우리는 나중에 코스타그램에서 모든 포스팅에 대한 정보를 가져올 건데요. 그러려면 먼저 모든 포스팅(썸네일)을 가져와야 합니다.

4addlqqos-Untitled3(rev).png

이게 첫 번째 썸네일을 검사한 모습인데요. 모든 썸네일을 가져오려면, 그냥 썸네일의 class 이름을 사용하면 됩니다.


driver.find_elements_by_css_selector('.post-list__post')
(post-list__post, post 둘 중 아무거나 사용해도 됩니다.)

그런데 개발자 도구를 이용해서 선택자를 복사하면 아래와 같은 선택자가 복사됩니다.


#app > main > div > div:nth-child(1)
꽤나 복잡하죠? 우리가 잘 모르는 nth-child(1) 같은 선택자도 들어가 있습니다. 위 선택자는 처음 썸네일에 대한 선택자입니다. 모든 썸네일을 가져오려면, 어차피 이 선택자를 수정해야 하는 거죠.

따라서 선택자를 복사하는 기능은 웹사이트에서 특정 요소를 찾아야 할 때, 예를 들어 버튼 하나를 클릭해야 한다거나, 입력 창에 값을 넣어줘야 한다거나, 이럴 때 주로 유용하게 쓰입니다.

주의사항: 개발자 도구로 선택자를 복사해서 코드에 붙여 넣으면 가끔 선택자에 해당하는 태그를 못 찾는 경우가 있습니다. 그 이유는 개발자 도구에서 보는 HTML 코드와 웹 페이지의 실제 HTML 코드는 다소 다를 수 있기 때문인데요. 선택자를 그대로 복사해서 넣었는데 태그를 못 찾는 오류가 난다면 직접 선택자를 작성해 주세요.



## 다양한 find_element 메소드

Selenium에는 CSS 선택자를 사용하는 find_element_by_css_selector() 메소드 외에도 웹 요소를 찾는 다양한 메소드들이 있습니다.

바로 소개해드릴게요.

# 태그 이름으로 찾기
driver.find_element_by_tag_name('tag_name')
태그 이름으로 요소를 찾습니다. 파라미터로 태그 이름을 그대로 써 주면 됩니다.

# id로 찾기
driver.find_element_by_id('id-name')
id 값으로 요소를 찾습니다. 파라미터로 id 값을 그대로 써 주면 됩니다. id 값이 'id-name'이라면, '#id-name'이 아닌 'id-name'을 그대로 넣어주시면 됩니다.

# class로 찾기
driver.find_element_by_class_name('class-name')
class 이름으로 요소를 찾습니다. 파라미터로 class 이름을 (. 없이) 그대로 써 주면 됩니다.

간단하게 태그 이름이나, id 값, 또는 class 이름으로만 요소를 찾을 수 있다면 방금 소개해드린 메소드들을 활용하는 게 더 편리할 수도 있겠죠?

# 복수 요소 찾기
find_element 메소드들은 매칭되는 웹 요소 하나를 리턴해 줍니다. HTML 문서에서 가장 먼저 나타나는 요소인데요. 매칭되는 모든 요소를 찾고 싶다면 어떻게 해야 할까요?

find_elements와 같이 element 뒤에 s를 붙여주면 됩니다. 그러니까 아래와 같이 쓸 수 있겠죠?

# CSS 선택자
driver.find_elements_by_css_selector('selector')

# 태그 이름
driver.find_elements_by_tag_name('tag_name')

# class 이름
driver.find_elements_by_class_name('class-name')
참고로 id 값을 사용하는 find_element_by_id() 메소드는 s가 붙는 버전이 없습니다. 주어진 id 값은 최대 하나의 요소만 가질 수 있기 때문이죠. id의 특성 기억하시죠?

Beautiful Soup처럼 find_element 메소드들은 웹 요소 자체를 리턴하고 find_elements 메소드들은 웹 요소 리스트를 리턴합니다.


## Selenium Wait

import time
from selenium import webdriver


driver = webdriver.Chrome('/Users/codeit/web_automation/chromedriver')    # chromedriver 경로
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)
driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()



## Selenium Explicit Wait

# Explicit Wait이란?
이번 레슨에서는 새로운 종류의 wait, explicit wait을 소개해드리겠습니다.

implicit = '암시적인', explicit = '명시적인', 이런 뜻을 가지고 있는데요.

저번 영상에서 봤던 implicit wait은 처음에 한 번만 설정해 주면 됐습니다. 한 번만 설정해 주면, 찾으려는 웹 요소가 없을 때마다 최대한 X초를 기다려 줍니다. wait이 필요할 때마다 설정해 주는 것이 아니기 때문에 암시적이라고 할 수 있습니다.

반면 explicit wait은 필요한 모든 요소에 대해 정확히 어떤 상태가 될 때까지 기다릴 것인지, 최대 몇 초 기다릴 것인지를 일일이 설정해 주는 방식입니다. 즉 명시적인 wait 방식인 거죠.

그럼 explicit wait을 어떻게 쓰는지 알아봅시다.

# Explicit Wait 써 보기
이전 영상에서는 로그인을 할 때 implicit wait과 time.sleep()을 썼었습니다.


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
이번에는 똑같은 작업을 해 보는데, explicit wait을 써 보겠습니다. 참고로 implicit wait과 explicit wait을 둘 다 사용하는 것은 권장되지 않습니다. time.sleep()은 explicit wait과 같이 쓰셔도 되지만 explicit wait을 잘 구현하시면 time.sleep()이 필요 없을 겁니다.

일단 임포트 문을 몇 개 추가해 줘야 합니다.


from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
먼저 로그인 링크를 클릭하는 코드를 구현해 볼 건데, 링크나 버튼 같은 요소는, 웹사이트에 존재하지만 클릭할 수 없는 상태일 수 있다고 했죠? 우리는 로그인 버튼이 로딩되기까지 기다리는데, 정확히 말해서 로그인 링크가 클릭 가능한 상태일 때까지 기다리고 싶습니다. explicit wait을 이용하면 이걸 구현할 수 있습니다.


login_link = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.top-nav__login-link')))
이렇게 구현합니다. 상당히 복잡하죠? 각 부분을 살펴봅시다.


WebDriverWait(driver, 3)
우선 WebDriverWait이라는 걸 만들어 주고, wait 기간을 설정해 줍니다. 지금은 3초로 돼있습니다. 무조건 3초를 기다린다는 것이 아니라, 최대 3초를 기다린 다는 것입니다.


WebDriverWait(driver, 3).until()
그리고 .until() 메소드를 호출해 줍니다. 특정 조건이 충족될 때까지 기다리겠다는 뜻입니다.


WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.top-nav__login-link')))
마지막으로 안에 조건을 넣어주는데, 이건 CSS 선택자로 .top-nav__login-link에 해당하는 요소를 찾는데, 그 요소가 clickable, 즉 클릭 가능해야 한다는 뜻입니다.

사실 이 코드를 복사해서 조건과 선택자 부분만 바꿔가면서 쓰면 되니까 코드를 깊게 이해하실 필요는 없습니다.

이제 login_link를 가져왔으니, 클릭해 줍시다.


login_link = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.top-nav__login-link')))
login_link.click()
그리고 나머지 코드도 완성해 줄게요.


login_link = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.top-nav__login-link')))
login_link.click()

id_box = WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.login-container__login-input')))
id_box.send_keys('codeit')

pw_box = WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.login-container__password-input')))
pw_box.send_keys('datascience')

login_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.login-container__login-button')))
login_button.click()
동작 4개 다 비슷한데, 아이디와 비밀번호 칸은 element_to_be_clickable() 대신 visibility_of_element_located() 메소드를 사용해 줬습니다. 웹 요소가 실제로 보일 때까지 기다리라는 뜻입니다. 웹 요소가 보인다는 것은, 단순히 존재하는 상태를 넘어선 겁니다. 웹 요소가 보이면 주로 상호 작용이 가능합니다.

각 요소마다 wait 기간을 바꿔줘도 되는데, 똑같은 기간을 사용할 것이라면 WebDriverWait을 한 번만 만들어 줘도 됩니다.


wait = WebDriverWait(driver, 3)

login_link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.top-nav__login-link')))
login_link.click()

id_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.login-container__login-input')))
id_box.send_keys('codeit')

pw_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.login-container__password-input')))
pw_box.send_keys('datascience')

login_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.login-container__login-button')))
login_button.click()
전체 코드는 아래와 같습니다.


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Chrome()

driver.get('https://workey.codeit.kr/costagram/index')

wait = WebDriverWait(driver, 3)

login_link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.top-nav__login-link')))
login_link.click()

id_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.login-container__login-input')))
id_box.send_keys('codeit')

pw_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.login-container__password-input')))
pw_box.send_keys('datascience')

login_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.login-container__login-button')))
login_button.click()

driver.quit()

# 유용한 wait 조건들
element_to_be_clickable() 웹 요소가 클릭 가능한 상태일 때까지 기다림.
visibility_of_element_located() 웹 요소가 실제로 보일 때까지 기다림.
text_to_be_present_in_element() 웹 요소 안에 텍스트가 로딩될 때까지 기다림.
invisibility_of_element_located() 웹 요소가 안 보일 때까지 기다림.

여기서 모든 wait 조건을 확인하실 수 있습니다
explicit wait은 코드가 훨씬 복잡한 대신 더 정확한 wait 조건을 사용할 수 있습니다. 이걸 활용하면 무조건 정해진 기간을 기다리는 time.sleep()을 쓰는 것보다 더 효율적인 코드를 짤 수 있겠죠? 웹 요소에 알맞은 조건을 잘 사용하면 로딩 문제 때문에 코드가 실패할 확률도 낮아집니다. 많은 동작이 실패 없이 연속적으로 이루어저야 할 때는 explicit wait을 활용해 보세요!


### Selenium 액션 체인

## 액션 체인이란?
Selenium에서 액션 체인(Action Chain)은 사용자의 마우스, 키보드 동작(action)을 사슬(chain)처럼 이어서 실행하는 것을 뜻합니다.

액션 체인을 사용하면:

우리가 배운 클릭, 키보드 입력뿐만이 아니라 더블 클릭, 드래그 등 사용자의 모든 마우스, 키보드 동작을 자동화할 수 있습니다. 클릭, 키보드 입력 외의 동작은 액션 체인이 꼭 필요합니다.
많은 동작을 한 번에 묶어서 실행할 수 있습니다. 사용자 동작 시나리오가 복잡해질 때 유용합니다.
전에 말씀드렸듯이 많은 작업은 클릭, 키보드 입력으로 해결되기 때문에 나중에 더 심화된 액션이 필요할 때 이 노트를 읽어보셔도 좋습니다.

## 액션 체인 사용하기
액션 체인을 사용하는 방법에는 두 가지가 있습니다.

한 줄의 코드에서, 꼬리에 꼬리를 무는 방식으로 연결하는 방식.
하나의 액션을 한 줄로 만들어서, 나열하는 방식입니다.
두 방법 모두, 하나의 동작이 완료되면 그다음 동작이 순서대로 실행됩니다.

1. 연결하는 방식


from selenium.webdriver.common.action_chains import ActionChains

# 사용자 동작에 필요한 웹 요소들 찾기

(ActionChains(driver)
    # 사용자 액션 정의 
    .perform())
2. 나열하는 방식


from selenium.webdriver.common.action_chains import ActionChains

# 사용자 동작에 필요한 웹 요소들 찾기

actions = ActionChains(driver)

# 사용자 액션 정의

actions.perform()
# 사용자 액션 정의 부분에는 액션을 수행하는 메소드들을 쭉 써 주면 됩니다. 액션 메소드들은 마지막 섹션에 정리돼 있습니다.

예시로 전에 봤던 코스타그램 로그인 동작을 다시 액션 체인을 통해 구현해 볼게요.

1. 연결하는 방식


import time
from selenium import webdriver
from selenium.webdriver import ActionChains

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')

# 로그인 링크 클릭
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

# 아이디 박스, 비밀번호 박스, 로그인 버튼 찾아놓기
id_box = driver.find_element_by_css_selector('.login-container__login-input')
pw_box = driver.find_element_by_css_selector('.login-container__password-input')
login_button = driver.find_element_by_css_selector('.login-container__login-button')

# 액션 실행
(ActionChains(driver)
    .send_keys_to_element(id_box, 'codeit')
    .send_keys_to_element(pw_box, 'datascience')
    .click(login_button)
    .perform())

driver.quit()
액션 실행 부분에서는 ActionChains를 만들어 주고, 필요한 동작을 모두 정의해 주고, 끝에 .perform() 메소드를 호출해 줍니다. 코드를 다 한 줄에 써도 되지만, 그러면 한 줄이 너무 길어지기 때문에 괄호 ()로 코드를 감싸줬습니다. 괄호 안에 있는 코드는 코드 한 줄로 인식됩니다.

액션 체인을 사용할 때는 액션 체인에 필요한 웹 요소들을 먼저 다 찾아놔야 합니다. 그런데 로그인 과정을 생각해 보면, 로그인 링크를 클릭해야 로그인 창이 뜨죠? 로그인 링크를 클릭한 다음에만 아이디 입력 박스 같은 요소들을 찾을 수 있습니다. 로그인 링크를 클릭하기 전에는 필요한 요소들을 다 찾아놓을 수 없는 거죠. 따라서 처음에 로그인 링크를 클릭하는 것은 체인으로 연결할 수 없고, 나머지 아이디 입력, 비밀번호 입력, 로그인 버튼 클릭만 연결할 수 있습니다.

아래 코드는 오류가 납니다. 로그인 링크를 클릭하기 전에 아이디 입력 박스를 찾으려고 하기 때문이죠.


import time
from selenium import webdriver
from selenium.webdriver import ActionChains

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')

# 로그인 링크, 아이디 박스, 비밀번호 박스, 로그인 버튼 찾아놓기
login_link = driver.find_element_by_css_selector('.top-nav__login-link')
id_box = driver.find_element_by_css_selector('.login-container__login-input')
pw_box = driver.find_element_by_css_selector('.login-container__password-input')
login_button = driver.find_element_by_css_selector('.login-container__login-button')

# 액션 실행
(ActionChains(driver)
    .click(login_link)
    .send_keys_to_element(id_box, 'codeit')
    .send_keys_to_element(pw_box, 'datascience')
    .click(login_button)
    .perform())

driver.quit()
2. 나열하는 방식


import time
from selenium import webdriver
from selenium.webdriver import ActionChains

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')

# 로그인 링크 클릭
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

# 아이디 박스, 비밀번호 박스, 로그인 버튼 찾아놓기
id_box = driver.find_element_by_css_selector('.login-container__login-input')
pw_box = driver.find_element_by_css_selector('.login-container__password-input')
login_button = driver.find_element_by_css_selector('.login-container__login-button')

# 액션 실행
actions = ActionChains(driver)
actions.send_keys_to_element(id_box, 'codeit')
actions.send_keys_to_element(pw_box, 'datascience')
actions.click(login_button)
actions.perform()

driver.quit()
나열하는 방식은 한 줄에 하나의 액션을 써 줍니다.

연결하는 방식과 나열하는 방식 중 편하신 방법을 사용하시면 됩니다.

## 액션 체인 기능
액션 체인으로 구현할 수 있는 기능을 정리해드릴게요.

함수에 parameter=None은 파라미터가 선택적이라는 뜻입니다. 나머지 파라미터는 필수입니다.

# 클릭
.click(element=None)
웹 요소 element를 파라미터로 전달해 주면 element를 클릭하고 그렇지 않으면 현재 마우스가 위치해 있는 곳을 클릭합니다.

# 더블 클릭
.double_click(element=None)
웹 요소 element를 파라미터로 전달해 주면 element를 더블 클릭하고 그렇지 않으면 현재 마우스가 위치해 있는 곳을 더블 클릭합니다.

# 우클릭
.context_click(element=None)
웹 요소 element를 파라미터로 전달해 주면 element를 우클릭하고 그렇지 않으면 현재 마우스가 위치해 있는 곳을 우클릭합니다.

# 드래그 앤 드롭
.drag_and_drop(source, target)
source 웹 요소를 클릭해서 target 웹 요소까지 드래그한 다음, 드롭해 줍니다.

# 마우스 이동
.move_to_element(element)
element 웹 요소까지 마우스를 움직입니다.

# 키보드 입력
.send_keys(keys)
.send_keys_to_element(element, keys)
send_keys()는 현재 선택된 웹 요소에 키보드 입력을 보내고, send_keys_to_element()는 element 요소에 키보드 입력을 보냅니다.

# 동작 중지
.pause(seconds)
seconds초 동안 동작을 멈춥니다. 액션 체인에는 time.sleep() 대신 .pause()를 써 주세요.

나머지 사용자 동작은 여기서 확인해 보세요.


### 웹사이트 제어하기 정리 노트

## Selenium 라이브러리
Selenium은 웹 브라우저를 자동화하는 도구입니다.

Selenium을 활용하면 클릭이나 스크롤, 키보드 입력 같은 사용자의 동작을 제어할 수 있습니다. 코드로 작성된 내가 원하는 동작들이 브라우저에서 자동으로 실행되는 거죠.

Selenium을 활용하면 로그인, 검색, 버튼 클릭 같은 작업을 자동화할 수 있기 때문에, 데이터 수집 등 웹을 다방면으로 응용하는데 쓸 수 있습니다.

Selenium 문서 1, 문서 2 참고하세요!

## Selenium 임포트
selenium 라이브러리에서 webdriver를 임포트해 줍니다.


from selenium import webdriver

## 웹 드라이버 생성
웹 드라이버는 웹 브라우저를 운전(drive) 하는 도구입니다. 웹 드라이버가 사용자 동작을 실행해 줍니다.

먼저 웹 드라이버를 생성해 줍니다. 크롬 브라우저를 사용하니까, 크롬 드라이버를 만들어 줍니다.


driver = webdriver.Chrome('chromedriver 경로')
그리고 get() 메소드를 활용하면 원하는 웹사이트에 접속할 수 있습니다.


driver.get('https://www.example.com')

## 웹 요소 찾기
먼저 제어하려는 웹 요소를 찾아야 합니다. 웹 요소를 찾기 위해선 CSS 선택자를 활용하는 find_element_by_css_selector() 메소드를 쓸 수 있습니다.


driver.find_element_by_css_selector(selector)
선택자 selector에 매칭되는 웹 요소가 선택됩니다.

## 웹 요소 제어하기
웹 요소에 가장 많이 쓰이는 동작은 클릭과 키보드 입력입니다. click() 과 send_keys() 메소드를 활용하면 됩니다.


driver.find_element_by_css_selector(selector).click()
driver.find_element_by_css_selector(selector).send_keys('input')
필요한 웹 요소를 찾고, 그걸 클릭하거나 키보드 인풋을 전달해 줍니다.

## Wait 설정하기
웹은 로딩되는 데 시간이 걸립니다. 웹 요소가 로딩되지 않았는데 그걸 찾으려고 하거나, 클릭하려고 하면 오류가 납니다. Selenium을 사용할 때는 웹 요소의 로딩을 기다려 주는 것이 중요합니다.

두 가지 wait 방식을 영상으로 알아봤습니다.

# implicit wait
웹 드라이버에 implicit wait을 설정해 주면 찾으려고 하는 웹 요소가 없을 때, 최대 설정해 준 기간만큼 기다려 줍니다.


driver = webdriver.Chrome('chromedriver 경로')
driver.implicitly_wait(3)
드라이버를 생성하고, 한 번만 설정해 주면 됩니다. 위 코드는 최대 3초를 기다려 준다는 뜻입니다. 3초가 지나도 웹 요소를 찾지 못하면 오류가 납니다.

하지만 implicit wait은 웹 요소가 존재하는지 만을 확인합니다. 웹 요소가 존재하지만 아직 클릭은 못하는 상태일 수도 있습니다.

따라서 추가적인 wait 방식이 필요합니다.

# time.sleep
time.sleep은 정해진 기간만큼 동작을 멈춥니다. 항상 정해진 기간만큼 기다려 주는 거죠.

먼저 time 모듈을 임포트해 주고


import time
필요한 곳에 time.sleep을 추가해 줍니다.


time.sleep(1)
위 코드는 1초 동안 기다린다는 뜻이겠죠?

time.sleep은 보통 웹사이트 코드가 로딩되거나 바뀌고 난 직후에 추가해 주는 것이 좋습니다. 예를 들어 웹사이트에 처음 접속하거나, 어떤 버튼을 클릭해서 새로운 창이 뜬다면 (새로운 창에 대한 HTML 코드가 어딘가 로딩되겠죠?), 직후에 time.sleep을 걸어 주는 거죠.




### Selenium으로 웹 스크래핑하기

import time
from selenium import webdriver

dirver = webdriver.Chrome('/Users/codeit/web_automation/chromedriver')
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/music')
time.sleep(1)

popular_artists =[]

for artist in driver.find_elements_by_css_selector('ul.popular__order li'):
    popular_artists.append(artist.text.strip())

print(popular_artists)



### 꿈의 직장 전화번호 모으기 IV

import requests
from bs4 import BeautifulSoup

# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')

branch_infos = []

# 모든 지점에 대한 태그 가져오기
branch_tags = soup.select('div.branch')

for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    branch_infos.append([branch_name, address, phone_number])

# 테스트 코드
print(branch_infos)



## 해설
정리된 Selenium 메소드들입니다.



이전 과제의 코드를 한 줄씩 Selenium 코드로 바꿔 볼게요.

먼저 imoprt문을 바꿔줘야 합니다. requests와 BeautifulSoup 대신 selenium에서 webdriver를 가져옵니다.


import requests
from bs4 import BeautifulSoup

from selenium import webdriver
이전에는 requests로 요청을 보내고, 응답으로 돌아온 HTML 코드를 BeautifulSoup을 통해 정리했다면, Selenium에서는 웹 드라이버를 생성하고, 웹 드라이버로 요청을 보내면 됩니다.


# HTML 코드 받아오기
response = requests.get("https://workey.codeit.kr/orangebottle/index")

# BeautifulSoup 사용해서 HTML 코드 정리
soup = BeautifulSoup(response.text, 'html.parser')

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/orangebottle/index')
이제 모든 지점에 대한 태그를 가져오면 되는데, soup.select()는 driver.find_elements_by_css_selector()로 대체할 수 있습니다.

그리고 selenium에서는 보통 '태그' 대신 '웹 요소'라는 용어를 사용하기 때문에, 코드의 동작에는 상관이 없지만, 변수 이름을 약간 바꿔 줬습니다.


# 모든 지점에 대한 태그 가져오기
branch_tags = soup.select('div.branch')

# 모든 지점에 대한 웹 요소 가져오기
branches = driver.find_elements_by_css_selector('div.branch')
이제 각 지점에 대한 정보를 가져오면 되는데, 아래와 같이 코드를 바꿔주면 됩니다.


for branch_tag in branch_tags:
    # 각 태그에서 지점 이름, 전화번호 가져오기
    branch_name = branch_tag.select_one('p.city').get_text()
    address = branch_tag.select_one('p.address').get_text()
    phone_number = branch_tag.select_one('span.phoneNum').get_text()
    branch_infos.append([branch_name, address, phone_number])

for branch in branches:
    # 각 branch 요소에서 지점 이름, 전화번호 가져오기
    branch_name = branch.find_element_by_css_selector('p.city').text
    address = branch.find_element_by_css_selector('p.address').text
    phone_number = branch.find_element_by_css_selector('span.phoneNum').text
    branch_infos.append([branch_name, address, phone_number])
.select_one() → .find_element_by_css_selector()
.get_text() → .text

그리고 마지막으로, selenium은 웹 드라이버를 종료해 줘야 한다는 것 잊지 마세요!


driver.quit()

## 모범 답안

from selenium import webdriver

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/orangebottle/index')

branch_infos = []

# 모든 지점에 대한 웹 요소 가져오기
branches = driver.find_elements_by_css_selector('div.branch')

for branch in branches:
    # 각 branch 요소에서 지점 이름, 전화번호 가져오기
    branch_name = branch.find_element_by_css_selector('p.city').text
    address = branch.find_element_by_css_selector('p.address').text
    phone_number = branch.find_element_by_css_selector('span.phoneNum').text
    branch_infos.append([branch_name, address, phone_number])

driver.quit()


### Selenium으로 JavaScript 사용하기

import time
from selenium import webdriver

# 웹 드라이버 설정

# 현재 scrollHeight 가져오기
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    # scrollHeight까지 스크롤
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    # 새로운 내용 로딩될때까지 기다림
    time.sleep(0.5)

    # 새로운 내용 로딩됐는지 확인
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height



### 플레이리스트 정보 스크래핑

#

import time
from selenium import webdriver
from openpyxl import Workbook


wb = Workbook(write_only=True)
ws = wb.create_sheet('플레이 리스트')
ws.append(['제목', '해시태그', '좋아요 수', '곡 수'])

dirver = webdriver.Chrome('/Users/codeit/web_automation/chromedriver')
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/music')
time.sleep(3)

# 현재 scrollHeight 가져오기
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    # scrollHeight까지 스크롤
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    # 새로운 내용 로딩될때까지 기다림
    time.sleep(0.5)

    # 새로운 내용 로딩됐는지 확인
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

playlists = driver.find_elements_by_css_selector('div.playlist__meta')

for playlist in playlists:
    title = playlist.find_element_by_css_selector('h3.title').text
    hashtags = playlist.find_element_by_css_selector('p.tags').text
    like_cunt = playlist.find_element_by_css_selector('span.data__like-count').text
    music_count = playlist.find_element_by_css_selector('span.data__music-count').text

    ws.apend([title, hashtags, like_cunt,music_count])

driver.quit()
wb.save('플레이리스트_정보.xlsx')



### Selenium과 Beautiful Soup

웹 스크래핑을 할 때는 로그인, 스크롤 같은 동작이 필요한 경우가 많은데요. 이런 동작은 Beautiful Soup으로 할 수 없기 때문에 Selenium을 활용했습니다.

웹에서 필요한 동작 (로그인, 스크롤 등)과 웹에서 데이터를 추출해 내는 과정을 모두 Selenium으로 해결했는데, 사실 Selenium과 Beautiful Soup을 같이 써도 됩니다. Selenium으로 필요한 동작을 실행하고, Beautiful Soup으로 데이터를 추출해 내는 거죠.

먼저 Selenium으로 웹사이트에 접속해서, 필요한 동작을 다 실행하면 동작에 따라 웹사이트의 HTML 코드가 바뀔 겁니다. 그럼 그 HTML 코드를 가져와서 BeautifulSoup을 만들고 Beautiful Soup으로 필요한 데이터를 추출해 내는 거죠. (앞으로 HTML 코드에서 필요한 정보를 추출해 내는 것을 파싱(parsing)이라고 할게요.)

이렇게 하면 좋은 점이 뭘까요?

일단은 Beautiful Soup 문법을 쓸 수 있다는 겁니다. Beautiful Soup은 HTML 파싱에 최적화된 라이브러리기 때문에 태그를 선택하는 다양한 문법이 존재하고 (.find(), .select_one(), .tagname 등) 문법이 Selenium 보다 더 간결합니다. Beautiful Soup 문법이 익숙하시다면 이 방법으로 웹 스크래핑을 하는 것도 좋겠죠?

그리고 파싱 작업을 할 때는 Beatufiul Soup이 더 효율적입니다. 즉, Selenium보다 더 빠르다는 거죠. 물론 간단한 작업을 할 때는 큰 차이가 없겠지만, HTML 코드가 엄청나게 복잡하고 코드에서 많은 정보를 추출해 내야 한다면 속도 차이가 있을 겁니다.

하지만 Beautiful Soup과 Selenium을 둘 다 활용하는 건 번거로울 수 있고, 꼭 Selenium으로 웹 스크래핑 작업을 다 해야 하는 경우도 있습니다. Selenium으로 필요한 모든 동작을 수행한 다음 한꺼번에 파싱을 하는 것이 아니라, 스크래핑 중간중간에 계속 Selenium 동작이 필요한 경우죠. (마지막 과제로 해 보실 코스타그램 스크래핑도 이렇습니다).

상황과 취향에 따라 Selenium만 쓰거나, Selenium + Beautiful Soup을 같이 쓰시면 됩니다.

그럼 이제 Selenium과 Beautiful Soup을 둘 다 사용하는 걸 직접 보여드릴게요. 일단 아래는 저번 영상에서 봤던 플레이리스트 스크래핑 코드입니다.


import time
from selenium import webdriver
from openpyxl import Workbook

wb = Workbook(write_only=True)
ws = wb.create_sheet('플레이리스트')
ws.append(['제목', '태그', '좋아요 수', '노래 수'])

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/music')
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

### 스크롤 완료 ### 

playlists = driver.find_elements_by_css_selector('.playlist__meta')

for playlist in playlists:
    title = playlist.find_element_by_css_selector('h3.title').text
    hashtags = playlist.find_element_by_css_selector('p.tags').text
    like_count = playlist.find_element_by_css_selector('span.data__like-count').text
    music_count = playlist.find_element_by_css_selector('span.data__music-count').text
    ws.append([title, hashtags, like_count, music_count])

driver.quit()

wb.save('플레이리스트.xlsx')
중간에 '스크롤 완료' 부분에 도달하면, 스크롤이 다 끝났을 겁니다. 그러니까 필요한 HTML 코드가 다 로딩된 상태인 거죠. 여기서 웹 페이지의 HTML 코드를 가져와서, BeautifulSoup을 만들면 됩니다.

현재 웹 페이지의 HTML 코드는 driver.page_source로 가져올 수 있습니다.


music_page = driver.page_source
driver.quit()

soup = BeautifulSoup(music_page, 'html.parser')
driver를 통해 웹 페이지의 코드를 가져왔으면, 더 이상 driver를 쓸 일이 없기 때문에 driver.quit()을 위로 옮겨줬습니다.

이렇게 soup을 만들어 줬으면, Beautiful Soup으로 정보를 추출하면 됩니다. Beautiful Soup 문법을 사용해서 아래와 같이 바꿔줄 수 있겠죠?


soup = BeautifulSoup(music_page, 'html.parser')

playlists = soup.select('.playlist__meta')

for playlist in playlists:
    title = playlist.select_one('h3.title').get_text()
    hashtags = playlist.select_one('p.tags').get_text()
    like_count = playlist.select_one('span.data__like-count').get_text()
    music_count = playlist.select_one('span.data__music-count').get_text()
    ws.append([title, hashtags, like_count, music_count])
코드의 로직은 똑같은데, Selenium 대신 Beautiful Soup을 활용한 겁니다. 전체 코드는 아래와 같습니다.


import time
from selenium import webdriver
from openpyxl import Workbook
from bs4 import BeautifulSoup

wb = Workbook(write_only=True)
ws = wb.create_sheet('플레이리스트')
ws.append(['제목', '태그', '좋아요 수', '노래 수'])

driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/music')
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

### 스크롤 완료 ### 
music_page = driver.page_source
driver.quit()

soup = BeautifulSoup(music_page, 'html.parser')

playlists = soup.select('.playlist__meta')

for playlist in playlists:
    title = playlist.select_one('h3.title').get_text()
    hashtags = playlist.select_one('p.tags').get_text()
    like_count = playlist.select_one('span.data__like-count').get_text()
    music_count = playlist.select_one('span.data__music-count').get_text()
    ws.append([title, hashtags, like_count, music_count])

wb.save('플레이리스트.xlsx')
그러니까 과정을 정리해 보면 아래와 같습니다:

Selenium으로 웹사이트 접속 후 필요한 동작을 수행하기
driver.page_source로 현재 웹사이트 HTML 코드 가져오기
가져온 HTML 코드로 BeautifulSoup 만들기
Beautiful Soup으로 데이터 추출하기


#### Selenium과 웹 스크래핑 정리 노트

# Selenium VS Beautiful Soup
Beautiful Soup으로 하는 웹 스크래핑을 대부분 Selenium으로도 할 수 있습니다. 아래는 Beautiful Soup의 메소드들과 매칭되는 Selenium 메소드들을 정리해 놓은 표입니다. 필요할 때 참고하세요.

(참고로 Selenium과 BeautifulSoup을 같이 사용할 수도 있습니다. Selenium과 Beautiful Soup 노트를 참고하세요!)

# Beautiful Soup / Selenium
soup.select_one() / driver.find_element_by_css_selector()
soup.select() / driver.find_elements_by_css_selector()
Tag / webElement
tag.select() / web_element.find_elements_by_css_selector()
tag.get_text() / web_element.text
tag.strings / -
tag.stripped_strings / -
ta['attr'] / web_element.get_attribute('attr')


우리가 생각하는 HTML 태그를 Beautiful Soup에서는 그냥 태그(tag)라고 불렀지만, Selenium에서는 보통 웹 요소(web element)라고 부릅니다.

Beautiful Soup을 사용할 때는 단순히 HTML 태그의 내용만 봤지만, Selenium을 사용할 때는 HTML 태그에 해당하는 웹 요소와 상호 작용이 있기 때문이죠. (태그를 클릭한다거나 태그에 무언가를 입력한다고 하면 이상하겠죠?)

웹 요소 가져오기

driver.find_element_by_css_selector(selector) # HTML 문서에서 매칭되는 처음 요소
driver.find_elements_by_css_selector(selector) # HTML 문서에서 매칭되는 모든 요소
위와 같이 HTML 코드 전체에서 웹 요소를 찾을 수도 있고


web_element.find_element_by_css_selector(selector) # web_element 안에서 매칭되는 처음 요소
web_element.find_elements_by_css_selector(selector) # web_element 안에서 매칭되는 모든 요소
웹 요소 안에서 다른 웹 요소를 찾을 수도 있습니다.

find_element_by_css_selector()는 항상 웹 요소를 리턴하고 find_elements_by_css_selector()는 웹 요소가 있는 리스트를 리턴합니다.

웹 요소에서 데이터 추출하기

web_element.text
웹 요소의 텍스트를 가져오려면 .text를 활용합니다. Beautiful Soup의 .get_text()처럼 웹 요소 안의 모든 텍스트를 하나로 합쳐서 리턴해 줍니다.


web_element.get_attribute('attr')
웹 요소의 속성을 가져오려면 .get_attribute()을 씁니다. 파라미터로 속성 이름을 넣어주면 됩니다.

Selenium과 JavaScript
자바스크립트는 웹 페이지에 다양한 기능을 넣는데 사용되는 프로그래밍 언어입니다.

웹사이트에서 무언가를 클릭하면 웹사이트가 바뀐다든가, 스크롤을 하면 새로운 내용이 로딩된다든가, 이런 게 모두 자바스크립트의 역할입니다.

Selenium으로 자바스크립트 코드를 실행할 수 있습니다.


driver.execute_script("자바스크립트 코드")
driver.execute_script() 메소드를 활용하면 됩니다. 파라미터로 실행하고 싶은 자바스크립트 코드를 넣어 줍니다.

자바스크립트로는 많은 걸 할 수 있지만, 웹 스크래핑을 할 때 가장 많이 쓰이는 동작은 스크롤입니다. 스크롤을 해야 추가적인 데이터가 나오는 경우가 많기 때문이죠.


driver.execute_script("window.scrollTo(0, 200);")
위 코드는 (0, 200) 위치까지 스크롤을 하라는 뜻입니다.

이런 자바스크립트 코드를 이용해서, 더 이상 데이터가 없을 때까지, 웹 페이지 끝까지 스크롤을 하는 Selenium 코드를 짤 수 있습니다.


# 현재 scrollHeight 가져오기
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    # scrollHeight까지 스크롤                               
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    # 새로운 내용 로딩될때까지 기다림
    time.sleep(0.5)
    
    # 새로운 내용이 로딩됐는지 확인
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height
웹 페이지 끝까지 스크롤을 해야 하는 경우 이 코드를 쓰시면 됩니다.


### 코스타그램 스크래핑 I

# 해설
1. 필요 라이브러리들 import

import time
from selenium import webdriver
다음은 웹 드라이버를 생성하고 implicit_wait을 설정해 줄게요.


# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)
그리고 코스타그램 (https://workey.codeit.kr/costagram/index)에 접속한 다음, 웹사이트가 로딩되도록 1초 기다려 줍니다.


driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)
2. 코스타그램 로그인
이제 로그인을 하면 되는데요, 로그인 코드는 이전에 많이 보셨죠? 그대로 쓰시면 됩니다.


# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
여기에 적절한 wait을 추가해 줄게요.


# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
time.sleep(1)
웹사이트 코드가 바뀌는 부분마다, 뒤에 sleep을 1초간 넣어줬습니다.

3. 웹 페이지 끝까지 스크롤
스크롤도 이 전에 보신 적 있죠? 음악 웹사이트를 스크롤할 때 사용했던 코드를 똑같이 쓰면 됩니다.


# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height
sleep이 이미 들어가 있으니, 따로 추가해 주지 않겠습니다.

4. 각 썸네일(포스팅)을 클릭하고, 창이 뜨면 닫기 버튼 누르기
각 썸네일을 클릭하려면, 썸네일에 해당하는 웹 요소를 모두 가지고 와야겠죠?



하이라이트된 부분이 첫 번째 썸네일에 해당하는 부분인데요. div 태그에, class 이름은 post-list__post, 그리고 post로 돼있죠? class 속성에 여러 값을 전달해 줄 때는, "post-list__post post"처럼 띄어쓰기를 활용한다고 했습니다.

사실 post-list__post, post 둘 다 썸네일에만 사용되기 때문에 둘 중 아무거나 선택자로 써도 됩니다. 둘을 AND 연산으로 조합해도 되고요. 여기서는 그냥 post-list__post를 쓸게요.


# 모든 썸네일 요소 가져오기
posts = driver.find_elements_by_css_selector('.post-list__post')
자 이제, 각 썸네일을 클릭하고, 포스트 상세 창이 뜨면, 닫아주면 됩니다. 반복적인 작업을 실행하기 위해서 for 문을 써 주면 되겠죠?


for post in posts:
    # 썸네일 클릭

    # 닫기 버튼 클릭
포스트를 클릭하는 건 아주 간단한데요,  post는 썸네일에 해당하는 웹 요소니까 post를 클릭해 주면 됩니다.


for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 닫기 버튼 클릭
포스트 상세 창이 뜨도록 0.5초 정도 기다려 주는 코드도 추가해 줬습니다.

그럼 이제 닫기 버튼을 찾아야 하는데요.



close-btn이라는 클래스 이름은 닫기 버튼에만 쓰이기 때문에 이걸 그대로 선택자로 사용하시면 됩니다.


for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn')
참고로 여기서 post.find_element_by_css_selector('.close-btn')을 하시면 안됩니다. 이전에 설명드렸듯이, 포스팅 상세 창에 대한 코드는 post 안에 생기는 게 아니라, 별도로 생기기 때문이죠.

그럼 닫기 버튼을 클릭하고, 창이 없어지도록 또 0.5초를 기다려 줄게요.


for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)
5. 웹 드라이버 종료
웹 드라이버를 종료하려면 .quit() 메소드를 쓰면 됩니다.


driver.quit()
이제 코드가 완성됐는데요. 코드를 실행해 보시면 웹 브라우저가 자동으로 실행되고, 로그인, 스크롤, 모든 포스트를 열고 닫는 작업이 자동화된 걸 확인하실 수 있을 겁니다.

모범 답안

import time
from selenium import webdriver

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# 모든 썸네일 요소 가져오기
posts = driver.find_elements_by_css_selector('.post-list__post')

for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)

driver.quit()


### 코스타그램 스크래핑 II

# 해설
vd2sog4hi-prac10-1.png

먼저 이미지에 해당하는 <div> 태그를 가져옵시다. 이미지를 담고 있는 <div> 태그의 class 이름은 post-container__image죠? 이걸 선택자로 쓰겠습니다.


for post in posts:
    ...

    # 이미지 주소 가져오기
    driver.find_element_by_css_selector('.post-container__image')

    ...
그리고 이 <div> 태그의 style 속성을 가져오면 되는데요, 속성을 가져오려면 .get_attribute() 메소드를 쓰면 됩니다.


for post in posts:
    ...

    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')

    ...
style_attr의 값은 아래와 같겠죠?


background-image: url("/images/costagram/posts/post0.jpg"); background-position: center center; background-size: cover;
여기서 "" 안에 있는 /images/costagram/posts/post0.jpg 부분을 빼 내야 합니다. 어떻게 하면 좋을까요?

이럴 때는 문자열을 " 기준으로 나눠주면 됩니다. 특정 문자를 기준으로 문자열을 나눌 때는 .split() 메소드를 활용하면 되는데요, style_attr에 .split('"')을 하면 아래와 같은 결과가 나옵니다.


print(style_attr.split('"'))

['background-image: url(', '/images/costagram/posts/post0.jpg', '); background-position: center center; background-size: cover;']
" 기준으로 문자열이 세 부분으로 나눠졌죠? 이미지 주소는 두 번째 문자열이니까, 그걸 그대로 가져오면 됩니다.


for post in posts:
    ...

    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]

    ...
마지막으로 이미지 주소를 출력해 주면 코드가 완성되겠죠?


for post in posts:
    ...

    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    print(image_url)

    ...
모범 답안

import time
from selenium import webdriver

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# 모든 썸네일 요소 가져오기
posts = driver.find_elements_by_css_selector('.post-list__post')

for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    print(image_url)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)

driver.quit()



### 웹에서 이미지 가져오기

이번 토픽에서는 이미지 주소를 몇 번 가져와 봤는데요, 사실 우리가 원하는 건 이미지 주소가 아니라 실제 이미지겠죠? 이번 레슨에서는 웹에서 이미지를 가져오는 방법에 대해 알아보겠습니다.

먼저 이미지 주소에 대해 알아봅시다.

이미지 주소란?
이미지 주소 (image URL)는 이미지가 저장돼 있는 위치를 뜻합니다. 이미지도 결국 어떤 URL에 저장돼 있습니다.

예를 들어 이 주소로 가시면 codeit 이미지가 보일 겁니다: https://www.codeit.kr/static/images/brand/logo_white.png

그런데 우리가 지금까지 가져와 봤던 이미지 주소는 조금 다르게 생겼죠? 예를 들어 이전 과제에서 가져온 이미지 주소는 이렇게 생겼었습니다.


/images/costagram/posts/post0.jpg
이 "주소"는 이미지 URL 전체가 아니라 URL의 경로 부분입니다.

우리가 처음 URL에 대해 배울 때, URL은 도메인 이름 부분이 있고, 경로 부분이 있다고 했죠?


https://www.codeit.kr/ - 도메인 이름
static/images/brand/logo_white.png - 경로
https:// 다음 첫 / 부분 까지를 도메인 이름이라고 하고 나머지를 경로라고 합니다.

HTML 코드에 있는 이미지 주소는 경로 부분만 포함돼 있는 경우가 많습니다. 이미지를 가져오기 위해서는 URL 전체가 필요하기 때문에, 경로 부분에 웹사이트의 도메인 이름을 더해줘서 URL을 만들어 줘야 합니다.

'https://workey.codeit.kr/costagram/index'의 경우 도메인 이름이 'https://workey.codeit.kr/'이겠죠? (참고로 'https://workey.codeit.kr/costagram', 'https://workey.codeit.kr/costagram/index' 둘 다 코스타그램 홈페이지로 연결됩니다.) 'https://workey.codeit.kr/'에 저번 과제에서 가져온 이미지 주소를 더해주면 됩니다.


https://workey.codeit.kr/images/costagram/posts/post0.jpg
...
https://workey.codeit.kr/images/costagram/posts/post35.jpg
이렇게 되겠죠? 위 주소 중 하나로 들어가 보시면 코스타그램에 있는 이미지가 있을 겁니다.

이건 저번 과제의 해답 코드인데요.


import time
from selenium import webdriver

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# 모든 썸네일 요소 가져오기
posts = driver.find_elements_by_css_selector('.post-list__post')

for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    print(image_url)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)

driver.quit()
코드를 조금 바꿔서 이미지 URL 전체를 image_urls라는 리스트에 저장해 주겠습니다.


image_urls = []

for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 이미지 URL 저장
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_path = style_attr.split('"')[1]
    image_url = "https://workey.codeit.kr" + image_path
    image_urls.append(image_url)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)
웹에서 가져온 '이미지 주소' 부분 앞에 https://workey.codeit.kr를 더해줬습니다.

파이썬으로 이미지 다운로드하기
이제 파이썬을 이용해서 이미지를 다운로드해 봅시다. 파이썬으로 이미지를 다운로드할 때도 requests 라이브러리를 활용할 수 있습니다.

이미지를 다운로드하는 방법은 아래와 같습니다.


response = requests.get(image_url)
with open('image.jpg', 'wb+') as f:
    f.write(response.content)
이미지 URL image_url에 요청을 보내고, 응답의 내용을 'image.jpg' 파일에 저장해 주는 코드입니다. 이미지를 다운로드할 때는 이 코드를 사용하시면 됩니다.

그럼 코스타그램에 있는 모든 이미지를 다운로드 받고 싶다면 아래와 같이 코드를 써 주면 되겠죠?


for i in range(len(image_urls)):
    image_url = image_urls[i]
    response = requests.get(image_url)
    filename = 'image{}.jpg'.format(i)
    with open(filename, 'wb+') as f:
        f.write(response.content)
image_urls 리스트에 있는 모든 URL에 요청을 보내고 싶기 때문에 for 문을 써 줬고, 각 이미지를 새로운 파일에 저장해 줘야 하기 때문에 반복문이 실행될 때마다 파일 이름을 바꿔주는 코드를 써 줬습니다.

전체 코드는 아래와 같습니다.


import time
import requests
from selenium import webdriver

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# 모든 썸네일 요소 가져오기
posts = driver.find_elements_by_css_selector('.post-list__post')

image_urls = []

for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 이미지 URL 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_path = style_attr.split('"')[1]
    image_url = "https://workey.codeit.kr" + image_path
    image_urls.append(image_url)

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)

driver.quit()

# 이미지 다운로드
for i in range(len(image_urls)):
    image_url = image_urls[i]
    response = requests.get(image_url)
    filename = 'image{}.jpg'.format(i)
    with open(filename, 'wb+') as f:
        f.write(response.content)



### 코스타그램 스크래핑 III

# 해설
1. '코스타그램.xlsx' (또는 '코스타그램.csv') 라는 이름으로 엑셀 파일을 만들어 주세요.
엑셀 파일을 만들려면, openpyxl 라이브러리의 Workbook이 필요합니다. import해 줍시다.


import time
from selenium import webdriver
from openpyxl import Workbook
아래와 같이 워크북을 만듭니다.


import time
from selenium import webdriver
from openpyxl import Workbook

# 워크북 생성
wb = Workbook(write_only=True)
ws = wb.create_sheet()
미리 저장해 주는 코드도 작성해 줍시다.


driver.quit()

wb.save('코스타그램.xlsx')
워크북을 저장하는 코드는 코드 맨 끝에 추가해 주세요 (driver.quit() 바로 전에 써 주셔도 됩니다).

2. 파일의 헤더 행은 '이미지 주소', '내용', '해시태그', '좋아요 수', '댓글 수'로 작성해 주세요.
헤더 행도 추가해 줍시다.


wb = Workbook(write_only=True)
ws = wb.create_sheet()
ws.append(['이미지 주소', '내용', '해시태그', '좋아요 수', '댓글 수'])
행을 추가하려면 ws.append()를 쓰면 되겠죠?

3. 저번 과제에서 가져온 이미지 주소에 이어서, 각 포스팅의 내용, 해시태그들, 좋아요 수, 댓글 수를 가져온 다음 엑셀 파일에 저장해 주세요. (이미지 주소를 출력하는 코드는 지워주세요.)
각 포스팅마다 상세 정보를 가져와야 하니까 반복문의 # 나머지 정보 가져오기 부분에 코드를 작성하도록 할게요.


for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    # 나머지 정보 가져오기

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)
먼저 상세 정보 부분에 해당하는 HTML 코드를 봅시다. 첫 번째 사진에서는 내용과 해시태그 부분을 볼 수 있고, 두 번째 사진에서는 좋아요 수와 댓글 수 부분을 볼 수 있습니다.

jno1vm3bp-prac11-2(rev).png

ad7y178gp-prac11-3(rev).png

각 class 이름은 각 태그에만 사용되기 때문에, class 이름을 선택자로 사용할 수 있습니다. (선택자에 다른 추가 조건을 더해주지 않아도 됩니다.) 아래와 같이 코드를 짤 수 있겠죠?


    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    # 나머지 정보 가져오기
    content = driver.find_element_by_css_selector('.content__text')
    hashtags = driver.find_element_by_css_selector('.content__tag-cover')
    like_count = driver.find_element_by_css_selector('.content__like-count')
    comment_count = driver.find_element_by_css_selector('.content__comment-count')
해시태그의 경우 각자의 해시태그를 가져오는 것이 아니라 모든 해시태그가 들어있는 

를 가져왔는데요. 여기에 .text를 해 주면 안에 있는 해시태그들이 하나로 합쳐져서 리턴되기 때문이죠. 플레이리스트 정보 스크래핑 영상에서 해시태그를 가져올 때 사용했던 로직과 똑같습니다.
그럼 모든 태그(웹 요소)에 .text를 해주고, 문자열 양옆에 공백을 제거해 주는 .strip() 메소드도 적용해 줄게요.


    # 이미지 주소 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    # 나머지 정보 가져오기
    content = driver.find_element_by_css_selector('.content__text').text.strip()
    hashtags = driver.find_element_by_css_selector('.content__tag-cover').text.strip()
    like_count = driver.find_element_by_css_selector('.content__like-count').text.strip()
    comment_count = driver.find_element_by_css_selector('.content__comment-count').text.strip()
그럼 이제 가져온 정보를 엑셀 파일에 써 주기만 하면, 코드가 완성됩니다.


    ...
    comment_count = driver.find_element_by_css_selector('.content__comment-count').text.strip()
    ws.append([image_url, content, hashtags, like_count, comment_count])
웹사이트를 열어서 로그인을 하고, 스크롤을 한 다음, 모든 포스팅을 열고 닫으면서 정보를 긁어오고, 긁어온 정보를 엑셀 파일에 저장하는 작업을 모두 자동화했습니다. 웹 자동화의 힘이 느껴지시나요?

이번 토픽에서 배운 내용을 활용해서, 항상 수동적으로 처리했던 웹 작업을 자동화해 보세요!

모범 답안

import time
from selenium import webdriver
from openpyxl import Workbook

# 워크북 생성
wb = Workbook(write_only=True)
ws = wb.create_sheet()
ws.append(['이미지 주소', '내용', '해시태그', '좋아요 수', '댓글 수'])

# 웹 드라이버 설정
driver = webdriver.Chrome()
driver.implicitly_wait(3)

driver.get('https://workey.codeit.kr/costagram/index')
time.sleep(1)

# 로그인
driver.find_element_by_css_selector('.top-nav__login-link').click()
time.sleep(1)

driver.find_element_by_css_selector('.login-container__login-input').send_keys('codeit')
driver.find_element_by_css_selector('.login-container__password-input').send_keys('datascience')

driver.find_element_by_css_selector('.login-container__login-button').click()
time.sleep(1)

# 페이지 끝까지 스크롤
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(0.5)

    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# 모든 썸네일 요소 가져오기
posts = driver.find_elements_by_css_selector('.post-list__post')

for post in posts:
    # 썸네일 클릭
    post.click()
    time.sleep(0.5)

    # 포스팅 정보 가져오기
    style_attr = driver.find_element_by_css_selector('.post-container__image').get_attribute('style')
    image_url = style_attr.split('"')[1]
    content = driver.find_element_by_css_selector('.content__text').text.strip()
    hashtags = driver.find_element_by_css_selector('.content__tag-cover').text.strip()
    like_count = driver.find_element_by_css_selector('.content__like-count').text.strip()
    comment_count = driver.find_element_by_css_selector('.content__comment-count').text.strip()
    ws.append([image_url, content, hashtags, like_count, comment_count])

    # 닫기 버튼 클릭
    driver.find_element_by_css_selector('.close-btn').click()
    time.sleep(0.5)

driver.quit()

wb.save('코스타그램.xlsx')



###  Selenium 기능 더 알아보기: 옵션 고르기

이번 챕터의 마지막 부분에서는 우리가 아직 다루지 않은 Selenium 기능 몇 가지를 알아보도록 할게요. 이 챕터의 모든 내용을 잘 숙지하시면, 웬만한 웹 작업은 자동화하실 수 있을 거에요.

첫 번째는 옵션 고르기입니다.

여기 선거 정보를 확인할 수 있는 웹사이트가 있는데요, '국회의원선거' 탭을 누르면, 이렇게 시/도를 고르는 옵션 박스가 나옵니다.



아래와 같이 많은 옵션들이 있습니다.



이 중에 하나를 선택하면, 선택한 시/도에 해당하는 선거구 옵션들이 나옵니다.



선거구를 선택하고, 검색 버튼을 누르면 데이터를 조회할 수 있습니다.



웹에서 데이터를 조회하려고 할 때는 이렇게 옵션을 골라야 하는 경우가 많습니다. 참고로 이 웹사이트는 티비랭킹닷컴과 달리 옵션으로 고르는 파라미터가 URL에 들어가지 않습니다. 티비랭킹닷컴은 아래와 같이 조회하고 싶은 기간을 URL에 넣어줬으면 됐었죠?


https://workey.codeit.kr/ratings/index?year=2010&month=1&weekIndex=0
그런데 선거 웹사이트는 URL에 조회 파라미터가 들어가지 않기 때문에 꼭 방금 한 것처럼 옵션을 마우스로 선택해 줘야 하는 거죠 (선거 웹사이트의 URL을 보시면 옵션이 바뀌어도 URL이 바뀌지 않습니다).

위에서 봤던 드롭다운 메뉴에서 여러 옵션 중 하나를 선택하는 웹 요소를 select 요소라고 하는데요. Selenium에서는 select 요소를 다루는 간편한 문법이 따로 있습니다. 같이 살펴봅시다.

Selenium으로 Select 요소 다루기
먼저 select 요소의 HTML 코드를 살펴봅시다.



Select 요소는 select 태그로 돼있고, 안에 있는 옵션들은 option 태그로 돼있습니다. option 태그의 value 속성은 옵션을 선택했을 때 실제로 서버에 전달되는 값입니다. 그러니까 서울특별시를 선택하면 '1100'이 전달되는 거죠. 서버는 '1100'이라는 값을 받아서 어떻게 처리를 하겠죠?

사실 우리가 이미 배운 웹 요소를 찾는 방법과, 클릭하는 방법을 사용해서 옵션을 고를 수도 있는데, 그러려면 선택하고 싶은 옵션(예를 들어 '서울특별시')의 value 값을 찾아서, 선택자를 만들어야 합니다. 하지만 select 전용 문법을 사용하면 옵션 이름, value, 또는 옵션 위치를 사용해서 쉽게 옵션을 선택할 수 있기 때문에 훨씬 더 간편합니다.

Select 문법을 사용하려면 먼저 Select라는 툴을 임포트해 줍니다.


from selenium.webdriver.support.ui import Select
그리고 select 웹 요소를 찾아서, Select 안에 넣어 줍니다.


cityCode_select = Select(driver.find_element_by_css_selector('#cityCode'))
select 태그는 'cityCode'라는 id 값을 가지고 있죠?

그리고 Select를 이용해서 옵션 이름, 옵션 value, 또는 옵션 인덱스로 옵션을 선택할 수 있습니다.


# 옵션 이름으로 선택 (웹사이트에서 보이는 옵션 이름)
cityCode_select.select_by_visible_text('서울특별시')

# 옵션의 value로 선택 ('서울특별시' 옵션의 value는 1100)
cityCode_select.select_by_value('1100')

# 옵션의 인덱스로 선택 ('서울특별시'는 두 번째 옵션)
cityCode_select.select_by_index(1)
선거구를 선택하는 것도 비슷하게 할 수 있습니다. 선거구 select 요소는 'sggCityCode'라는 id 값을 가지고 있습니다.




sggCityCode_select = Select(driver.find_element_by_css_selector('#sggCityCode'))

# 옵션 이름으로 선택 (웹사이트에서 보이는 옵션 이름)
sggCityCode_select.select_by_visible_text('종로구')

# 옵션의 value로 선택 ('종로구' 옵션의 value는 2110101)
sggCityCode_select.select_by_value('2110101')

# 옵션의 인덱스로 선택 ('종로구'는 두 번째 옵션)
sggCityCode_select.select_by_index(1)
서울특별시와 종로구를 선택하고, 필요한 국회의원 정보를 가져오는 코드를 짜 보면 아래와 같겠죠?


import time
from selenium import webdriver
from selenium.webdriver.support.ui import Select

driver = webdriver.Chrome()
driver.implicitly_wait(3)

# 선거 웹사이트 접속
driver.get('http://info.nec.go.kr/main/showDocument.xhtml?electionId=0020200415&topMenuId=CP&secondMenuId=CPRI03')
time.sleep(1)

# '국회의원선거'탭 클릭 
driver.find_element_by_css_selector('#electionId2').click()

# 서울특별시 선택
cityCode_select = Select(driver.find_element_by_css_selector('#cityCode'))
cityCode_select.select_by_visible_text('서울특별시')

# 종로구 선택
sggCityCode_select = Select(driver.find_element_by_css_selector('#sggCityCode'))
sggCityCode_select.select_by_visible_text('종로구') 

# '검색'버튼 클릭
driver.find_element_by_css_selector('#spanSubmit').click()

# 필요한 국회의원 정보 가져오기

# 웹 드라이버 종료
driver.quit()



Selenium Select는 옵션을 선택하는 것 외에도 현재 선택된 옵션을 조회한다든지, 모든 옵션을 리스트로 확인한다든지, 여러 기능을 제공합니다. 그리고 우리가 이번 레슨에서 봤던 select 요소는 옵션을 하나만 고를 수 있었는데요, 옵션 여러 개를 고를 수 있는 mutliple select 요소도 있습니다 (<select multiple>처럼 select 태그에 multiple이라는 불린 속성이 있습니다). multiple select도 방금 본 예시와 다를 것 없이 .select_by 메소드들을 쓰면 됩니다.

여기서 Select의 모든 기능을 확인하실 수 있습니다.



### Selenium 기능 더 알아보기: 웹 브라우저 제어하기

## 기본 기능
먼저 기본적인 기능들을 소개해 드릴게요.

웹 페이지 내비게이션
웹 페이지로 이동

driver.get('URL')
뒤로가기(이전 페이지로 이동)

driver.back()
앞으로가기(이후 페이지로 이동)

driver.forward()
새로고침

driver.refresh()
현재 URL 가져오기

driver.current_url
웹 브라우저 창 (window) 제어
*어떤 웹사이트들은 브라우저 창 크기에 따라 콘텐츠가 다르게 디스플레이되거나, 창 크기가 너무 작으면 아예 로딩이 안될 수 있습니다.

풀스크린

driver.fullscreen_window()
최대화

driver.maximize_window()
최소화

driver.minimize_window()
크기 조절

driver.set_window_size(800, 600) # 창 크기 (800, 600)으로 설정
스크린샷

driver.get_screenshot_as_file('image_name.png') # image_name.png라는 파일에 저장
더 알고 싶다면?
여기서 웹 드라이버의 모든 기능을 알아보세요!


## 웹 브라우저 창 여러 개 다루기

웹사이트에서 링크를 클릭하면 새로운 창에 웹사이트가 열리는 경우도 있습니다. Selenium으로 여러 창을 다루는 방법을 알려드릴게요.

아래 코드를 실행하면 쿠팡의 '커피' 검색 페이지로 이동합니다.


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

# 쿠팡 '커피' 검색 결과 페이지 접속
driver.get('https://www.coupang.com/np/search?component=&q=%EC%BB%A4%ED%94%BC&channel=user')
time.sleep(1)


각 아이템은 li 태그에 있는데요. 'li.search-product' 선택자를 이용하면 모든 아이템을 가져올 수 있습니다.


products = driver.find_elements_by_css_selector('li.search-product')
그리고 아이템을 클릭하면 아이템 상세 페이지가 새로운 탭에 뜹니다.


products[0].click()
첫 번째 아이템에 대한 상세 페이지가 새로운 탭에 열렸습니다.



하지만 여기서 주의하셔야 할 점은 현재 웹 브라우저에서 열린 탭과, Selenium이 포커스된 탭은 다릅니다. Selenium이 포커스된 탭은, Selenium 웹 드라이버가 현재 '다루고 있는' 탭입니다. 이건 우리가 설정해 주지 않는 한 바뀌지 않습니다. 그러니까 지금처럼 새로운 상세 페이지가 열렸어도, Selenium은 .get()으로 접속한 첫 번째 탭, 검색 페이지에 포커스되어 있는 거죠.

그래서 이렇게 새로운 탭이 열렸어도, 계속 검색 결과 페이지에 있는 것처럼 새로운 아이템들을 클릭할 수 있습니다.


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

# 쿠팡 '커피' 검색 결과 페이지 접속
driver.get('https://www.coupang.com/np/search?component=&q=%EC%BB%A4%ED%94%BC&channel=user')
time.sleep(1)

products = driver.find_elements_by_css_selector('li.search-product')

products[0].click()
products[1].click()
products[2].click()


총 3개의 상세 페이지가 열렸습니다.

Selenium이 포커스된 탭을 바꾸려면 아래 코드를 실행하면 됩니다. 아래 코드를 실행하면 i 번째 탭으로 포커스가 바뀝니다.


driver.switch_to.window(driver.window_handles[i])
driver.window_handles는 현재 열려있는 탭 리스트라고 생각하시면 됩니다. i 번째 탭을 driver.window_handles[i]로 가져온 다음, driver.switch_to.window() 메소드에 넣어 주면 됩니다.

참고로 현재 포커스된 탭은 driver.current_window_handle로 확인하실 수 있습니다.


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

# 쿠팡 '커피' 검색 결과 페이지 접속
driver.get('https://www.coupang.com/np/search?component=&q=%EC%BB%A4%ED%94%BC&channel=user')
time.sleep(1)

products = driver.find_elements_by_css_selector('li.search-product')

products[0].click()
products[1].click()
products[2].click()

# 모든 탭 리스트
print(driver.window_handles)

# 현재 포커스된 탭 - 아직 탭을 바꿔주지 않았기 때문에 처음 .get()으로 접속한 탭
print(driver.current_window_handle)

# 두 번째 탭으로 바꿔주기
driver.switch_to.window(driver.window_handles[1])

# 현재 포커스된 탭
print(driver.current_window_handle)

['CDwindow-E0DB6E3F17B5CC63EBB89879D206E223', 'CDwindow-01259B3ADA63D626F216561895A50607', 'CDwindow-9DDF6AE91890E04C40BBCDA42DD33E33', 'CDwindow-B524F37AA36AC60A3B2E53962C866490']
CDwindow-E0DB6E3F17B5CC63EBB89879D206E223
CDwindow-01259B3ADA63D626F216561895A50607
출력된 값을 비교하면서 코드가 잘 이해되셨는지 확인해 보세요!

탭은 자원을 소모합니다. 탭이 많이 열려 있으면 엄청난 자원을 소모하기 때문에 더 이상 안 쓰는 탭은 닫아주는 것이 좋습니다. 현재 포커스된 탭은 .close()로 닫을 수 있습니다.


driver.close()
탭을 닫으면 현재 포커스된 탭이 없어져 버리기 때문에 새로운 탭으로 포커스를 바꿔줘야 합니다.

자 그럼 이걸 활용해서 '커피' 검색 결과 페이지의 모든 아이템을 클릭하고, 상세 페이지에서 필요한 정보를 모은 다음, 상세 페이지를 닫아줄 수 있습니다.


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

# 쿠팡 '커피' 검색 결과 페이지 접속
driver.get('https://www.coupang.com/np/search?component=&q=%EC%BB%A4%ED%94%BC&channel=user')
time.sleep(1)

products = driver.find_elements_by_css_selector('li.search-product')

# 검색 결과 페이지로 계속 돌아올 것이기 때문에 저장해 놓기
search_result_window = driver.current_window_handle

for product in products:
    # 아이템 클릭
    product.click()
    time.sleep(1)

    # 아이템 상세 페이지로 포커스 이동
    driver.switch_to.window(driver.window_handles[1])
    
    # 아이템 상세 페이지에서 필요한 정보 가져오기

    # 아이템 상세 페이지 닫기
    driver.close()

    # 검색 결과 페이지로 포커스 이동 - 그래야 아이템 (product)를 클릭할 수 있음
    driver.switch_to.window(search_result_window)

driver.quit()
driver.switch_to.window()
driver.window_handles
두 가지를 활용하면 여러 탭을 자유롭게 다룰 수 있습니다.


### Selenium 기능 더 알아보기: iframe 다루기

## iframe이란?
iframe은 HTML 태그 중 하나입니다. iframe 태그는 HTML 문서 안에 또 다른 HTML 문서를 삽입할 때 사용합니다. 예를 들어 광고 같은 서드파티 콘텐츠를 하나의 iframe에 담을 수 있습니다.

네이버 블로그도 iframe을 사용하는데요. 블로그 내용 전체가 iframe 안에 있습니다.



iframe 태그는 실제 HTML 문서를 담고 있는 것이 아니라 HTML 문서를 참조합니다. (개발자 도구로 보면 iframe 안에 HTML 코드가 있는 것처럼 보이지만 실제로는 그렇지 않고, HTML 문서를 참조하기만 합니다.)

따라서 iframe 안에 있는 태그/요소를 찾으려고 하면 실패합니다.



예를 들어 iframe 태그 안에는 위와 같은 코드가 있는데, id가 'blogTitleText'인 요소를 찾으려고 하면 오류가 나는 거죠.


driver.find_element_by_css_selector('#blogTitleText') # 웹 요소를 찾을 수 없다는 오류
iframe에 있는 코드를 접근하려면 iframe으로 이동해야 합니다. Selenium으로 어떻게 하는지 알아봅시다.


## iframe 스위칭

iframe으로 이동하려면 저번 레슨에서 봤던 것과 비슷하게 driver.switch_to를 쓰면 됩니다. 이번엔 탭/창이 아닌 프레임으로 이동하는 것이니까 driver.switch_to.frame()을 사용합니다.


driver.switch_to.window() # 탭/창으로 이동
driver.switch_to.frame() # 프레임으로 이동
driver.switch_to.frame() 안에는

iframe 웹 요소
iframe의 id 속성값
iframe의 name 속성값 (name 속성값은 중복될 수 있습니다)
iframe 인덱스
를 넣을 수 있습니다.



다시 HTML 코드를 보면 보면 iframe이 총 두 개 있고, 첫 번째 iframe의 id와 name은 모두 'mainFrame'입니다. 웹 페이지의 모든 내용은 'mainFrame' 안에 있습니다.

'mainFrame' iframe으로 이동하려면 아래 코드 중 하나를 써 주면 됩니다.


# 웹 요소 파라미터로 사용
driver.switch_to.frame(driver.find_element_by_css_selector('#mainFrame')

# iframe의 id 또는 name 속성값 사용
driver.switch_to.frame('mainFrame')

# iframe 인덱스 사용
driver.switch_to.frame(0)
이제 iframe 안으로 이동했으니, iframe에 해당하는 HTML 코드를 접근할 수 있습니다.



예를 들어 블로그 글의 내용은 <div class="se-main-container"> 안에 있는데, 이 태그를 가져와서 .text를 붙이면, 글 내용을 가져올 수 있겠죠?


import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(3)

# 코드잇 네이버 블로그 접속
driver.get('https://blog.naver.com/codeitofficial')
time.sleep(1)

# 'mainFrame'으로 이동
driver.switch_to.frame('mainFrame')

# 블로그 글 내용 출력
print(driver.find_element_by_css_selector('div.se-main-container').text)

driver.quit()

안녕하세요, 온라인 코딩 스쿨 코드잇입니다.
오늘은 추천 시스템이 무엇인지 간단하게 알아보고, 추천 시스템을 만들 수 있는 두 가지 방법에 대해서 간단히 알아보겠습니다.

...
끝내기 전에 iframe 스위칭에 대해 조금 더 자세히 알려드릴게요. 웹 페이지에는 여러 iframe이 있을 수 있고, iframe 안에 iframe이 또 있을 수도 있습니다.



위 그림에서 가장 바깥에 있는 네모는 웹 페이지 전체를 뜻하고, 웹 페이지 안에는 A와 B iframe, A iframe 안에는 C와 D iframe이 있습니다.

driver.switch_to.frame()을 이용하면 iframe 안으로만 들어갈 수 있고, 한 번에 한 단계씩만 들어갈 수 있습니다. (iframe 안으로 이동해야 iframe에 해당하는 HTML 코드에 접근할 수 있기 때문입니다.)

웹 페이지 전체를 메인 프레임이라고 하면, 메인 프레임에서는 A나 B로만 들어갈 수 있습니다. C나 D로는 바로 갈 수 없습니다.

그리고 A에서는 C나 D로만 들어갈 수 있습니다.

메인 프레임에서 C로 가려면 아래와 같이 코드를 써 줘야 하는 거죠.


driver.get('URL')

driver.switch_to.frame('A')
driver.switch_to.frame('C')
그럼 현재 iframe에서 밖으로 나오는 건 어떻게 할까요?


# 상위 iframe으로 이동
driver.switch_to.parent_frame()

# 웹 페이지 / 최상위 프레임 / 메인 프레임으로 이동 
driver.switch_to.default_content()
위 메소드들을 사용하면 됩니다. C에서 driver.switch_to.parent_frame()을 실행하면 A로 이동하고, driver.switch_to.default_content()를 실행하면 메인 프레임으로 이동하는 거죠.

C에서 B로 이동하고 싶다면 아래와 같이 코드를 써 줄 수 있겠죠?


# 메인 프레임으로 이동
driver.switch_to.default_content()

# B로 이동
driver.switch_to.frame('B')
복잡한 iframe 구조를 가지고 있는 웹사이트를 다룰 때는 방금 배운 내용에 대해 잘 생각해 보시면서 코드를 짜 보세요!



### Selenium 기능 더 알아보기: Headless 모드

## Headless 모드란?
Headless 모드는 웹 브라우저가 눈에 보이지 않는 상태로, 백그라운드에서 실행되는 모드를 뜻합니다. 지금까지는 Selenium 코드를 실행하면 브라우저가 뜨고, 웹사이트에 접속하고, 로그인, 스크롤 동작을 하는 것이 눈에 보였었죠? Headless 모드를 사용하면 이 모든 것이 보이지 않는 상태로 진행됩니다.

그럼 왜 headless 모드를 쓰는 걸까요?

바로 퍼포먼스(performance) 때문입니다. Headless 모드를 사용하면 웹 브라우저를 안 띄워도 되기 때문에 컴퓨터의 자원(CPU, RAM 등)이 덜 소모되고, 더 빠르게 동작합니다.

하지만 단점도 있습니다.

Headless 모드를 사용하면 웹 브라우저가 안 보이기 때문에 오류가 나도 어느 시점에서 오류가 났는지 파악하기 힘들 수가 있습니다. 오류 메시지만 보고 오류를 파악해야 하는 거죠.

Selenium으로 하는 작업이 복잡하고, 오랜 시간이 걸린다면 headless 모드를 고려해 보세요!


## Headless 모드 사용하기

그럼 headless Chrome (구글 크롬을 headless 모드로 사용하는 것)을 어떻게 사용하는지 알아보겠습니다.

먼저 selenium 라이브러리에서 Options를 임포트해 줍니다.


from selenium.webdriver.chrome.options import Options
그리고 옵션을 만들어 준 다음, headless 옵션을 설정해 주고, 브라우저 창 크기도 (1920, 1080)으로 설정해 줍니다.


options = Options()
options.add_argument("--headless")
options.add_argument("window-size=1920,1080")
브라우저 창 크기를 충분히 크게 설정해 주지 않으면 웹사이트의 내용이 잘 로딩되지 않고 (음악 웹사이트도 가로 길이가 너무 짧으면 내용이 보이지 않습니다) 스크롤이 원활하게 안 될 수도 있습니다.

그리고 크롬 드라이버를 생성할 때 이 옵션을 넣어줍니다.


driver = webdriver.Chrome("chromedriver 경로", options=options)
# 또는
driver = webdriver.Chrome(options=options) # 경로 설정이 필요 없는 경우 (chromedriver가 C:\Windows 또는 /usr/local/bin에 위치해 있는 경우)
한번 이번 챕터에서 실행해 봤던 Selenium 코드를 headless 모드로 실행해 보세요!


### 웹 스크래핑 주의사항 (합법과 불법?)

## 웹 스크래핑, 합법과 불법?
웹에서 데이터를 스크래핑하는 것이 불법이라고 생각하는 사람들이 많은데요. 데이터를 스크래핑하는 것 그 자체는 불법이 아닙니다. 하지만 어떤 데이터를 수집하고, 수집한 데이터를 어떻게 사용하는지에 따라 문제가 발생할 수 있습니다. 저작권이 있는 데이터를 허락 없이 배포하거나, 상업적 용도로 활용하면 안 됩니다.

예를 들어, 몇 년 전에 채용정보 플랫폼 사람인HR이 경쟁사 잡코리아에 올라온 채용공고 등을 끌어다 썼다가, 저작권 침해금지 청구 소송을 당한 사례가 있습니다: http://it.chosun.com/site/data/html_dir/2017/09/27/2017092785016.html

웹 스크래핑을 하기 전에, 수집할 데이터를 어떻게 활용할 것인지, 데이터를 의도대로 활용하면 문제가 될 법한 사항은 없는지, 한 번 고려해 보세요.


## 웹 스크래핑 에티켓과 차단

웹 스크래핑은 사람이 아닌 프로그램이 요청을 보냅니다. 프로그램은 짧은 시간에 수많은 요청을 보낼 수 있는데요. 같은 서버로 짧은 시간 내에 수많은 요청을 보내면 서버 트래픽(traffic)이 심해지고, 서버에 과부하가 올 수 있습니다. 그러면 웹사이트의 원활한 서비스에 방해가 되고, 많은 사용자한테 피해가 가는 거죠. 서버에 부하를 많이 주면 스크래핑 프로그램이 악성 프로그램으로 판단돼서 웹사이트의 접속이 차단되거나 제한될 수가 있습니다.

따라서 같은 도메인 내에서 스크래핑을 반복적으로 할 때는 적절한 주기로 스크래핑을 해 주는 것이 좋습니다. 이번 챕터에서 배웠던 time.sleep()을 활용하는 것도 하나의 방법이겠죠?


## robots.txt

robots.txt 파일은 웹 크롤러 (크롤링 전용 프로그램)들이 웹사이트를 크롤링 할 때 지켜야 하는 규칙을 명시해 놓은 파일인데요, 웹 스크래핑을 할 때도 robots.txt 파일을 가이드라인으로 삼으면 좋습니다.

robots.txt 파일은 웹사이트 루트(도메인 뒤에 경로가 붙지 않은 주소)에서 찾을 수 있습니다.

구글: https://www.google.com/robots.txt


User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
Disallow: /sdch
Disallow: /groups
Disallow: /index.html?
Disallow: /?

...

# AdsBot
User-agent: AdsBot-Google
Disallow: /maps/api/js/
Allow: /maps/api/js
Disallow: /maps/api/place/js/
Disallow: /maps/api/staticmap
Disallow: /maps/api/streetview
예를 들어 위에는 구글 robots.txt 파일의 일부인데요. robots.txt 파일에서

User-agent는 누구에게 규칙이 적용되는지를 뜻합니다. *는 모두에게 규칙이 적용된다는 뜻입니다.
Disallow는 경로 아래에 있는 모든 URL에 대해 크롤링을 금지한다는 뜻이고, Allow는 경로 아래에 있는 모든 URL에 대해 크롤링을 허용한다는 뜻합니다.
처음 몇 줄을 해석해 보면, 모든 크롤러는 www.google.com/search로 시작하는 URL은 크롤링하면 안 되는데, 그중 www.google.com/search/about, www.google/com/search/static, www.google.com/search/howsearchworks로 시작하는 URL들은 괜찮다는 뜻입니다.

페이스북: https://facebook.com/robots.txt


# Notice: Collection of data on Facebook through automated means is
# prohibited unless you have express written permission from Facebook
# and may only be conducted for the limited purpose contained in said
# permission.
# See: http://www.facebook.com/apps/site_scraping_tos_terms.php

User-agent: Applebot
Disallow: /ajax/
Disallow: /album.php
Disallow: /checkpoint/
Disallow: /contact_importer/
Disallow: /dialog/
Disallow: /fbml/ajax/dialog/
Disallow: /feeds/
이번에는 페이스북의 robots.txt인데요. 본문을 읽기도 전에, 주석 부분에 메시지가 하나 있습니다.


# Notice: Collection of data on Facebook through automated means is
# prohibited unless you have express written permission from Facebook
# and may only be conducted for the limited purpose contained in said
# permission.
# See: http://www.facebook.com/apps/site_scraping_tos_terms.php
해석하면, 페이스북으로부터 허락을 받지 않았다면, 페이스북에 있는 데이터를 수집하는 것은 금지돼있다는 겁니다. 이렇게 주의사항이 적혀있는 경우도 있으니, 꼭 확인해 주세요.


## 예측할 수 없는 인터넷 상태

이번 챕터에서 봤듯이 Selenium을 쓸 때는 웹사이트가 로딩되도록 기다려 주는 것이 정말 중요합니다. 웹 요소가 로딩되지 않은 상태에서 웹 요소와 상호작용을 하려고 하면 오류가 나고 프로그램이 종료되기 때문이죠. 웹사이트의 로딩 속도는 그날 인터넷의 상태, 서버의 상태에 따라 편차가 클 수 있습니다. 로딩 속도를 확실히 예측할 수가 없는 거죠.

따라서 Selenium을 쓸 때는 최대한 안정적인 코드를 짜 줄 필요가 있습니다. 이번 챕터에서 배운 wait을 잘 활용해야 하는 거죠. 충분한 wait 기간을 설정해 줬는데도 웹 요소를 찾을 수 없다는 오류가 난다면 인터넷/서버의 상태를 의심해 보시고 실제로 문제가 있다면 나중에 재시도해 보세요.


## 웹사이트의 지속적인 변화

웹사이트는 지속적으로 변합니다. 그건 웹사이트의 HTML 코드가 변한다는 뜻이겠죠? 웹사이트의 HTML 코드가 변하면 잘 되던 웹 스크래핑 코드가 더 이상 안될 수도 있습니다. 그러면, 아쉽지만 새로운 웹사이트 구조에 맞는 스크래핑 코드를 새로 짜 줘야 합니다.

잘 작동하던 코드가 어느 날 안된다면 웹사이트의 구조가 변한 것일 수도 있으니까 이 점 유의해 주세요.



### groupby

### 직업 탐구하기 I

## 해설
주어진 데이터에는 'age',' gender', 'occupation' column이 있습니다.

이 데이터를 이용해서 '직업별 평균 나이'를 구하고자 합니다.

먼저 직업 'occupation'을 기준으로 groupby 오브젝트를 만들어 봅시다.


import pandas as pd

df = pd.read_csv('data/occupations.csv')

occupation_group = df.groupby('occupation')
occupation_group

<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x118e582e8>
이 groupby 오브젝트에 .mean() 메소드를 활용하면, 각 column에 대해 직업별 평균 데이터를 알 수 있습니다.


occupation_group.mean()


occupation_group.mean()의 결과도 DataFrame으로 나오게 됩니다.  
이 DataFrame을 우리가 원하는 'age' column으로 정렬해 줍시다.


occupation_group.mean().sort_values(by='age')


결과 전체를 살펴보면 student, entertainment, artist 등이 평균 연령이 낮고,  doctor, educator, healthcare 등의 직업은 평균 연령이 높은 것을 알 수 있습니다.


import pandas as pd

df = pd.read_csv('data/occupations.csv')

occupation_group = df.groupby('occupation')
occupation_group.mean().sort_values(by='age')



### 직업 탐구하기 II

## 실습 설명
이번에는 여자 비율이 높은 직업과, 남자 비율이 높은 직업이 무엇인지 궁금한데요.

groupby 문법을 사용해서 '여성 비율'이 높은 순으로 직업을 나열해 보세요.

DataFrame이 아닌 Series로, 'gender'에 대한 값만 아래와 같이 출력되어야 합니다.


occupation
homemaker        0.857143
healthcare       0.687500
librarian        0.568627
artist           0.464286
administrator    0.455696
none             0.444444
writer           0.422222
marketing        0.384615
other            0.342857
student          0.306122
educator         0.273684
salesman         0.250000
lawyer           0.166667
entertainment    0.111111
scientist        0.096774
executive        0.093750
programmer       0.090909
retired          0.071429
technician       0.037037
engineer         0.029851
doctor           0.000000
Name: gender, dtype: float64
주의 사항: 자동 채점 과제입니다. 정답은 print 없이 출력해 주세요.


## 해설
동일한 데이터를 이용해서 '직업에 따른 성별 비율'을 구하고자 합니다.

먼저 직업 'occupation'을 기준으로 groupby 오브젝트를 만들어 봅시다.


import pandas as pd

df = pd.read_csv('data/occupations.csv')

occupation_group = df.groupby('occupation')
occupation_group

<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x118e582e8>
이 groupby 오브젝트에 .mean() 메소드를 활용하면, 각 column에 대해 직업별 평균 데이터를 알 수 있습니다.


occupation_group.mean()


하지만 우리가 원하는 'gender'에 대한 값은 여기에 나오지 않네요.

'gender'가 'M', 'F' 등의 문자로 되어 있어서, 평균을 구할 수 없기 때문입니다.

그럼 어떻게 해야 평균 성별을 구할 수 있을까요?

남성을 0, 여성을 1로 변경해주면, 숫자로 계산을 할 수 있습니다.


df.loc[df['gender'] == 'M', 'gender'] = 0
df.loc[df['gender'] == 'F', 'gender'] = 1
남자를 의미하는 'M'이 숫자 0으로, 여자를 의미하는 'F'가 숫자 1로 바뀌었습니다.

이제 각 그룹에 대한 평균값을 구할 수 있습니다.

우리는 이 중에서 gender에 대한 평균만 Series로 가져와 봅시다.


occupation_group.mean()['gender']

occupation
administrator    0.455696
artist           0.464286
doctor           0.000000
educator         0.273684
engineer         0.029851
entertainment    0.111111
executive        0.093750
healthcare       0.687500
homemaker        0.857143
lawyer           0.166667
librarian        0.568627
marketing        0.384615
none             0.444444
other            0.342857
programmer       0.090909
retired          0.071429
salesman         0.250000
scientist        0.096774
student          0.306122
technician       0.037037
writer           0.422222
Name: gender, dtype: float64
보기 편하게 데이터를 정렬해 줍시다.


occupation_group.mean()['gender'].sort_values(ascending=False)

occupation
homemaker        0.857143
healthcare       0.687500
librarian        0.568627
artist           0.464286
administrator    0.455696
none             0.444444
writer           0.422222
marketing        0.384615
other            0.342857
student          0.306122
educator         0.273684
salesman         0.250000
lawyer           0.166667
entertainment    0.111111
scientist        0.096774
executive        0.093750
programmer       0.090909
retired          0.071429
technician       0.037037
engineer         0.029851
doctor           0.000000
Name: gender, dtype: float64
이제 각 직업별로 성별 비율을 알 수 있게 되었습니다.  
남자가 숫자 0, 여자가 숫자 1이기 때문에, 평균이 0에 가까울 수록 남자가 많고, 1에 가까울 수록 여자가 많다는 뜻입니다.

즉, 평균값이 0.250000이라면 25퍼센트가 여성이고, 75퍼센트가 남성입니다.

결과를 분석해 봅시다.

homemaker의 경우 여성 비율이 86퍼센트 정도 되고, healthcare는 여성 비율이 69퍼센트 정도 되네요.  
반대로, doctor는 100퍼센트가 남성이며, engineer는 3퍼센트 정도만 제외하고 모두 남성임을 알 수 있습니다.

## 모범 답안

import pandas as pd

df = pd.read_csv('data/occupations.csv')

occupation_group = df.groupby('occupation')

df.loc[df['gender'] == 'M', 'gender'] = 0
df.loc[df['gender'] == 'F', 'gender'] = 1

occupation_group.mean()['gender'].sort_values(ascending=False)


### 데이터 합치기

Inner Join : 교집합 내용 합치기
Left Outer Join : 왼쪽 데이터를 기준으로 합치기
Right Outer Join : 오른쪽 데이터를 기준으로 합치기
Full Outer Join : 전체 데이터를 합치기


# pd.merge(price_df, quantity_df, on='Product')  //Inner

# pd.merge(price_df, quantity_df, on='Product', how='left')  //Left Outer

# pd.merge(price_df, quantity_df, on='Product', how='right')  //Right Outer

# pd.merge(price_df, quantity_df, on='Product', how='outer')  //Full Outer



### 박물관이 살아 있다 IV

## 실습 설명
파이썬 사전과 .map()을 사용해서 지역명을 알아낸 솔희는, 조금 더 편한 방법을 고민하고 있습니다.

고민하던 중, '지역번호와 지역명에 대한 데이터는 누군가 이미 만들어두지 않았을까'라는 생각을 하게 되는데요.

인터넷에서 지역번호와 지역명이 있는 데이터 region_number.csv를 구했습니다!

이 데이터를 먼저 살펴보고, .merge() 메소드를 활용해서 museum_3.csv에 '지역명' column을 추가해 보세요.

단, museum_3.csv의 박물관 순서가 유지되어야 합니다.

## 해설
museum_3.csv와 region_number.csv를 합쳐서 '지역명' column을 추가하려고 합니다.

먼저 두 데이터를 불러옵시다.


import pandas as pd

museum = pd.read_csv("data/museum_3.csv", dtype={'지역번호': str})
region_number = pd.read_csv("data/region_number.csv", dtype={'지역번호': str})
.merge()메소드를 활용해서 두 DataFrame을 합쳐봅시다.

두 DataFrame에 '지역번호'라는 공통 column이 있습니다. 기준이 되는 column 이름은 on 옵션을 통해 지정할 수 있습니다.

combined = pd.merge(museum, region_number, on='지역번호')
combined에 아래와 같이 데이터가 합쳐진 것을 확인할 수 있습니다.

하지만 이렇게 하면 museum_3.csv의 박물관 순서가 유지되지 않고, '지역번호'를 기준으로 바뀌게 되죠.

museum_3.csv에 등장하는 순서대로 박물관이 나열되어야 하니까, 왼쪽 DataFrame을 기준으로 하는게 좋겠죠?  
how='left'를 붙여 줍시다.


combined = pd.merge(museum, region_number, on='지역번호', how='left')


이제 완성되었습니다.

완성 코드는 다음과 같습니다.

## 모범 답안

import pandas as pd

museum = pd.read_csv("data/museum_3.csv", dtype={'지역번호': str})
region_number = pd.read_csv("data/region_number.csv", dtype={'지역번호': str})

combined = pd.merge(museum, region_number, on='지역번호', how='left')

combined



######## 데이터 퀄리티 올리기 ########

### 데이터 퀄리티의 중요성

데이터의 수치가 좋지 않을때 데이터 클리닝을 통해서 옯은 값을 산출 해낸다.

# 완결성 Completeness
필수적인 데이터는 모두 기록되어 있어야 함.

# 결측값 (채워져야 하는데 비어 있는 값)
NaN (Not a Number)

# 유일성 Uniqueness
동일한 데이터가 불필요하게 중복되어 있으면 안 됨.

이메일 인증하기, 주민등록번호 본인 확인, 휴대폰 번호 본인 확인 = 데이터의 유일성 유지되어야

# 통일성 Conformity
데이터가 동일한 형식으로 저장되어 있어야 함.

# 정확성 Accuracy
데이터가 정확해야 함.

# 이상점 Outlier
다른 값들과 동떨어지게 다른 값

## df.isnull()  // 불린 값으로 변환해서 보여줌.
## df.isnull().sum()  // 결측값을 합산하고 요약해서 보여줌.

df.dropna()  // 결측값을 제외 시킨다. (데이터 원본은 그대로 두고 새로운 데이터를 보여줌)

df.dropna(inplace=True)  // 데이터 원본을 수정
df

df.dropna(axis='columns')

df.fillna(0)  // 결측값을 0으로 수정한다.

df.mean()  // 평균값을 볼 수 있다.

df.fillna(df.mean())  // 결측값을 평균으로 수정한다.

df.median()  // 각 항목의 평균값을 볼 수 있다.

df.fillna(df.median())  // 결측값을 평균 값으로 대체한다.

df.fillna(df.median(), inplace=True) 


### 스팀 게임 데이터 정리하기

# 실습 설명
스팀(Steam)은 온라인 게임을 유통하는 플랫폼입니다.  
스팀 플랫폼에서 가장 반응이 좋은 게임이 무엇인지 알아보려고 하는데요.

데이터를 살펴보니 결측값이 있는 것 같네요. 분석에 앞서 결측값을 제거해 봅시다.

결측값이 있는 모든 row를 삭제하고, DataFrame을 출력해 주세요.

주의 사항: 자동 채점 과제입니다. 정답 출력 코드는 print 없이 작성해 주세요. (예시: df)
해설 보기

# 해설
주어진 데이터에 결측값이 있는지 확인해 봅시다.  
.isnull() 메소드를 사용하면 결측값이 있는 데이터만 True로 표시해 주고, .sum() 메소드를 활용하면 총 몇 개의 결측값이 있는지 확인할 수 있습니다.


import pandas as pd

df = pd.read_csv('data/steam_1.csv')

df.isnull().sum()

Name      0
Hours    10
dtype: int64
'Hours' column에 총 10개의 결측값이 있네요.

dropna 메소드를 사용하면 결측값이 존재하는 row를 모두 없앨 수 있겠죠?


df.dropna(inplace=True)
잘 제거되었습니다. 이제 다양한 분석이나 시각화를 할 수 있겠네요.

전체 코드는 다음과 같습니다.


import pandas as pd

df = pd.read_csv('data/steam_1.csv')

df.dropna(inplace=True)

# 정답 출력
df


6
### 데이터 클리닝: 유일성
import pandas as pd
df = pd.read_csv('data/dust.csv', index_col=0)
df.head()

df.index
df.index.value_counts()
df.drop_duplicates()  // 중복된 값을 지워준다.

df.drop_duplicates(inplace=True)  
df

df.T   //low와 column의 위치가 바뀐다.

df.T.drop_duplicates()

df.T.drop_duplicates().T
df = df.T.drop_duplicates().T


### 데이터 모델
Entity(개체) : 저장하고 싶은 데이터의 대상

실제 대상 하나하나(로우)
Entity Type : 일반화한 Entity 종류 (테이블)

attribute(속성)
Entity에 대해서 저장하려는 내용
학생: 학번, 이름, 성별, 입학년도, 전공
수업: 시간, 이름, 과, 지도 교수

Relationship(관계)
Entity들 사이 연결점

Constraint(제약 조건)
겹치면 안되거나 단일이거나 제약 조건이 있다.



Relation 테이블
Relationship 테이블들 사이 맺어지는 관계
foreign key 를 이용해 관계를 만들수 있다.



### 데이터 모델 정리
# 데이터 모델
데이터 모델이란, 다양한 데이터 요소들을 이해하고 사용하기 편한 형태로 정리해놓은 모형을 의미합니다.

우리가 데이터를 저장하려고 하는 대상: Entity(개체)
Entity에 대해서 저장하려고 하는 특징: Attribute(속성)
Entity들 사이 생기는 연결점: Relationship(관계)
여러 데이터 요소들에 있는 규칙: Constraint(제약 조건)
이 네 가지 요소들을 파악한 후, 이 내용들을 발전시켜 데이터 모델들을 만드는 과정을 데이터 모델링이라고 부릅니다.

# 릴레이셔널 모델
우리에게 가장 익숙하고 가장 널리 사용되는 모델은 릴레이셔널 모델입니다. 릴레이션은 데이터를 로우와 컬럼으로 정리한 테이블, 또는 표를 의미하는데요. Entity는 테이블, attribute은 컬럼, relationship은 foreign key를 사용해서 정리해놓은 모형입니다.

이렇게요.

릴레이셔널 모델을 모델링한다는 건, 정확히 어떤 테이블을 만들고, 이 테이블들을 또 어떤 컬럼들로 나누고, foreign key를 어떻게 만들지를 정해나가는 겁니다.

# ERM
릴레이셔널 모델이 실제로 사용하기에는 굉장히 편리하긴 한데요. 모델링을 할 때는 로우에 대해서 신경을 쓰지 않기 때문에 데이터를 조금 다른 형태로 표현하는 모델을 같이 사용합니다. 바로 Entity-Relationship 모델, 줄여서 ERM이라는 모델입니다.

ERM에서는 Entity를 하나의 네모로, attribute을 네모 안에 문자열로, 그리고 relationship을 선으로 표현합니다. 그리고 나중에 배우겠지만 이 선들의 끝을 어떻게 표현하는지에 따라 관계의 특징을 표현할 수 있습니다.

위에서 봤던 릴레이셔널 모델을 똑같이 ERM으로는 이렇게 나타낼 수 있습니다.


ERM에서는 로우를 매번 표현해주지 않아도 되고, 선과 선의 끝점들을 통해서 Entity들 사이 관계를 조금 더 자세하게 표현할 수 있습니다. 정확히 어떤 내용을 저장할 수 있는지는 다음 챕터에서 자세하게 알아보겠습니다.

데이터 모델 스펙트럼
데이터 모델은 얼마나 자세하게 표현됐는지에 따라 세 가지로 분류됩니다.

개념 모델
가장 추상적인 내용을 담고 있는 모델을 개념 모델이라고 부릅니다. 대략적으로 Entity들과 Entity들 사이에 있는 관계 정도만 표현합니다.

# 논리 모델
그 다음에는 논리 모델이 있습니다. 개념 모델보다는 조금 더 자세한 내용을 담고 있는데요. Entity들이 갖는 Attribute들과 primary key, Entity들 사이 관계를 표현해줄 foreign key, 이런 내용까지 표현합니다.

# 물리 모델
마지막으로는 물리 모델이 있습니다. 물리 모델은 실제로 데이터베이스를 구축할 때 필요한 내용에 최대한 가까운 내용을 담고 있는 모델입니다. 각 컬럼의 데이터 타입, 요소들의 이름, 나중에 배울 인덱스라는 걸 어디에 만들어줄 건지...이런 내용까지 표현하죠.

어떤 모델을 사용해야 될까?
각 모델들은 각각의 장단점들이 있는데요. 자신이 현재 하고 있는 작업에 적합한 모델을 만들어나가면 됩니다.

예를 들어 경영진이 새로운 기능을 추가하려고 사용할 데이터를 파악 및 정리할 때는 개념 모델을 써도 충분하고, 이 내용을 개발자나 데이터베이스 관리자가 구체화 시킬 때는 논리 모델을 사용하고, 마지막에 이걸 실제로 데이터베이스에 저장할 때는 물리 모델을 만들어서 내용을 그대로 반영할 수 있는 거죠.


user
id, email, name

address
id, user_id, address


### Entity, Attribute, Relationship 후보 찾기

모든 명사는 Entity 후보다
모든 동사는 Relationship 후보다
하나의 값으로 표현할 수 있는 명사는 Attribute 후보다


세번째 Attribute 예외의 경우
주소가 여러개 일 경우

user = id,email,name
address = id,user_id,value


## 일대일, 일대다 관계 모델링

user
id, email, name, gender, age

review
id, user_id, score, description
# 일대다 관계에서는 항상 다수 Entity에 foreign key를 추가한다.


